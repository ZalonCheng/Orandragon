<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="5. Basic Nonlinear Programming This chapter is concerned with the following multi-dimensional general constrained minimization problem, \[ \begin{array}{rCll} \min &amp;amp; f(x)\\ \text{subject to}&amp;a">
<meta property="og:type" content="article">
<meta property="og:title" content="5. Basic Nonlinear Programming">
<meta property="og:url" content="http://yoursite.com/2020/12/22/MA6268 Nonlinear Optimization/5. Basic nonlinear programming/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:description" content="5. Basic Nonlinear Programming This chapter is concerned with the following multi-dimensional general constrained minimization problem, \[ \begin{array}{rCll} \min &amp;amp; f(x)\\ \text{subject to}&amp;a">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-10-31T12:41:53.547Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="5. Basic Nonlinear Programming">
<meta name="twitter:description" content="5. Basic Nonlinear Programming This chapter is concerned with the following multi-dimensional general constrained minimization problem, \[ \begin{array}{rCll} \min &amp;amp; f(x)\\ \text{subject to}&amp;a">
  <link rel="canonical" href="http://yoursite.com/2020/12/22/MA6268 Nonlinear Optimization/5. Basic nonlinear programming/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>5. Basic Nonlinear Programming | Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/MA6268 Nonlinear Optimization/5. Basic nonlinear programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">5. Basic Nonlinear Programming

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:48:40" itemprop="dateCreated datePublished" datetime="2020-12-22T13:48:40+08:00">2020-12-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-10-31 20:41:53" itemprop="dateModified" datetime="2019-10-31T20:41:53+08:00">2019-10-31</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MA6268-Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">MA6268 Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2020/12/22/MA6268 Nonlinear Optimization/5. Basic nonlinear programming/" class="post-meta-item leancloud_visitors" data-flag-title="5. Basic Nonlinear Programming" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/12/22/MA6268 Nonlinear Optimization/5. Basic nonlinear programming/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/12/22/MA6268 Nonlinear Optimization/5. Basic nonlinear programming/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="basic-nonlinear-programming">5. Basic Nonlinear Programming</h1>
<p>This chapter is concerned with the following multi-dimensional general constrained minimization problem, <span class="math display">\[
\begin{array}{rCll}
\min &amp; f(x)\\
\text{subject to}&amp; g_i(x)=0,&amp; i=1,2,\dots,m\\
&amp;h_j(x)\le0,&amp; j=1,2,\dots,p\\
&amp;x\in\mathbb R^n,
\end{array}
\]</span> where <span class="math inline">\(f,g_i,h_j:\mathbb R^n\rightarrow \mathbb R\)</span> are continuous functions, for <span class="math inline">\(i=1,2\dots,m\)</span> and <span class="math inline">\(j=1,2,\dots,p\)</span>. The feasible set of the problem is the following subset of <span class="math inline">\(\mathbb R^n\)</span>, <span class="math display">\[
S=\left\{
x\in\mathbb R^n\;\; 
\begin{array}{|rl}
\;g_i(x)=0,&amp;i=1,2,\dots,m
\\
h_j(x)\le0,&amp;j=1,2,\dots,p
\end{array}
\right\}.
\]</span> <span class="math inline">\(S\)</span> is a closed set. The KKT necessary conditions are very useful in locating possible candidates for a global minimizer.</p>
<h2 id="regular-point">5.1 Regular point</h2>
<p><strong>Definition</strong> (Active constraint)</p>
<p>Let <span class="math inline">\(x^*\in S\)</span>. An inequality constraint <span class="math inline">\(h_j(x)\le 0\)</span> is said to be active at <span class="math inline">\(x^*\)</span> if <span class="math inline">\(h_j(x^*)=0\)</span>. Otherwise, it is said to be inactive at <span class="math inline">\(x^*\)</span>. Graphically, if the constraint is active, then the point lies on the boundary defined by the condition <span class="math inline">\(h_j(x)=0\)</span>.</p>
<p><strong>Definition</strong> (Regular point)</p>
<p>Let <span class="math inline">\(x^* \in S\)</span> be a feasible point. Let <span class="math display">\[
J(x^*)=\{j\in\{1,\dotsm,p\}\;|\;h_j(x^*)=0\},
\]</span> be the index set of active constraints at <span class="math inline">\(x^*\)</span>. Suppose the set of gradient vectors <span class="math display">\[
\{\nabla g_i(x^*)\;|\; i=1,2,\dotsm,m\}\cup\{\nabla h_j(x^*)\;|\;j\in J(x^*)\}
\]</span> are linearly independent. Then we say <span class="math inline">\(x^*\)</span> is a regular point, or the regularity condition holds at <span class="math inline">\(x^*\)</span>.</p>
<p>The above condition is called <strong>constraint qualification</strong> at <span class="math inline">\(x^*\in S\)</span>. There are other types of constraint qualifications. In this chapter, we shall only consider the above linearly independence constraint qualification (LICQ).</p>
<p>In particular, if <span class="math inline">\(x^*\)</span> is an interior-point of the feasible region, then <span class="math inline">\(J(x^*)=\emptyset\)</span>. We shall call a point <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(J(x^*)=\emptyset\)</span> a regular point.</p>
<p><strong>Remark</strong></p>
<ol type="1">
<li><p>For an equality constraint nonlinear programming problem without inequality constraints, <span class="math inline">\(x^*\)</span> is a regular point if and only if the set of gradient vectors <span class="math display">\[
\{\nabla g_i(x^*)\;i=1,2,\dotsm,m\}
\]</span> is linearly independent.</p></li>
<li><p>Suppose an NLP contains only inequality constraints and no equality constraint. Then <span class="math inline">\(x^*\)</span> is a regular point if and only if the set of gradient vectors <span class="math display">\[
\{\nabla h_j(x^*)\;j\in J(x^*)\}
\]</span> is linearly independent. In partucular, if <span class="math inline">\(x^*\)</span> is an interior-point of the feasible region, then <span class="math inline">\(J(x^*)=\emptyset\)</span>. We shall call the point <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(J(x^*)=\emptyset\)</span> a regular point.</p></li>
</ol>
<p><strong>Example</strong></p>
<p>Consider the constraints, <span class="math display">\[
\begin{array}{rcl}
g_1(x)&amp;=&amp; x_1-x_2+5x_3-26=0\\
h_1(x)&amp;=&amp; x_1+2x_2^3-4\le 0\\
h_2(x)&amp;=&amp; (x_1-3)^3-4x_2^2+x_3\le 0\\
h_3(x)&amp;=&amp; x_1^2+x_2^2-x_3-4\le 0.
\end{array}
\]</span> Show that the feasible point <span class="math inline">\(x^*=[2;1;5]\)</span> is a regular point.</p>
<p>Solution</p>
<p>Equality constraint <span class="math inline">\(g_1(x^*)=0\)</span>.</p>
<p>Active inequality constriants <span class="math inline">\(h_1(x^*)=0\)</span> and <span class="math inline">\(h_2(x^*)=0\)</span>.</p>
<p>The gradient vectors at <span class="math inline">\(x^*\)</span> are <span class="math display">\[
\begin{array}{rcl}
[\nabla g_1(x^*)\; \nabla h_1(x^*)\;\nabla h_2(x^*)]=
\begin{bmatrix}
1&amp;1&amp;3\\
-1&amp;6&amp;-8\\
5&amp;0&amp;1
\end{bmatrix}
\end{array}.
\]</span> Tha matrix has rank 3, thus is linearly independent. Therefore, <span class="math inline">\(x^*\)</span> is a regular point.</p>
<h2 id="karush-kuhn-tucker-necessary-conditions">5.2 Karush-Kuhn-Tucker necessary conditions</h2>
<p>We state the <strong>KKT necessary conditions</strong> for a local minimizer <span class="math inline">\(x^*\)</span> at which the regularity condition holds.</p>
<p><strong>Theorem</strong> (KKT first order necessary conditions)</p>
<p>Suppose <span class="math inline">\(f,g_i,h_j:\mathbb R^n\rightarrow \mathbb R\)</span> for all <span class="math inline">\(i=1,2,\dotsm,m\)</span> and <span class="math inline">\(j=1,2,\dotsm,p\)</span>, has continuous first partial derivatives on the feasible set <span class="math inline">\(S\)</span>. Suppose <span class="math inline">\(x^*\in S\)</span> is a <strong>regular point</strong>.</p>
<p>If <span class="math inline">\(x^*\)</span> is a local minimizer, then there exists (unique) scalars <span class="math inline">\(\lambda_1^*,\dotsm,\lambda_m^*\)</span> and <span class="math inline">\(\mu_1^*,\dotsm,\mu_p^*\)</span> such that the following conditions hold, <span class="math display">\[
\begin{array}{rCll}
\nabla f(x^*)+\sum_{i=1}^m\lambda_i^*\nabla g_i(x^*)+\sum_{j=1}^p\mu_j^*\nabla h_j(x^*)&amp;=&amp;0\\
\mu_j^*&amp;\ge&amp;0,  &amp;\forall j=1,\dotsm,p\\
\mu_j^*&amp;=&amp;0,&amp;\forall j\notin J(x^*),
\end{array}
\]</span> where <span class="math inline">\(J(x^*)\)</span> is the index set of active inequality constraints at <span class="math inline">\(x^*\)</span>.</p>
<p><strong>Theorem</strong> (KKT second order necessary conditions)</p>
<p>Suppose <span class="math inline">\(f,g_i,\)</span> and <span class="math inline">\(h_j\)</span>, for all <span class="math inline">\(i=1,2,\dotsm,m\)</span> and <span class="math inline">\(j=1,2,\dotsm,p\)</span>, have continuous second partial derivatives on <span class="math inline">\(S\)</span>. Let <span class="math display">\[
H_L(x^*)=H_f(x^*)+\sum_{i=1}^m \lambda_i^*H_{g_i}(x^*)+\sum_{j=1}^p \mu_j^*H_{h_j}(x^*).
\]</span> If <span class="math inline">\(x^*\)</span> is a local minimizer, then <span class="math display">\[
y^TH_L(x^*)y\ge 0,
\]</span> for all <span class="math inline">\(y\in T(x^*)\)</span>, where <span class="math display">\[
T(x^*)=
\left\{
y\in\mathbb R^n \;
\begin{array}{|rl}
\nabla g_i(x^*)^Ty=0, &amp;i=1,2,\dotsm,m\\
\; \nabla h_j(x^*)^T y=0, &amp; j\in J(x^*).
\end{array}
\right\}.
\]</span> <strong>Remark</strong> (On the KKT necessary conditions)</p>
<ol type="1">
<li><p>We say that <span class="math inline">\((x^*,\lambda^*,\mu^*)\)</span> is a KKT point whenever <span class="math inline">\((x^*,\lambda^*,\mu^*)\)</span> satisfies the KKT first order conditions. KKT points are some times also called KKT solutions.</p></li>
<li><p>The scalar <span class="math inline">\(\lambda_1^*,\dotsm,\lambda_m^*\)</span> and <span class="math inline">\(\mu_1^*,\dotsm,\mu_p^*\)</span> are called <strong>Lagrange multipliers</strong>.</p></li>
<li><p>The conditions <span class="math inline">\(\mu_j^*=0,\;\forall j\notin J(x^*)\)</span> means that the Lagrange multiplier corresponding to the inactive constraint must be zero. Since <span class="math inline">\(h_j(x^*)&lt;0\;\forall j\notin J(x^*)\)</span> and <span class="math inline">\(h_j(x^*)=0\;\forall j\in J(x^*)\)</span>, the condition <span class="math inline">\(\mu_j^*=0\;\forall j\notin J(x^*)\)</span> is equivalent to <span class="math display">\[
\mu_j^*h_j(x^*)=0,\quad \forall j=1,\dotsm,p.
\]</span> This is called the <strong>complementary slackness condition</strong>. The name is derived from the fact that for each <span class="math inline">\(j\)</span>, whenever the constraint <span class="math inline">\(h_j(x^*)\le 0\)</span> is slack, i.e., <span class="math inline">\(h_j(x^*)&lt;0\)</span>, the constraint <span class="math inline">\(\mu_j^*\ge 0\)</span> must not be slack, and vice versa.</p></li>
<li><p>The set <span class="math inline">\(T(x^*)\)</span> consists of vectors in the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span>. To see this, note that the normal space <span class="math inline">\(N(x^*)\)</span> to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span> is the subspace spanned by the set of normal vectors <span class="math display">\[
\nabla g_1(x^*),\nabla g_1(x^*),\dotsm\nabla g_m(x^*),\quad \nabla h_j(x^*)\;\forall j\in J(x^*).
\]</span> Now the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span> is the subspace of vectors that are orthogonal to all the normal vectors at <span class="math inline">\(x^*\)</span>. <span class="math display">\[
\text{Tangent} (x^*)=\{y\in\mathbb R^n\;|\; u^Ty=0,\forall u\in N(x^*)\}
\]</span></p></li>
<li><p>In deciding whether <span class="math inline">\(x^*\)</span> is a local optimizer, the definiteness of <span class="math inline">\(H_L(x)\)</span> is tested <strong>only</strong> for vectors in <span class="math inline">\(T(x^*)\)</span>.</p></li>
</ol>
<p>The next result provides an easier check for the KKT second order conditions.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(S\)</span> be the feasible set. Suppose <span class="math inline">\(x^*\in S\)</span> and <span class="math inline">\(J(x^*)\)</span> is the index set of active constraints at <span class="math inline">\(x^*\)</span>.</p>
<p>Consider the matrix <span class="math display">\[
\mathcal D(x^*)=\left(\nabla g_1(x^*),\nabla g_2(x^*),\dotsm,\nabla g_m(x^*),[\nabla h_j(x^*)\;|\;j\in J(x^*)]\right).
\]</span> Then <span class="math display">\[
y^TH_L(x^*)y\ge 0,\quad \forall y\in T(x^*)\iff Z(x^*)^TH_L(x^*)Z(x^*)\in \mathbb S_+^n,
\]</span> where <span class="math inline">\(Z(x^*)\in \mathbb R^{n\times q}\)</span> is a matrix whose columns form a basis of the null space of <span class="math inline">\(\mathcal D(x^*)^T\)</span>.</p>
<p>Proof.</p>
<p>Note that <span class="math inline">\(T(x^*)\)</span> is the null space of the matrix <span class="math inline">\(\mathcal D(x^*)^T\)</span>. So we have <span class="math inline">\(T(x^*)=\{Z(x^*)u\;|\; u\in\mathbb R^q\}\)</span>. Thus <span class="math display">\[
\begin{array}{rlc}
y^TH_L(x^*)y\ge 0,&amp;\forall y\in T(x^*)&amp;\iff\\
u^TZ(x^*)^TH_L(x^*)Z(x^*)u\ge 0, &amp; \forall u\in\mathbb R^q&amp;\iff\\
Z(x^*)^TH_L(x^*)Z(x^*)\in\mathbb S_+^N.
\end{array}
\]</span></p>
<h2 id="examples-to-illustrate-the-kkt-necessary-conditions.">5.3 Examples to illustrate the KKT necessary conditions.</h2>
<p><strong>Corollary</strong></p>
<p>Suppose the following two conditions hold,</p>
<ol type="1">
<li>A global minimizer <span class="math inline">\(x^*\)</span> is known to exist.</li>
<li><span class="math inline">\(x^*\)</span> is a regular point.</li>
</ol>
<p>Then <span class="math inline">\(x^*\)</span> is a KKT point, i.e. there exists <span class="math inline">\(\lambda^*\in \mathbb R^m\)</span> and <span class="math inline">\(\mu^*\in\mathbb R^p\)</span> such that the following conditions hold <span class="math display">\[
\begin{array}{rCl}
\nabla f(x^*)+\sum_{i=1}^m\lambda_i^*\nabla g_i(x^*)+\sum_{j=1}^p\mu_j^*\nabla h_j(x^*)=0\\
g_i(x^*)=0,\;i=1,2,\dotsm,m\\
\mu_j^*\ge 0,\;h_j(x^*)\le 0,\;\mu_j^*h_j(x^*)=0,\;\forall j=1,2,\dotsm,p.
\end{array}
\]</span> <strong>Example</strong> (Projection onto a simplex)</p>
<p>Given <span class="math inline">\(g\in\mathbb R^n\)</span> and <span class="math inline">\(b&gt;0\)</span>. Consider the problem, <span class="math display">\[
\min\left\{f(x)=\frac{1}{2}||x-g||^2\;|\; e^Tx=b,x\ge 0 \right\},
\]</span> where <span class="math inline">\(e\)</span> is the vector of all ones. Its optimal solution <span class="math inline">\(x^*\)</span> can be computed analytically.</p>
<p>Solution</p>
<p>Assume <span class="math inline">\(g_1\ge g_2\ge \dotsm\ge g_n\)</span>.</p>
<ol type="1">
<li><p>Claim that the optimal solution <span class="math inline">\(x^*\)</span> must also satisfy the condition <span class="math inline">\(x_1^*\ge x_2^*\ge\dotsm\ge x_n^*\)</span>. To prove this claim, we suppose on the contrary that there exists indices <span class="math inline">\(i&lt;j\)</span> such that with <span class="math inline">\(x_i^*&lt;x_j^*\)</span>. Consider a new point <span class="math inline">\(\bar x\)</span> defined by (Just switch two elements) <span class="math display">\[
\bar x=\begin{cases}
x_k^*&amp;\text{if }k\neq i,j\\
x_j^*&amp;\text{if }k=i\\
x_i^*&amp;\text{if }k=j
\end{cases}.
\]</span> Then it follows <span class="math display">\[
\begin{array}{rcl}
f(\bar x)-f(x^*)&amp;=&amp;\frac{1}{2}\left((x_i^*-g_j)^2+(x_j^*-g_i^2)\right)-\frac{1}{2}\left((x_i^*-g_i)^2+(x_j^*-g_j^2)\right)\\
&amp;=&amp;(g_i-g_j)(x_i^*-x_j^*)\\
&amp;\le&amp; 0.
\end{array}
\]</span> Hence <span class="math inline">\(f(\bar x)\le f(x^*)\)</span>. Since <span class="math inline">\(x^*\)</span> is a global minimizer, this contradicts the fact that <span class="math inline">\(x^*\)</span> is unique. Therefore, <span class="math inline">\(x_1^*\ge x_2^*\ge\dotsm\ge x_n^*\)</span> holds.</p></li>
<li><p>Then the KKT conditions are given by, <span class="math display">\[
\begin{array}{rcl}
x^*-g-\mu-\lambda e&amp;=&amp;0,\quad \mu\ge 0,\;\lambda\in \mathbb R,\; x^*\ge 0\\
e^Tx^*&amp;=&amp;b\\
\mu x^*&amp;=&amp;0.
\end{array}
\]</span> The first equation implies that <span class="math inline">\(x^*=g+\mu+\lambda e\)</span>.</p>
<p>Now if <span class="math inline">\(g_i+\lambda &gt;0\)</span>, let <span class="math inline">\(x_i^*=g_i+\lambda\)</span> and <span class="math inline">\(\mu_i=0\)</span>. If <span class="math inline">\(g_i+\lambda \le 0\)</span>, let <span class="math inline">\(x_i^*=0\)</span> and <span class="math inline">\(\mu_i=-(g_i+\lambda)\)</span>.</p>
<p>Now suppose <span class="math inline">\(x_i^*&gt;0\)</span> for <span class="math inline">\(i=1,\dotsm,r\)</span> and <span class="math inline">\(x^*_i=0\)</span> for <span class="math inline">\(i=r+1,\dotsm,n\)</span>. Then we have <span class="math display">\[
b=e^Tx=\sum_{i=1}^rx^*_i=\sum_{i=1}^r g_i+\lambda r\implies \lambda =\frac{1}{r}\left(b-\sum_{i=1}^rg_i\right).
\]</span> Now we need to find the largest <span class="math inline">\(r\)</span> denoted as <span class="math inline">\(\bar r\)</span> such that <span class="math display">\[
\bar r = \max \{r\;|\;x_r\ge 0\},
\]</span> where <span class="math inline">\(x_r=g_r+\lambda=g_r+\frac{1}{r}\left(b-\sum_{i=1}^rg_i\right)\)</span>.</p></li>
</ol>
<p>Finally, it follows <span class="math display">\[
   x_i^*=\begin{cases} g_i+\lambda &amp; \text{if } i\le \bar r\\0&amp; \text{if }i&gt;\bar r.\end{cases}
\]</span></p>
<ol start="3" type="1">
<li>If the components of <span class="math inline">\(g\)</span> is not sorted in a descending order, we can find a permutation matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(\hat g=Pg\)</span> has its components arranged in a descending order. Then the optimal solution <span class="math inline">\(x^*\)</span> is given by <span class="math inline">\(x^*=P^T\hat x^*\)</span>.</li>
</ol>
<h2 id="interpretation-of-the-lagrange-multipliers">5.4 Interpretation of the Lagrange multipliers</h2>
<p>The Lagrange multipliers <span class="math inline">\(\lambda_i^*(i=1,\dotsm,m)\)</span>, <span class="math inline">\(\mu_j^*(j=1,\dotsm,p)\)</span> associated with the local optimizer <span class="math inline">\(x^*\)</span> in the KKT theorem measures the sensitivity of <span class="math inline">\(f(x^*)\)</span> to a small perturbation of the corresponding constraints <span class="math inline">\(g_i(x)=0\)</span> or <span class="math inline">\(h_j(x)\le 0\)</span>. In this section, we shall justify this interpretation for an equality constrained NLP. Consider the following equality constrained NLP, <span class="math display">\[
\begin{array}{rll}
\min&amp;f(x)\\
\text{subject to} &amp;g_i(x)=0&amp;\forall i=1,2,\dotsm,m.
\end{array}
\]</span> Suppose the constraints are relaxed as follows, <span class="math display">\[
\hat g_i(x;c)=g_i(x)+c_i,\quad i=1,2,\dotsm,m.
\]</span> Let <span class="math inline">\(C\subseteq \mathbb R^n\)</span> be an open neighborhood of <span class="math inline">\(0\)</span>. For each <span class="math inline">\(c\in C\)</span>, suppose there is a local minimizer <span class="math inline">\(x^*(c)\)</span> of <span class="math inline">\(f\)</span> on the constraint set <span class="math display">\[
\{x\in\mathbb R^n\;|\; \hat g_i(x;c)=0,\;\forall i=1,\dotsm,m\}.
\]</span> Note that <span class="math inline">\(x^*(0)=x^*\)</span> and <span class="math inline">\(\lambda^*(0)=\lambda^*\)</span>. Assume the regularity condition holds at each <span class="math inline">\(x^*(c)\)</span>. Now by the KKT theorem, there exists <span class="math inline">\(\lambda^*(c)\in\mathbb R^m\)</span> such that <span class="math display">\[
\nabla f(x^*(c))+\sum_{i=1}^m\lambda_i^*(c)\nabla \hat g_i(x^*(c))=0.
\]</span> Note that <span class="math display">\[
\frac{\partial \hat g_i}{x_l}(x)=\frac{\partial g_i}{x_l}(x),
\]</span> thus for each <span class="math inline">\(1\le l\le n\)</span>, we have <span class="math display">\[
\frac{\partial f}{\partial x_l}(x^*(c))+\sum_{i=1}^m \lambda_i^*(c)\frac{\partial g_i}{\partial x_l}(x^*(c))=0.
\]</span> <strong>Proposition</strong></p>
<p>Let <span class="math inline">\(F(c)=f(x^*(c))\)</span>. Suppose that <span class="math inline">\(F(c)\)</span> changes smoothly with respect to changes in <span class="math inline">\(c\)</span>. Then <span class="math display">\[
\frac{\partial F(c)}{\partial c_k}=\lambda^*_k(c),\quad \forall k=1,\dotsm,m.
\]</span> Proof. <span class="math display">\[
\begin{array}{rcl}
\frac{\partial F(c)}{\partial c_k}&amp;=&amp;\sum_{l=1}^n \frac{\partial f}{\partial x_l}(x^*(c))\frac{\partial x_l(c)}{\partial c_k}\\
&amp;=&amp;\sum_{l=1}^n\left[-\sum_{i=1}^m \lambda_i^*(c)\frac{\partial g_i}{\partial x_l}(x^*(c))\right]\frac{\partial x_l(c)}{\partial c_k}\\
&amp;=&amp;-\sum_{i=1}^m \lambda_i^*(c)\left[\sum_{l=1}^n\frac{\partial g_i}{\partial x_l}(x^*(c))\frac{\partial x_l(c)}{\partial c_k}\right].
\end{array}
\]</span> Since <span class="math inline">\(g_i(x^*(c))=-c_i\)</span> for all <span class="math inline">\(i=1,\dotsm,m\)</span>, we have <span class="math display">\[
\sum_{l=1}^n\frac{\partial g_i}{\partial x_l}(x^*(c))\frac{\partial x_l(c)}{\partial c_k}=-\frac{\partial c_i}{\partial c_k}=\begin{cases}0&amp;\text{if }k\neq i\\-1&amp;\text{if }k=i\end{cases}.
\]</span> Therefore, we have <span class="math display">\[
\frac{\partial F(c)}{c_k}=\lambda_k^*(c).
\]</span> <strong>Remark</strong></p>
<ol type="1">
<li>Proposition says that the small change in the <span class="math inline">\(k\)</span>th constraint from <span class="math inline">\(g_k(x)=0\)</span> to <span class="math inline">\(g_k(x)+c_k=0\)</span> will change the optimal objective value <span class="math inline">\(f(x^*)\)</span> at the rate of <span class="math inline">\(\lambda_k^*\)</span>. That is the new optimal objective value is given approximately by <span class="math inline">\(f(x^*)+\lambda_k^*c_k\)</span>.</li>
<li>It is also applied to the inequality constraints.</li>
<li>At an optimal solution, a decision-maker can decide whether it is worth to relax the <span class="math inline">\(k\)</span>th constraint based on the multiplier values <span class="math inline">\(\lambda^*_k\)</span> and <span class="math inline">\(\mu^*_k\)</span>.</li>
</ol>
<p><strong>Example</strong></p>
<p>From the example, the global minimizer <span class="math inline">\(x^*=[0;1]\)</span> with multiplier <span class="math inline">\(\mu_3=\frac{1}{2}\)</span>. Thus relaxing the constraint to <span class="math inline">\(g_i(x)+\epsilon\le 0\)</span> would change the objective value by <span class="math inline">\(\frac{1}{2}\epsilon\)</span>.</p>
<h2 id="kkt-sufficient-conditions">5.5 KKT sufficient conditions</h2>
<p>When a KKT point satisfies a stronger second order condition, we obtain a strict local minimizer. Note that the sufficient conditions for a strict local minimizer <strong>do not</strong> require any regularity conditions.</p>
<p><strong>Theorem</strong> (KKT sufficient conditions)</p>
<p>Let <span class="math inline">\(f,g_i,h_j:\mathbb R^n\rightarrow \mathbb R,\;\forall i=1,2,\dotsm,m\)</span> and <span class="math inline">\(j=1,2,\dotsm,p\)</span> be functions with continuous second partial derivatives.</p>
<p>Let <span class="math inline">\(S\)</span> be the feasible set of NLP. Suppose <span class="math inline">\(x^*\in S\)</span> is a KKT point, i.e., there exist <span class="math inline">\(\lambda^*\in\mathbb R^m\)</span> and <span class="math inline">\(\mu^*\in\mathbb R^p\)</span> such that <span class="math display">\[
\begin{array}{rCl}
\nabla f(x^*)+\sum_{i=1}^m\lambda_i^*\nabla g_i(x^*)+\sum_{j=1}^p\mu_j^*\nabla h_j(x^*)&amp;=&amp;0\\
g_i(x^*)&amp;=&amp;0,\;\forall i=1,2,\dotsm,m\\
\mu_j^*\ge 0,\;h_j(x^*)\le 0,\;\mu_j^*h_j(x^*)&amp;=&amp;0,\;\forall j=1,2,\dotsm,p.
\end{array}
\]</span> Let <span class="math display">\[
H_L(x^*)=H_f(x^*)+\sum_{i1=}^m \lambda_i^*H_{g_i}(x^*)+\sum_{j=1}^p\mu_{j}^*H_{h_{j}}(x^*).
\]</span> Suppose <span class="math display">\[
y^TH_L(x^*)y&gt;0,\;\forall y\in T(x^*)-\{0\},
\]</span> where <span class="math inline">\(T(x^*)\)</span> is the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span>. Then <span class="math inline">\(x^*\)</span> is a <strong>strict local minimizer</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<p>Suppose <span class="math display">\[
y^TH_L(x^*)y&lt;0,\;\forall y\in T(x^*)-\{0\},
\]</span> where <span class="math inline">\(T(x^*)\)</span> is the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span>. Then <span class="math inline">\(x^*\)</span> is a <strong>strict local maximizer</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<p><strong>Example</strong></p>
<p>Consider the following nonlinear programming problem, <span class="math display">\[
\begin{array}{rl}
\min &amp; f(x)=-(x_1+x_2)\\
\text{subject to} &amp; h(x)=1-x_1x_2\le 0,\;x\in\mathbb R^2.
\end{array}
\]</span> Verify that <span class="math inline">\(x^*=[-1;-1]\)</span> is a strict local minimizer.</p>
<p>By using the KKT condition, we have <span class="math display">\[
\begin{array}{rcl}
\nabla f(x^*)+\mu^*\circ \nabla h(x^*)&amp;=&amp;0\\
1-x_1^*x_2^*\le 0,\quad \mu^* &amp;\ge &amp;0.
\end{array}
\]</span> That is <span class="math inline">\(\mu^*=1\)</span>. <span class="math inline">\(([-1;-1],1)\)</span> is a KKT point.</p>
<p>From the second order KKT condition, it follows <span class="math display">\[
\begin{array}{rcl}
H_L(x^*)&amp;=&amp;H_f(x^*)+\sum_{i=1}^m \lambda_i^*H_{g_i}(x^*)+\sum_{j=1}^p \mu_j^*H_{h_j}(x^*)\\
&amp;=&amp; 0+0+\begin{bmatrix}0&amp;-1\\-1&amp;0\end{bmatrix}\\
&amp;=&amp;\begin{bmatrix}0&amp;-1\\-1&amp;0\end{bmatrix}.
\end{array}
\]</span> For <span class="math inline">\(Z(x^*)\)</span>, it follows <span class="math inline">\(\mathcal D(x^*)=[1;1]\)</span>. Then <span class="math display">\[
\text{Null}(\mathcal D(x^*)^T)=\text{Null}([1,1])
=\left\{\begin{bmatrix}x_1\\x_2\end{bmatrix}\;\Bigg|\;x_1+x_2=0\right\}
=\left\{\begin{bmatrix}x_1\\-x_1\end{bmatrix}\right\}.
\]</span> Then we can choose <span class="math inline">\(Z(x*)=[1;-1]\)</span>, and it follows <span class="math display">\[
Z(x^*)^TH_L(x^*)Z(x^*)=[1,-1]\begin{bmatrix}0&amp;-1\\-1&amp;0\end{bmatrix}\begin{bmatrix}1\\-1\end{bmatrix}=2\ge 0.
\]</span> Hence, <span class="math inline">\(x^*\)</span> is a strict local minimizer.</p>
<h2 id="kkt-conditions-for-constrained-convex-programming-problems">5.6 KKT conditions for constrained convex programming problems</h2>
<p>Convexity is a very strong condition. In fact, for a convex problem, a feasible point <span class="math inline">\(x^*\)</span> is a KKT point implies it is a global minimizer.</p>
Consider the following convex programming problem, $$
<span class="math display">\[\begin{array}{rCl}
\min&amp;f(x)\\
\text{subject to} &amp; g_i(x)=a_i^Tx-b_i=0,&amp;i=1,\dotsm,m\\
&amp;h_j(x)\le 0,&amp;j=1,\dotsm,p

\end{array}\]</span>
<span class="math display">\[
where $f,h_j:\mathbb R^n\rightarrow \mathbb R$ are convex functions. It can be expressed as
\]</span>
<span class="math display">\[\begin{array}{rCl}
\min&amp;f(x)\\
\text{subject to} &amp; Ax-b=0\\
&amp;h_j(x)\le 0,&amp;j=1,\dotsm,p

\end{array}\]</span>
<p>$$ <strong>Theorem</strong> (KKT point is an optimal solution under convexity)</p>
<p>Suppose <span class="math inline">\(f,h_j:\mathbb R^n\rightarrow \mathbb R,\;j=1,\dotsm,p\)</span> are <strong>differentiable convex functions</strong>, and <span class="math inline">\(g_i(x)=a_i^Tx-b_i,\;i=1,\dotsm,m\)</span>. Let <span class="math inline">\(S\)</span> be the feasible region of NLP. If <span class="math inline">\(x^*\in S\)</span> is a KKT point, then <span class="math inline">\(x^*\)</span> is a global minimizer of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<p>Proof.</p>
<p>If <span class="math inline">\(x^*\)</span> is a KKT point, then we need to prove <span class="math inline">\(f(x)\ge f(x^*)\)</span>.</p>
<p>Since we have <span class="math display">\[
\begin{array}{rcll}
f(y)&amp;\ge&amp; f(x)+\nabla f(x)^T(y-x)\;\forall x,y\in\mathbb R^n&amp;\implies \\
f(x)&amp;\ge&amp; f(x^*)+\nabla f(x^*)(x-x^*).
\end{array}
\]</span> Therefore, to prove <span class="math inline">\(f(x)\ge f(x^*)\)</span>, we only need to prove <span class="math display">\[
\nabla f(x^*)(x-x^*)\ge 0.
\]</span> <span class="math inline">\(x^*\in S\)</span> is a KKT point implies <span class="math inline">\(\exists \lambda \in \mathbb R^m\)</span> and <span class="math inline">\(\mu_j^*\ge 0,\;\forall j\in J(x^*)\)</span> such that <span class="math display">\[
\begin{array}{rcl}
\nabla f(x^*)+\sum_{i=1}^m \lambda_i^*a_i+\sum_{j\in J(x^*)}\mu_j^*\nabla h_j(x^*)&amp;=&amp;0
\quad\implies\\
\nabla f(x^*)(x-x^*)+\sum_{i=1}^m \lambda_i^*a_i(x-x^*)+\sum_{j\in J(x^*)}\mu_j^*\nabla h_j(x^*)(x-x^*)&amp;=&amp;0
\quad\implies\\
\nabla f(x^*)(x-x^*)&amp;=&amp;-\sum_{j\in J(x^*)}\mu_j^*\nabla h_j(x^*)(x-x^*).
\end{array}
\]</span> Since <span class="math inline">\(h_j\)</span> is convex, we have <span class="math display">\[
0\ge h_j(x)\ge h_j(x^*)+\nabla h_j(x^*)(x-x^*)=\nabla h_j(x^*)(x-x^*).
\]</span> Then we have <span class="math display">\[
\nabla f(x^*)(x-x^*)\ge 0.
\]</span> <strong>Remark</strong></p>
<p>The converse of the Theorem is not true without additional assumption, i.e. a global minimizer of a convex program may not be a KKT point. With regularity condition, a global minimizer is a KKT point. For convex programming problem with at least one inequality constraints, the <strong>Slater's condition</strong> ensures that a global minimizer is a KKT point.</p>
<p><strong>Theorem</strong> (Converse of the theorem under Slater’s condition)</p>
<p>Suppose <span class="math inline">\(f,h_j:\mathbb R^n\rightarrow \mathbb R,\;j=1,\dotsm,p\)</span> are <strong>differentiable convex functions</strong>, and <span class="math inline">\(g_i(x)=a_i^Tx-b_i,\;i=1,\dotsm,m\)</span>. Suppose <span class="math inline">\(p\ge 1\)</span> and that the <strong>Slater's condition</strong> holds, i.e., there exists <span class="math inline">\(\hat x\in\mathbb R^n\)</span> such that <span class="math inline">\(g_i(\hat x)=0,\;\forall i=1,\dotsm,m\)</span> and <span class="math inline">\(h_j(\hat x)&lt;0,\;\forall j=1,\dotsm,p\)</span>. Let <span class="math inline">\(S\)</span> be the feasible region. Suppose <span class="math inline">\(x^*\in S\)</span> is a global minimizer of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>. Then <span class="math inline">\(x^*\)</span> is a KKT point.</p>
<h3 id="linear-equality-constrained-convex-program">5.6.1 Linear equality constrained convex program</h3>
<p>Consider the following linear equality constrained NLP, <span class="math display">\[
\text{(ENLP)}\quad \begin{array}{rCl}
\min &amp; f(x)\\
\text{subject to}&amp; Ax=b, &amp; x\in\mathbb R^n,
\end{array}
\]</span> where <span class="math inline">\(A\)</span> is an <span class="math inline">\(m\times n\)</span> matrix and <span class="math inline">\(f;\mathbb R^n\rightarrow \mathbb R\)</span> is a differentiable function on the feasible region <span class="math inline">\(S\)</span>. Note that a feasible solution <span class="math inline">\(x^*\)</span> is a KKT point if there exists <span class="math inline">\(\lambda^*\in\mathbb R^m\)</span> such that <span class="math display">\[
\nabla f(x^*)+\sum_{i=1}^m \lambda_i^* a_i=0 \iff \nabla f(x^*)+A^T\lambda^*=0.
\]</span> <strong>Theorem</strong> (Linear equality constrained convex program)</p>
<p>Consider the following linear equality constrained convex NLP, <span class="math display">\[
\text{(ECP)}\quad \begin{array}{rCl}
\min &amp; f(x)\\
\text{subject to}&amp; Ax=b, &amp; x\in\mathbb R^n,
\end{array}
\]</span> where <span class="math inline">\(A\)</span> is an <span class="math inline">\(m\times n\)</span> matrix and <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> is a differentiable convex function. Suppose the feasible region <span class="math inline">\(S\)</span> is non-empty. Then a point <span class="math inline">\(x^*\in S\)</span> is a KKT point if and only if <span class="math inline">\(x^*\)</span> is a global minimizer of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<p><strong>Remark</strong></p>
<p>No singularity or Slater’s condition is needed.</p>
<h3 id="linear-and-convex-quadratic-programming-problems">5.6.2 Linear and convex quadratic programming problems</h3>
<p><strong>Linear Programming</strong> <span class="math display">\[
\text{(LP)}\quad \begin{array}{rCl}
\min &amp; f(x)=c^Tx\\
\text{subject to}&amp; b-Ax=0\\
&amp; x\in\mathbb R^n_+,
\end{array}
\]</span> where <span class="math inline">\(A\)</span> is an <span class="math inline">\(m\times n\)</span> matrix and <span class="math inline">\(b\in \mathbb R^m\)</span>.</p>
<p>Note the linear constraints can be rewritten as, <span class="math display">\[
\begin{array}{rcl}
g_i(x)=b_i-a_i^Tx&amp;=&amp;0,&amp;i=1,\dotsm,m\\
h_j(x)=-e_i^Tx&amp;\le&amp; 0,&amp; j=1,\dotsm,n,
\end{array}
\]</span> with <span class="math inline">\(\nabla g_i(x)=-a_i\)</span> and <span class="math inline">\(\nabla h_j(x)=-e_j\)</span>.</p>
<p>KKT conditions become <span class="math display">\[
\begin{array}{rcl}
c-\displaystyle\sum_{i=1}^m\lambda_ia_i-\sum_{j=1}^n\mu_je_j=0,\\
b_i-a_i^Tx=0,&amp;i=1,2,\dotsm,m\\
\mu_j\ge 0,\;-x_i\le 0,\;\mu_jx_j=0,&amp; j=1,2,\dotsm ,n.\\\hline 
\end{array}\\
\iff\\
\begin{array}{rcl}
\hline 
c-A^T\lambda-\mu=0,\\
b-A^Tx=0,\\
\mu\ge 0,\;x\ge 0,\;\mu\circ x=0,
\end{array}
\]</span> where <span class="math display">\[
\mu\circ x=\begin{bmatrix}\mu_1x_1\\\mu_2x_2\\\vdots\\\mu_nx_n \end{bmatrix}.
\]</span> <strong>Convex quadratic programming problem</strong> <span class="math display">\[
\text{(QP)}\quad \begin{array}{rCl}
\min &amp; f(x)=\frac{1}{2}x^TQx+c^Tx\\
\text{subject to}&amp; Ax-b\le 0\\
&amp; x\in\mathbb R^n_+,
\end{array}
\]</span> where <span class="math inline">\(Q\)</span> is positive semidefinite, <span class="math inline">\(b\in\mathbb R^m\)</span> and <span class="math inline">\(A\in\mathbb R^{m\times n}\)</span>.</p>
<p>KKT conditions become, <span class="math display">\[
\begin{array}{rcl}
Qx+c+\displaystyle\sum_{i=1}^m\mu_ia_i+\sum_{j=1}^n \hat \mu_j(-e_j)=0,\\
\mu_i\ge 0,\;a_i^Tx-b_i\le 0,\;\mu_i(a_i^Tx-b_i)=0,&amp; i=1,2,\dotsm ,n.\\
\hat \mu_j\ge 0,\;x_i\ge 0,\;\hat \mu_jx_j=0,&amp; j=1,2,\dotsm ,n.\\
\hline
\end{array}\\
\iff\\
\begin{array}{rcl}
\hline 
Qx+c+A^T\mu-\hat \mu=0,\\
\mu\ge 0,\;Ax-b\le 0,\;\mu\circ (Ax-b)=0,\\
\hat \mu\ge 0,\;x\ge 0,\;\hat \mu\circ x=0.
\end{array}
\]</span> <strong>Sparse regression problem</strong></p>
<p>Recall the sparse regression problem <span class="math display">\[
\frac{1}{2}||Ax-b||^2+\rho||x||_1,
\]</span> where <span class="math inline">\(\rho&gt;0\)</span> is a parameter. To derive the KKT conditions for this problem, we first reformulate it to the following form, <span class="math display">\[
\quad \begin{array}{rCl}
\min &amp; f(x,u^{(1)}),u^{(2)})=\frac{1}{2}||Ax-b||^2+\rho\langle e,u^{(1)}+u^{(2)}\rangle\\
\text{subject to}&amp; g(x,u^{(1)},u^{(2)})=x-u^{(1)}+u^{(2)}=0\\
&amp; h_1(x,u^{(1)},u^{(2)})=-u^{(1)}\le 0\\
&amp; h_2(x,u^{(1)},u^{(2)})=-u^{(2)}\le 0.
\end{array}
\]</span> We can see that <span class="math inline">\(u^{(1)}=\max (x,0),u^{(2)}=\max (-x,0)\)</span>, and therefore <span class="math inline">\(x=u^{(1)}-u^{(2)}\)</span>.</p>
<p>Then KKT conditions are given as follows, <span class="math display">\[
\left(
\begin{array}{c}
A^T(Ax-b)\\\rho e\\\rho e
\end{array}
\right)+
\left(
\begin{array}{c}
I\\-I\\I
\end{array}
\right)\lambda+
\left(
\begin{array}{c}
0\\-I\\0
\end{array}
\right)\mu^{(1)}+
\left(
\begin{array}{c}
0\\0\\-I
\end{array}
\right)\mu^{(2)}=
\left(
\begin{array}{c}
0\\0\\0
\end{array}
\right)\\
x-u^{(1)}+u^{(2)}=0\\
\mu^{(1)}\circ u^{(1)}=0,\;\mu^{(2)}\circ u^{(2)}=0\\
u^{(1)},u^{(2)},\mu^{(1)},\mu^{(2)}\ge0.
\]</span> From the first equation, it follows <span class="math display">\[
\rho e- \lambda-\mu^{(1)}=0\\
\rho e+ \lambda-\mu^{(2)}=0,
\]</span> which can be expressed as <span class="math display">\[
\mu^{(1)}=\rho e-\lambda \\
\mu^{(2)}=\rho e+\lambda .
\]</span> Then it follows <span class="math display">\[
A^T(Ax-b)+ \lambda=0\\
(\rho e-\lambda)\circ \max (x,0)=0\\(\rho e+\lambda)\circ \max (-x,0)=0\\
-\rho e\le\lambda \le\rho e.
\]</span> Then we have <span class="math display">\[
\lambda_i=
\begin{cases}
\{\rho_i\},&amp;\text{if }x_i&gt;0\\
\ [-\rho_i,\rho_i] &amp; \text{if }x_i=0\\
\{-\rho_i\},&amp;\text{if }x_i&lt;0
\end{cases}\quad i=1,\dotsm,n.
\]</span> ## 5.7 Proof of the KKT first order necessary conditions</p>
<p><strong>Theorem</strong> (KKT necessary conditions for equality constrained NLP)</p>
<p>Let <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> and <span class="math inline">\(g_i:\mathbb R^n\rightarrow \mathbb R,\;\forall i=1,\dotsm,m\)</span> be functions with continuous first partial derivatives. Suppose <span class="math inline">\(x^*\)</span> is a local minimizer of <span class="math inline">\(f\)</span> on the feasible set <span class="math display">\[
S=\{x\in\mathbb R^n\;|\; g_i(x)=0,i=1,\dotsm,m\}.
\]</span> Suppose that <span class="math inline">\(x^*\)</span> is singular. Then there exists a unique vector <span class="math inline">\(\lambda^*\in\mathbb R^n\)</span> such that <span class="math display">\[
\nabla f(x^*)+\sum_{i=1}^m\lambda_i^* \nabla g_i(x^*)=0.
\]</span> Proof.</p>
<p>We approximate the original constrained problem by a sequence of unconstrained optimization that involves a penalty for violation of the constraints.</p>
<p>Let <span class="math inline">\(x^*\)</span> be a local minimizer of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<p>For each positive integer <span class="math inline">\(k\)</span>, let <span class="math display">\[
F^k(x)=f(x)+\frac{k}{2}||g(x)||^2+\frac{\alpha}{2}||x-x^*||^2,
\]</span> where <span class="math inline">\(g(x)=[g_1(x),g_2(x),\dotsm,g_m(x)]\)</span>, and <span class="math inline">\(\alpha\)</span> is a positive scalar.</p>
<p>Since <span class="math inline">\(x^*\)</span> is a local minimizer, <span class="math inline">\(\exists \epsilon &gt;0\)</span>, such that <span class="math inline">\(f(x^*)\le f(x)\)</span> for all feasible <span class="math inline">\(x\)</span> in the closed ball <span class="math inline">\(B\)</span>.</p>
<p>Consider the penalized problem, <span class="math display">\[
\begin{array}{rl}
\min &amp; F^k(x)\\
\text{subject to } &amp; x\in B.
\end{array}
\]</span> Since <span class="math inline">\(B\)</span> is compact and <span class="math inline">\(F^k(x)\)</span> is continuous on <span class="math inline">\(B\)</span>, there is a global minimizer <span class="math inline">\(x^{(k)}\in B\)</span> for <span class="math inline">\(F^k\)</span>. We have <span class="math display">\[
F^k(x^{(k)})\le F^k(x^*)=f(x^*).
\]</span> <span class="math inline">\(f\)</span> is continuous on the compact set <span class="math inline">\(B\)</span> implies <span class="math inline">\(\left\{f(x^{(k)})\right\}_{k=1}^\infty\)</span> is bounded.</p>
<p>Since we have <span class="math display">\[
f(x^{(k)})\le F^k(x^{(k)})\le f(x^*),
\]</span> <span class="math inline">\(F^k(x^{(k)})\)</span> is also bounded. Therefore, <span class="math inline">\(\lim_{k\rightarrow \infty}||g(x^{(k)})||=0\)</span>, otherwise <span class="math inline">\(F^k(x^{(k)})\)</span> is not bounded.</p>
<p><strong>Claim</strong> The sequence <span class="math inline">\(\{x^{(k)}\}\)</span> converges to <span class="math inline">\(x^*\)</span>.</p>
<p>We denote <span class="math inline">\(\bar x=\lim_{k\rightarrow \infty} x^{(k)}\)</span>. We have known that <span class="math inline">\(g(\bar x)=0\)</span>. Then we have <span class="math display">\[
F^k(\bar x)=f(\bar x)+\frac{\alpha}{2}||\bar x-x^*||^2\le f(x^*).
\]</span> Since <span class="math inline">\(x^*\)</span> is the local minimizer, we also have <span class="math display">\[
f(x^*)\le f(\bar x).
\]</span> Therefore, <span class="math inline">\(\bar x=x^*\)</span>.</p>
<p>Since <span class="math inline">\(x^{(k)}\)</span> is the global optimal point for <span class="math inline">\(F^k(x)\)</span>, it is a stationary point of <span class="math inline">\(F^k(x)\)</span>, i.e., <span class="math display">\[
\nabla F^k(x^{(k)})=0.
\]</span> This is equivalent to <span class="math display">\[
\nabla f(x^{(k)})+kG_kg(x^{(k)})+\alpha (x^{(k)}-x^*)=0,
\]</span> where <span class="math display">\[
G_k=\left[\nabla g_1(x^{(k)}),\nabla g_2(x^{(k)}),\dotsm,\nabla g_m(x^{(k)})\right].
\]</span> Since <span class="math inline">\(x^*\)</span> is a regular point, this implies that the matrix <span class="math display">\[
G_*=\left[\nabla g_1(x^*),\nabla g_2(x^*),\dotsm,\nabla g_m(x^*)\right],
\]</span> has rank <span class="math inline">\(m\)</span>. We have <span class="math display">\[
(G_k^TG_k)^{-1}G_k^T\nabla f(x^{(k)})+kg(x^{(k)})+\alpha (G_k^TG_k)^{-1}G_k^T(x^{(k)}-x^*)=0.
\]</span> Define <span class="math inline">\(\lambda_k=kg(x^{(k)})\)</span>. Then when <span class="math inline">\(k\rightarrow \infty\)</span>, we have <span class="math display">\[
\lambda^*=\lim_{k\rightarrow \infty} kg(x^{(k)})=(G_*^TG_*)^{-1}G_*^T\nabla f(x^*).
\]</span> Therefore, we obtain the first order condition <span class="math display">\[
\nabla f(x^*)+G_*\lambda^*=0.
\]</span> Q.E.D.</p>
<p><strong>Remark</strong></p>
<ol type="1">
<li>The term <span class="math inline">\(\frac{k}{2}||g(x)||^2\)</span> imposes a penalty for violating the constraint <span class="math inline">\(g(x)=0\)</span>.</li>
<li>The term <span class="math inline">\(\frac{\alpha}{2}||x-x^*||^2\)</span> is introduced to ensure that <span class="math inline">\(x^*\)</span> is a strict local minimizer of the function <span class="math inline">\(f(x)+\frac{\alpha}{2}||x-x^*||^2\)</span> subject to <span class="math inline">\(g(x)=0\)</span>.</li>
</ol>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/12/22/MA6268 Nonlinear Optimization/4.6 Proximal Algorithms/" rel="next" title="4. Gradient Methods (2) - Proximal Algorithms">
                  <i class="fa fa-chevron-left"></i> 4. Gradient Methods (2) - Proximal Algorithms
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/12/22/MA6268 Nonlinear Optimization/6. Duality/" rel="prev" title="6. Basic Lagrange Duality and Saddle Point Optimality Conditions">
                  6. Basic Lagrange Duality and Saddle Point Optimality Conditions <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#basic-nonlinear-programming"><span class="nav-text">5. Basic Nonlinear Programming</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#regular-point"><span class="nav-text">5.1 Regular point</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#karush-kuhn-tucker-necessary-conditions"><span class="nav-text">5.2 Karush-Kuhn-Tucker necessary conditions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#examples-to-illustrate-the-kkt-necessary-conditions."><span class="nav-text">5.3 Examples to illustrate the KKT necessary conditions.</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#interpretation-of-the-lagrange-multipliers"><span class="nav-text">5.4 Interpretation of the Lagrange multipliers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kkt-sufficient-conditions"><span class="nav-text">5.5 KKT sufficient conditions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kkt-conditions-for-constrained-convex-programming-problems"><span class="nav-text">5.6 KKT conditions for constrained convex programming problems</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-equality-constrained-convex-program"><span class="nav-text">5.6.1 Linear equality constrained convex program</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-and-convex-quadratic-programming-problems"><span class="nav-text">5.6.2 Linear and convex quadratic programming problems</span></a></li></ol></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
