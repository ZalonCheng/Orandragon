<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="4. Gradient methods 4.1 Gradient methods for unconstrained optimization problem Consider the multidimensional unconstrained minimization problem \[ \begin{array}{rCl} \text{minimize} &amp;amp; f(x)\\">
<meta name="keywords" content="Optimization, Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="4. Gradient Methods (1)">
<meta property="og:url" content="http://yoursite.com/2020/12/22/Nonlinear Optimization/4.1 Gradient methods/index.html">
<meta property="og:site_name" content="Cheng-Zilong">
<meta property="og:description" content="4. Gradient methods 4.1 Gradient methods for unconstrained optimization problem Consider the multidimensional unconstrained minimization problem \[ \begin{array}{rCl} \text{minimize} &amp;amp; f(x)\\">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-10-31T14:26:22.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="4. Gradient Methods (1)">
<meta name="twitter:description" content="4. Gradient methods 4.1 Gradient methods for unconstrained optimization problem Consider the multidimensional unconstrained minimization problem \[ \begin{array}{rCl} \text{minimize} &amp;amp; f(x)\\">
  <link rel="canonical" href="http://yoursite.com/2020/12/22/Nonlinear Optimization/4.1 Gradient methods/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>4. Gradient Methods (1) | Cheng-Zilong</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cheng-Zilong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Learning Notes</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/Nonlinear Optimization/4.1 Gradient methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">4. Gradient Methods (1)

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:48:40" itemprop="dateCreated datePublished" datetime="2020-12-22T13:48:40+08:00">2020-12-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-10-31 22:26:22" itemprop="dateModified" datetime="2019-10-31T22:26:22+08:00">2019-10-31</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="gradient-methods">4. Gradient methods</h1>
<h2 id="gradient-methods-for-unconstrained-optimization-problem">4.1 Gradient methods for unconstrained optimization problem</h2>
<p>Consider the multidimensional unconstrained minimization problem <span class="math display">\[
\begin{array}{rCl}
\text{minimize} &amp; f(x)\\
\text{subject to}&amp; x\in S.
\end{array}
\]</span> In the situation where <span class="math inline">\(\nabla f(x)=0\)</span> cannot be solved analytically, we look for an approximate solution via iterative methods.</p>
<p><strong>General framework of an optimization algorithm</strong></p>
<hr>
<p><strong>For</strong> <span class="math inline">\(k=0,1,...,\)</span> <strong>if</strong> <span class="math inline">\(x^{(k)}\)</span> is optimal stop <strong>else</strong> determine an improved estimate of the solution <span class="math inline">\(x^{(k+1)}=x^{(k)}+\alpha_{k}\times p^{(k)}\)</span> <strong>end</strong> <strong>end</strong></p>
<hr>
<p>Here <span class="math inline">\(p^{(k)}\)</span> is a <strong>search direction</strong> that we hope points towards the solution, or that improves our solution in some sense. The scalar <span class="math inline">\(\alpha_k\)</span> is a step length that determines the point <span class="math inline">\(x^{(k+1)}\)</span>. Once the search direction has been solved, the step length can be solved as some auxiliary one-dimensional problems.</p>
<p><strong>Descent property</strong> At a given point <span class="math inline">\(x^*\)</span>, let <span class="math inline">\(\hat d=-\frac{\nabla f(x^*)}{||\nabla f(x^*)||}\)</span>. The value of <span class="math inline">\(f\)</span> decreases most rapidly along the unit direction <span class="math inline">\(\hat d\)</span> and the rate of change of <span class="math inline">\(f\)</span> at <span class="math inline">\(x^*\)</span> along the direction <span class="math inline">\(\hat d\)</span> is <span class="math inline">\(-||\nabla f(x^*)||\)</span>, i.e., as <span class="math inline">\(x\)</span> moves along <span class="math inline">\(\hat d\)</span> from <span class="math inline">\(x^*\)</span> by a small distance <span class="math inline">\(\delta\)</span>, the value <span class="math inline">\(f(x)\)</span> is changed by the amount <span class="math inline">\(-||\nabla f(x^*)||\delta\)</span>.</p>
<p>In general, a direction <span class="math inline">\(d\)</span> such that <span class="math inline">\(\langle \nabla f(x^*),d\rangle &lt;0\)</span> is called a descent direction.</p>
<p>The direction <span class="math inline">\(-\nabla f(x^*)\)</span> is known as the steepest descent direction since it gives the fastest rate of decrease in <span class="math inline">\(f(x)\)</span> among all directions.</p>
<p><strong>Steepest descent method with exact line search</strong></p>
<hr>
<p><strong>[step 0]</strong> Select an initial point <span class="math inline">\(x^{(0)}\)</span>, and <span class="math inline">\(\epsilon&gt;0\)</span> <strong>[step k]</strong> <strong>For</strong> <span class="math inline">\(k=0,1,...,\)</span></p>
<ol type="1">
<li><p>evaluate <span class="math inline">\(d^{(k)} = -\nabla f(x^{(k)})\)</span></p></li>
<li><p><strong>if</strong> <span class="math inline">\(||d^{(k)}||&lt;\epsilon\)</span>, stop and <span class="math inline">\(x^{(k)}\)</span> is an approximate solution</p>
<p><strong>else</strong></p>
<ol type="1">
<li><p>find the value <span class="math inline">\(t_k\)</span> that minimizes the one-dimensional function <span class="math display">\[
g(t)=f(x^{(k)}+td^{(k)})
\]</span></p></li>
<li><p>set <span class="math inline">\(x^{(k+1)}=x^{(k)}+t_kd^{(k)}\)</span>.</p></li>
</ol></li>
</ol>
<p><strong>end</strong></p>
<hr>
<p><strong>Remark</strong></p>
<p>The most difficult part of the steepest descent method usually is to find the <span class="math inline">\(t_k\)</span> that minimizes <span class="math inline">\(f\)</span> along the gradient direction.</p>
<p><strong>Theorem 4.1</strong> The steepest descent method with exact linesearch moves in <strong>perpendicular</strong> steps.</p>
<p><strong>Remarks</strong></p>
<ol type="1">
<li><p>Monotonic decreasing property</p>
<p>If <span class="math inline">\(x^{(k)}\)</span> is a steepest descent sequence for a function <span class="math inline">\(f(x)\)</span>, and if <span class="math inline">\(\nabla f(x^{(k)})\neq 0\)</span> for some <span class="math inline">\(k\)</span>, then <span class="math inline">\(f(x^{(k+1)})&lt;f(x^{(k)})\)</span>.</p></li>
<li><p>Convergence of a steepest descent method</p>
<p>Suppose <span class="math inline">\(f(x)\)</span> is a coercive function with continuous first derivatives on <span class="math inline">\(\mathbb R^n\)</span>. Let <span class="math inline">\(x^{(0)}\in \mathbb R^n\)</span>. Suppose <span class="math inline">\(\{x^{(k)}\}\)</span> is the steepest descent sequence for <span class="math inline">\(f(x)\)</span> with initial point <span class="math inline">\(x^{(0)}\)</span>. Then some subsequence of <span class="math inline">\(\{x^{(k)}\}\)</span> converges. The limit of any convergent subsequence of <span class="math inline">\(\{x^{(k)}\}\)</span> is a critical point of <span class="math inline">\(f(x)\)</span>.</p></li>
</ol>
<h3 id="convergence-rate-of-the-steepest-descent-method-for-an-unconstrained-convex-quadratic-minimization-problem">4.1.1 Convergence rate of the steepest descent method for an unconstrained convex quadratic minimization problem</h3>
<p>Here we consider <span class="math display">\[
\min_{x\in\mathbb R^n}\; q(x)=\frac{1}{2}x^TQx,
\]</span> where <span class="math inline">\(Q\)</span> is symmetric positive definite.</p>
<p><strong>Proposition</strong></p>
<p>For a symmetric positive definite <span class="math inline">\(Q\)</span>, suppose that <span class="math inline">\(\{x^{(k)}\}\)</span> is a sequence obtained from the steepest descent method with exact line search applied to the function <span class="math inline">\(q(x)\)</span>. Then</p>
<ol type="1">
<li><p>Let <span class="math inline">\(d^{k}=\nabla q(x^k)=Qx^k\)</span>, <span class="math display">\[
\frac{q(x^{k+1})}{q(x^{k})}=1-\frac{\langle d^k,d^k\rangle^2}{\langle d^k,Qd^k\rangle\langle d^k,Q^{-1}d^k\rangle}
\]</span></p></li>
<li><p><span class="math display">\[
\frac{q(x^{k+1})}{q(x^{k})}\le \left[\frac{\kappa (Q)-1}{\kappa (Q)+1}\right]^2=\rho(Q)
\]</span></p>
<p>where <span class="math inline">\(\kappa (Q) = \lambda_n/\lambda_1\)</span>, and <span class="math inline">\(\lambda_n\)</span> and <span class="math inline">\(\lambda_1\)</span> are the largest and smallest eigenvalues of <span class="math inline">\(Q\)</span>, respectively. The number <span class="math inline">\(\kappa(Q)\)</span> is called the condition number of <span class="math inline">\(Q\)</span>. When <span class="math inline">\(\kappa(Q)\ge 1\)</span> is small, say less than <span class="math inline">\(10^3\)</span>, <span class="math inline">\(Q\)</span> is said to be well-conditioned.</p></li>
</ol>
<p>Proof is intuitive and skipped.</p>
<p><strong>Remark</strong></p>
<ol type="1">
<li><p>From proposition, we see that the convergence rate <span class="math inline">\(\rho (Q)\)</span> of the steepest descent method depends on <span class="math inline">\(\kappa(Q)\)</span>. When <span class="math inline">\(\kappa(Q)\)</span> is large, the convergence rate <span class="math display">\[
\rho(Q)\approx 1-\frac{4}{\kappa(Q)}.
\]</span></p></li>
<li><p>The number of iterations needed to reduce the relative error <span class="math inline">\(q(x_k)/q(x_0)\)</span> to smaller than <span class="math inline">\(\epsilon\)</span> is given by <span class="math display">\[
k=\left[\frac{\log \epsilon }{\log \rho(Q)}\right]+1
\]</span> where <span class="math inline">\([a]\)</span> denotes the largest integer less than or equal to <span class="math inline">\(a\)</span>.</p></li>
</ol>
<h3 id="convergence-rate-for-the-steepest-descent-method-for-strongly-convex-function">4.1.2 Convergence rate for the steepest descent method for strongly convex function</h3>
<p>Let <span class="math inline">\(S\subset \mathbb R^n\)</span> be a convex set and <span class="math inline">\(f:S\rightarrow \mathbb R\)</span> a convex function. We assume that <span class="math inline">\(f\)</span> is strongly convex with parameter <span class="math inline">\(m\)</span> and has <span class="math inline">\(M\)</span>-Lipschitz continuous gradient on <span class="math inline">\(S\)</span>. Then its Hessian satisfies the following property, <span class="math display">\[
mI\preceq H_f(x)\preceq MI\;\forall x\in S.
\]</span> <strong>Lemma</strong></p>
<p>Let <span class="math inline">\(x^*\)</span> be a minimizer. Then <span class="math display">\[
f(x)-\frac{1}{2m}||\nabla f(x)||^2\le f(x^*)\le f(x)-\frac{1}{2M}||\nabla f(x)||^2 \; \forall x\in S.
\]</span> <strong>Theorem</strong></p>
<p>Let <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> be strongly convex with parameter <span class="math inline">\(m\)</span> and its gradient is <span class="math inline">\(M\)</span>-Lipschitz. Let <span class="math inline">\(x^*\)</span> be the unique of <span class="math inline">\(f\)</span> over <span class="math inline">\(\mathbb R^n\)</span>. Define <span class="math inline">\(E_k = f(x^k)-f(x^*)\)</span>, where <span class="math inline">\(\{x^k\}\)</span> is generated by the steepest descent method with exact linesearch. Then, <span class="math display">\[
E_{k+1}\le E_k-\frac{1}{2M}||\nabla f(x^k)||^2\\
E_{k+1}\le E_k\left(1-\frac{m}{M}\right).
\]</span> Proof.</p>
<p>By using the property of <span class="math inline">\(M\)</span>-Lipschitz, we have <span class="math display">\[
f(y)\le f(x)+\langle \nabla f(x),y-x \rangle+\frac{M}{2}||y-x||^2\;\forall x,y\in\mathbb R^n.
\]</span></p>
<p>If we choose <span class="math inline">\(y=x^k-td^k\)</span>, <span class="math inline">\(d^k=\nabla f(x^k)\)</span> and <span class="math inline">\(x=x^k\)</span>, we have <span class="math display">\[
f(x^k-td^k)\le f(x^k)-t\langle \nabla f(x^k),d^k \rangle+\frac{Mt^2}{2}||d^k||^2.
\]</span> We define the function <span class="math display">\[
f(t)=f(x^k)-t\langle \nabla f(x^k),d^k \rangle+\frac{Mt^2}{2}||d^k||^2.
\]</span> Then we have <span class="math display">\[
f_{\min}=f(\frac{1}{M})=f(x^k)-\frac{1}{2M}||d^k||^2.
\]</span> Therefore, we have <span class="math display">\[
f(x^{k+1})\le f(x^k)-\frac{1}{2M}||d^k||^2.
\]</span> The first inequality can be proved.</p>
<p>For the second inequality, since we have <span class="math display">\[
E_k\le \frac{1}{2m}||\nabla f(x)||^2.
\]</span> Then we have <span class="math inline">\(-||d^k||^2\le -2mE_k\)</span>. The second inequality can be proved.</p>
<p><strong>Remark</strong></p>
<p>From the theorem, we see that <span class="math display">\[
E(x^{k+1})/E(x^1)\le (1-m/M)^k\le \epsilon,
\]</span> which implies that we need the number of iterations <span class="math inline">\(k\)</span> to satisfy <span class="math display">\[
k\ge \frac{\log \epsilon ^{-1}}{\log\rho^{-1}}\approx \frac{m}{M}\log \epsilon^{-1} \;(\text{ if }m/M\ll 1),
\]</span> where <span class="math inline">\(\rho = 1-m/M\)</span>.</p>
<h2 id="line-search-strategies">4.2 Line search strategies</h2>
<ol type="1">
<li><p>Minimization rule = exact line search. <span class="math display">\[
\alpha_k=\arg\min\{f(x^k+\alpha d^k)\;|\;\alpha \ge 0\}.
\]</span> If the line search interval is limited to <span class="math inline">\(\alpha \in [0,\bar \alpha]\)</span>, it is called limited minimization rule.</p></li>
<li><p>Armijo rule (<strong>backtracking</strong> method).</p>
<p>Let <span class="math inline">\(\sigma\in(0,0.5)\)</span> and <span class="math inline">\(\beta\in (0,1)\)</span>. Start with <span class="math inline">\(\bar\alpha\)</span> and continue with $=,,$ until the following inequality is satisfied, <span class="math display">\[
f(x^k+\alpha d^k)\le f(x^k)+\alpha\sigma\langle \nabla f(x^k),d^k\rangle.
\]</span> Let <span class="math inline">\(r\)</span> be the first integer satisfying the inequality. Set <span class="math inline">\(\alpha_k=\beta^r\bar \alpha\)</span>.</p></li>
<li><p>Non-monotone line search. <span class="math display">\[
f(x^k+\alpha d^k)\le \max \{f(x^{k-l}),\dotsm,f(x^k)\}+\alpha \sigma \langle \nabla f(x^k),d^k\rangle.
\]</span></p></li>
</ol>
<h2 id="accelerated-proximal-gradient-method-for-convex-programming">4.3 Accelerated proximal gradient method for convex programming</h2>
<p>Consider a smooth convex function <span class="math inline">\(f\)</span> with <span class="math inline">\(L\)</span>-Lipschitz continuous gradient. We are interested in solving <span class="math display">\[
\min \{F(x)=f(x)+g(x)\;|\;x\in \mathbb R^n\},
\]</span> where <span class="math inline">\(g:\mathbb R^n\rightarrow (-\infty,\infty]\)</span> is a proper closed convex function.</p>
<p><strong>Example</strong></p>
<p>In sparse regression problem, <span class="math inline">\(f(x)=\frac{1}{2}||Ax-b||^2\)</span> and <span class="math inline">\(g(x)=\rho ||x||_1\)</span>.</p>
<p>For a given <span class="math inline">\(\bar x\)</span> and <span class="math inline">\(H\succeq 0\)</span>, consider the convex quadratic function, <span class="math display">\[
q(x;\bar x)= f(\bar x)+\langle \nabla f(\bar x),x-\bar x\rangle +\frac{1}{2}\langle x-\bar x,H(x-\bar x)\rangle.
\]</span> At the current point <span class="math inline">\(\bar x\)</span>, proximal gradient and APG methods solve a sub-problem of the form, <span class="math display">\[
\hat x =\arg \min \{g(x)+q(x;\bar x)\;|\;x\in \mathbb R^n\}.
\]</span> <strong>Accelerated proximal gradient (APG) method</strong></p>
<p>Given a positive sequence <span class="math inline">\(\{t_k\}\)</span> such that <span class="math inline">\(t_{k+1}^2-t_{k+1}\le t_k^2\)</span> starting with <span class="math inline">\(t_0=1\)</span>, <span class="math inline">\(t_1=1\)</span>. Given <span class="math inline">\(x^0\)</span>. For <span class="math inline">\(k=0,1,\dots\)</span>, do the following iterations.</p>
<p><strong>[Step 1]</strong> Set <span class="math inline">\(\beta_k=(t_k-1)/t_{k+1}\)</span> and <span class="math inline">\(\bar x^k=x^k+\beta_k(x^k-x^{k-1})\)</span>.</p>
<p><strong>[Step 2]</strong> Compute <span class="math display">\[
x^{k+1}=\arg \min \{g(x)+q(x;\bar x^k)\;|\;x\in \mathbb R^n\}.
\]</span> When <span class="math inline">\(t_k=1\)</span> for all <span class="math inline">\(k\)</span>, then <span class="math inline">\(\bar x^k=x^k\)</span> for all <span class="math inline">\(k\)</span>, and the method is the standard <strong>proximal gradient method</strong>.</p>
<p><strong>Lemma</strong></p>
<p>Assume that <span class="math inline">\(f(\hat x)\le q(\hat x;\bar x)\)</span>. (This assumption can be satisfied by choosing <span class="math inline">\(H\)</span>) Then we have the following <strong>decent property</strong>, <span class="math display">\[
F(x)+\frac{1}{2}||x-\bar x||_H^2\ge F(\hat x)+\frac{1}{2}||x-\hat x||_H^2\;\forall x\in \mathbb R^n.
\]</span> Proof.</p>
<p>We have <span class="math display">\[
\begin{array}{rCl}
F(x)-F(\hat x)&amp;=&amp;F(x)-f(\hat x)-g(\hat x)\\
&amp;\ge&amp; F(x)-q(\hat x;\bar x)-g(\hat x)\\
&amp;=&amp; g(x)-g(\hat x)+f(x)-f(\bar x)-\langle \nabla f(\bar x),\hat x-\bar x\rangle-\frac{1}{2}||\hat x-\bar x||_H^2.
\end{array}
\]</span> By convexity of <span class="math inline">\(g\)</span> and <span class="math inline">\(f\)</span>, we have <span class="math display">\[
g(x)-g(\hat x)\ge \langle \lambda,x-\hat x\rangle\quad \lambda \in \partial g(\hat x)\\
f(x)-f(\bar x)\ge \langle \nabla f(\bar x),x-\bar x\rang.
\]</span> Then we have <span class="math display">\[
F(x)-F(\hat x)\ge \langle \lambda+\nabla f(\bar x),x-\hat x\rangle -\frac{1}{2}||\hat x-\bar x||_H^2.
\]</span> From the optimality condition, we have <span class="math display">\[
0\in \partial g(\hat x)+\nabla f(\bar x)+H(\hat x-\bar x)\\
\implies\\
\lambda +\nabla f(\bar x)= H(\bar x-\hat x).
\]</span> Then we have <span class="math display">\[
F(x)-F(\hat x)\ge \langle H(\bar x-\hat x),x-\hat x\rangle -\frac{1}{2}||\hat x-\bar x||_H^2\\
=\frac{1}{2}||x-\hat x||_H^2-\frac{1}{2}||x-\bar x||_H^2.
\]</span> Q.E.D.</p>
<p><strong>Theorem</strong></p>
<p>Let <span class="math inline">\(x^*\)</span> be a minimizer of <span class="math inline">\(F(\cdot)\)</span>. Define <span class="math inline">\(E(\cdot)=F(\cdot)-F(x^*)\ge 0\)</span>. Assume that <span class="math inline">\(f(x^{k+1})\le q(x^{k+1};\bar x^k)\)</span> for all <span class="math inline">\(k\)</span>. Then <span class="math display">\[
E(x^{k+1})\le \frac{1}{2t_{k+1}^2}||x^*-x^0||_H^2.
\]</span> In practice, the sequence <span class="math inline">\(\{t_k\}\)</span> is typically defined recursively as follows, <span class="math display">\[
t_{k+1}=\frac{1+\sqrt{1+4t_k^2}}{2}\implies t_k\ge \frac{k}{2}\quad \forall k\ge 1
\]</span> Hence, <span class="math display">\[
0\le F(x^k)-F(x^*)\le \frac{1}{2t_k^2}||x^*-x^0||_H^2\le \frac{2}{k^2}||x^*-x^0||_H^2.
\]</span> That is, the iteration complexity of APG is <span class="math inline">\(O(1/k^2)\)</span>.</p>
<p><strong>Theorem</strong></p>
<p>Assume that <span class="math inline">\(f(x^{k+1})\le q(x^{k+1};\bar x^k)\)</span> for all <span class="math inline">\(k\)</span>. If <span class="math inline">\(t_k=1\)</span> for all <span class="math inline">\(k\)</span>. Then <span class="math display">\[
kE(x^k)+\frac{1}{2}||x^k-x^*||_H^2\le E(x^1)+\frac{1}{2}||x^1-x^*||_H^2\le \frac{1}{2}||x^0-x^*||_H^2.
\]</span> Hence <span class="math display">\[
0\le F(x^k)-F(x^*)\le \frac{1}{2k}||x^0-x^*||_H^2.
\]</span> That is, the iteration complexity of the proximal gradient method is <span class="math inline">\(O(1/k)\)</span>.</p>
<p><strong>Example</strong></p>
<p>Consider the sparse regression problem, <span class="math display">\[
\min \left\{\frac{1}{2}||Ax-b||^2+\rho||x||_1\;|\; x\in\mathbb R^n\right\},
\]</span> where <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span>, <span class="math inline">\(b\in \mathbb R^m\)</span> and <span class="math inline">\(\rho\)</span> are given data. Let <span class="math inline">\(f(x)=\frac{1}{2}||Ax-b||^2\)</span> and <span class="math inline">\(g(x)=\rho||x||_1\)</span>. Then <span class="math inline">\(\nabla f(x)=A^T(Ax-b)\)</span> is Lipschitz continuous with modulus <span class="math inline">\(L=\lambda_\max(AA^T)\)</span>. Pick <span class="math inline">\(H=LI_n\)</span>, the APG subproblem is given by <span class="math display">\[
\begin{array}{rCl}
x^{k+1}&amp;=&amp;\arg\min_{x}\left\{g(x)+\langle \nabla f(\bar x^k),x-\bar x^k\rangle +\frac{L}{2}||x-\bar x^k||^2\;|\; x\in\mathbb R^n\right\}\\
&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}(x-\bar x^k+\frac{2}{L}\nabla f(\bar x^k))^2-\frac{2}{L}||\nabla f(\bar x^k)||^2\right\}\\
&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}[x-(\bar x^k-\frac{2}{L}\nabla f(\bar x^k))]^2\right\}
\end{array}
\]</span> Then if we define <span class="math inline">\(y^k=\bar x^k-\frac{2}{L}\nabla f(\bar x^k)\)</span>, we have <span class="math display">\[
\begin{array}{rCl}
x^{k+1}&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}||x-y^k||^2\right\}\\
&amp;=&amp; P_{\lambda f}\left(\bar x^k-\lambda\nabla f(\bar x^k)\right).
\end{array}
\]</span> where <span class="math inline">\(\lambda = 2/L\)</span> and <span class="math inline">\(f=\rho||x||_1\)</span>. It is easy to see that <span class="math display">\[
P_{\lambda f}(x)=\text{sign}(x)\circ \max \{|x|-\lambda,0\}.
\]</span> <strong>Example</strong></p>
<p>Given <span class="math inline">\(G\in \mathbb S^n\)</span>, consider the projection problem onto the closed convex cone <span class="math inline">\(DNN_n^*=\mathbb S_+^N+\mathcal N^n\)</span>. This problem can be formulated as, <span class="math display">\[
\begin{array}{rcl}
&amp;&amp;\min\left\{\frac{1}{2}||S+Z-G||^2\;|\;S\in\mathbb S^n_+,\;Z\in\mathcal N^n\right\}\\
&amp;=&amp;\min\left\{\frac{1}{2}||Z-(G-S)||^2\;|\;S\in\mathbb S^n_+,\;Z\in\mathcal N^n\right\}\\
&amp;=&amp;\min\left\{\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2\;|\;S\in\mathbb S^n_+\right\}\\
&amp;=&amp;\min\left\{\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2+\delta_{\mathbb S_+^n}(S)\right\}.
\end{array}
\]</span> This can be shown by <span class="math display">\[
\begin{array}{rcl}
&amp;&amp;\min \left\{\frac{1}{2}||Z-A||^2\;|\; Z\in\mathcal N^n\right\}\\
&amp;=&amp;\min \left\{\delta_{\mathcal N^n}(Z)+\frac{1}{2}||Z-A||^2\right\}\\
&amp;=&amp;P_f(A)\\
&amp;=&amp; \Pi_{\mathcal N^n}(A).
\end{array}
\]</span> Or by checking the following property <span class="math display">\[
\begin{array}{rcl}
A&amp;=&amp;P_f(A)+P_{f^*}(A)\\
&amp;=&amp;\Pi_{\mathcal N^n}(A)+\Pi_{-\mathcal N^n}(A)\\
&amp;=&amp;\Pi_{\mathcal N^n}(A)-\Pi_{\mathcal N^n}(-A).
\end{array}
\]</span> We can also obtain the same result.</p>
<p>We define <span class="math inline">\(f(S)=\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2\)</span>. From the optimality condition, we have <span class="math display">\[
\begin{array}{rcl}
0&amp;\in&amp;\nabla f(\bar S)+\partial \delta_{\mathbb S_+^n}\quad \text{$f(\bar S)$ is differentiable}.\\
-\nabla f(\bar S)&amp;\in&amp; \partial \delta_{\mathbb S_+^n}(S)\\
\bar S-\nabla f(\bar S)&amp;\in&amp; (I+\partial \delta_{\mathbb S_+^n})(\bar S)
\end{array}
\]</span> Then we have <span class="math display">\[
\bar S=(I+\partial \delta_{\mathbb S_+^n})^{-1}(\bar S-\nabla f(\bar S))=\Pi_{\mathbb S_+^n}(\bar S-\nabla f(\bar S)).
\]</span> We have <span class="math display">\[
\nabla f(S)=\Pi_{\mathcal N^n}(S-G),
\]</span> and <span class="math display">\[
\begin{array}{rcl}
||\nabla f(S)-\nabla f(S&#39;)||&amp;=&amp;||\Pi_{\mathcal N^n}(S-G)-\Pi_{\mathcal N^n}(S&#39;-G)||\\
&amp;\le &amp; ||(S-G)-(S&#39;-G)||\\
&amp;=&amp; ||S-S&#39;||.
\end{array}
\]</span> Therefore <span class="math inline">\(\nabla f(\cdot)\)</span> is Lipschitz continuous with modulus <span class="math inline">\(L=1\)</span>. Pick <span class="math inline">\(H=LI_n\)</span>, the APG subproblem is given by <span class="math display">\[
S^{k+1}=\Pi\left(\bar S-\nabla f(\bar S)\right).
\]</span></p>
<h2 id="gradient-projection-method">4.4 Gradient projection method</h2>
<p>Let <span class="math inline">\(f:Q\rightarrow \mathbb R\)</span> be a continuously differentiable function (not necessarily convex) defined on a closed convex subset <span class="math inline">\(Q\)</span> of <span class="math inline">\(\mathbb R^n\)</span> with <span class="math inline">\(L\)</span>-Lipschitz continuous gradient. Consider <span class="math display">\[
\min\{f(x)\;|\; x\in Q\}.
\]</span> We say that <span class="math inline">\(\bar x\in Q\)</span> is a stationary point if <span class="math display">\[
\langle \nabla f(\bar x),y-\bar x\rangle \ge 0\quad\forall y\in Q.
\]</span> The gradient projection method generate a sequence of iterates as follows, <span class="math display">\[
x^{k+1}=P_Q(x^k-\alpha \nabla f(x^k)).
\]</span> For each <span class="math inline">\(x\in Q\)</span>, let <span class="math display">\[
x(\alpha )=x-\alpha\nabla f(x),\quad x_Q(\alpha )=P_Q(x(\alpha)).
\]</span></p>
<h2 id="stochastic-gradient-descent-method">4.5 Stochastic gradient descent method</h2>
<p>Suppose <span class="math inline">\(\tilde Z\)</span> is an n-dimensional random variable with mean <span class="math inline">\(\mu \in \mathbb R^n\)</span>. Consider the following problem, <span class="math display">\[
\min_{x\in\mathbb R^n}h(x)=\frac{1}{2}\mathbb E[||x-\tilde Z||^2],
\]</span> where <span class="math inline">\(\mathbb E(\cdot)\)</span> denotes the expectation with respect to the distribution of <span class="math inline">\(Z\)</span>, i.e., <span class="math display">\[
E[||x-\tilde Z||^2]=\int ||x-\tilde Z(w)||^2dP(w).
\]</span> If <span class="math inline">\(Z\)</span> is a discrete random variable, then <span class="math display">\[
E[||x-\tilde Z||^2]=\sum_{i=1}^Np_i||x-a_i||^2.
\]</span> In practice, to solve this problem, one may draw <span class="math inline">\(m\)</span> independent identically distributed samples of <span class="math inline">\(\tilde Z\)</span>, say <span class="math inline">\(S=\{z_1,\dots,z_m\}\)</span>, and then solve <span class="math display">\[
\min_{x\in\mathbb R^n}F_S(x)=\frac{1}{m}\sum_{i=1}^m \frac{1}{2}||x-z_i||^2.
\]</span> The corresponding optimality condition is <span class="math display">\[
0=\nabla F_S(x)=\frac{1}{m}\sum_{i=1}^m(x-z_i)\implies x=\frac{1}{m}\sum_{i=1}^mz_i.
\]</span></p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/12/22/Nonlinear Optimization/3.9 Cones/" rel="next" title="3. Basic Convex Analysis (2)">
                  <i class="fa fa-chevron-left"></i> 3. Basic Convex Analysis (2)
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/12/22/Nonlinear Optimization/4.6 Proximal Algorithms/" rel="prev" title="4. Gradient Methods (2) - Proximal Algorithms">
                  4. Gradient Methods (2) - Proximal Algorithms <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#gradient-methods"><span class="nav-text">4. Gradient methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#gradient-methods-for-unconstrained-optimization-problem"><span class="nav-text">4.1 Gradient methods for unconstrained optimization problem</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#convergence-rate-of-the-steepest-descent-method-for-an-unconstrained-convex-quadratic-minimization-problem"><span class="nav-text">4.1.1 Convergence rate of the steepest descent method for an unconstrained convex quadratic minimization problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convergence-rate-for-the-steepest-descent-method-for-strongly-convex-function"><span class="nav-text">4.1.2 Convergence rate for the steepest descent method for strongly convex function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#line-search-strategies"><span class="nav-text">4.2 Line search strategies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#accelerated-proximal-gradient-method-for-convex-programming"><span class="nav-text">4.3 Accelerated proximal gradient method for convex programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gradient-projection-method"><span class="nav-text">4.4 Gradient projection method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic-gradient-descent-method"><span class="nav-text">4.5 Stochastic gradient descent method</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cheng-Zilong</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cheng-Zilong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
