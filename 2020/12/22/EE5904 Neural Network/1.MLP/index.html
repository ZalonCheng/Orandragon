<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="1. What is Learning in NN Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded. Pro">
<meta property="og:type" content="article">
<meta property="og:title" content="Orandragon&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/2020/12/22/EE5904 Neural Network/1.MLP/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:description" content="1. What is Learning in NN Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded. Pro">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-04-24T12:01:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Orandragon&#39;s Blog">
<meta name="twitter:description" content="1. What is Learning in NN Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded. Pro">
  <link rel="canonical" href="http://yoursite.com/2020/12/22/EE5904 Neural Network/1.MLP/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title> | Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/EE5904 Neural Network/1.MLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:45:47" itemprop="dateCreated datePublished" datetime="2020-12-22T13:45:47+08:00">2020-12-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-04-24 20:01:00" itemprop="dateModified" datetime="2019-04-24T20:01:00+08:00">2019-04-24</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5904-Neural-Network/" itemprop="url" rel="index"><span itemprop="name">EE5904 Neural Network</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2020/12/22/EE5904 Neural Network/1.MLP/" class="post-meta-item leancloud_visitors" data-flag-title="" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/12/22/EE5904 Neural Network/1.MLP/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/12/22/EE5904 Neural Network/1.MLP/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="what-is-learning-in-nn">1. What is Learning in NN</h2>
<p>Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded.</p>
<p>Process of learning</p>
<ol type="1">
<li>The NN is stimulated by the environment</li>
<li>NN undergoes changes in its free parameters</li>
<li>NN responds in a new way to the environment</li>
</ol>
<p>Supervised Learning:</p>
<ol type="1">
<li>The NN is fed with input and produce an output</li>
<li>The teacher will tell what the desired output should be</li>
<li>The weights are adjusted by the error signals</li>
</ol>
<p>Unsupervised Learning:</p>
<ol type="1">
<li>NN is interacting with the environment by taking various actions</li>
<li>The learning system will be rewarded or penalized by its actions</li>
<li>The weights are adjusted by the reinforcement signal</li>
</ol>
<h2 id="perceptron">2. Perceptron</h2>
<p><span class="math display">\[
v = \sum_{i=1}^mw_ix_i+b\\
\]</span></p>
<p>For simplicity <span class="math display">\[
x(n)=[1,x_1(n),x_2(n),\dotsm,x_m(n)]^T\\
w(n)=[b(n),w_1(n),w_2(n),\dotsm,w_m(n)]^T\\
v(n)=w^T(n)x(n)
\]</span> where n denotes iteration step.</p>
<p>How to tune the parameters</p>
<p>First, consider the case, <span class="math display">\[
v=w^Tx&lt;0\quad \varphi(v)=0
\]</span> if the desired output is <span class="math inline">\(d=0\)</span>, then nothing needs to be done</p>
<p>if the desired output is <span class="math inline">\(d=1\)</span>, then we hope <span class="math inline">\(v&#39;\)</span> to be greater</p>
<p>then we have <span class="math display">\[
v&#39;-v = (w&#39;^T-w^T)x=\Delta w x&gt;0
\]</span> the most simple choice is <span class="math inline">\(\Delta w=x^T\)</span></p>
<p>then we have <span class="math display">\[
w&#39; = w+\eta x
\]</span> For the other case <span class="math display">\[
v=w^Tx&gt;0\quad \varphi(v)=1
\]</span> if the desired output is <span class="math inline">\(d=0\)</span>, then we hope <span class="math inline">\(v&#39;\)</span> to be smaller <span class="math display">\[
v&#39;-v = (w&#39;^T-w^T)x=\Delta w x&lt;0
\]</span> the most simple choice is <span class="math inline">\(\Delta w=-x^T\)</span> <span class="math display">\[
w&#39; = w-\eta x
\]</span> then we have <span class="math display">\[
w&#39;=w+\eta e x
\]</span></p>
<p>To summarize,</p>
<p>Perceptron Learning Algorithm</p>
<p>Start with a randomly chosen weight vector <span class="math inline">\(w(1)\)</span></p>
<p>while there exist input vectors that are misclassified by <span class="math inline">\(w(n)\)</span></p>
<p>Do Let <span class="math inline">\(x(n)\)</span> be a misclassified input vector</p>
<p>​ update the weight vector to</p>
<p>​ <span class="math inline">\(w(n+1)=w(n)+\eta e(n)x(n)\)</span></p>
<p>​ <span class="math inline">\(e(n)=d(n)-y(n)\)</span></p>
<p>​ where <span class="math inline">\(\eta&gt;0\)</span> and d=<span class="math inline">\(\begin{cases} 1\text{ if x belongs to class 1} \\ 0\text{ if x belongs to class 2}\end{cases}\)</span></p>
<p>​ n=n+1</p>
<p>end while</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">xc0 = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.5</span>], [<span class="number">1.0</span>, <span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.2</span>, <span class="number">1.1</span>]])</span><br><span class="line">xc1 = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2.5</span>, <span class="number">2.6</span>, <span class="number">2.4</span>, <span class="number">2.6</span>, <span class="number">2.5</span>], [<span class="number">3.0</span>, <span class="number">3.1</span>, <span class="number">3.2</span>, <span class="number">3.2</span>, <span class="number">3.0</span>]])</span><br><span class="line">w = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">fig1 = plt.plot(xc0[<span class="number">1</span>, :], xc0[<span class="number">2</span>, :], <span class="string">'bo'</span>, xc1[<span class="number">1</span>, :], xc1[<span class="number">2</span>, :], <span class="string">'ro'</span>)</span><br><span class="line">yplot = -w[<span class="number">1</span>]/w[<span class="number">2</span>]*np.array([<span class="number">0</span>, <span class="number">3</span>]) - w[<span class="number">0</span>]/w[<span class="number">2</span>]</span><br><span class="line">plt.plot(np.array([<span class="number">0</span>, <span class="number">3</span>]), yplot, <span class="string">'r'</span>)</span><br><span class="line">eta = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># This is for c0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">4</span>):</span><br><span class="line">        v = w.transpose().dot(xc0[:, j])</span><br><span class="line">        <span class="keyword">if</span> v &gt; <span class="number">0</span>:</span><br><span class="line">            y = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = <span class="number">0</span></span><br><span class="line">        e = <span class="number">0</span> - y</span><br><span class="line">        w = w + eta*e*xc0[:, j]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is for c1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">4</span>):</span><br><span class="line">        v = w.transpose().dot(xc1[:, j])</span><br><span class="line">        <span class="keyword">if</span> v &gt; <span class="number">0</span>:</span><br><span class="line">            y = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = <span class="number">0</span></span><br><span class="line">        e = <span class="number">1</span> - y</span><br><span class="line">        w = w + eta*e*xc1[:, j]</span><br><span class="line">    yplot = -w[<span class="number">1</span>] / w[<span class="number">2</span>] * np.array([<span class="number">0</span>, <span class="number">3</span>]) - w[<span class="number">0</span>] / w[<span class="number">2</span>]</span><br><span class="line">    plt.plot(np.array([<span class="number">0</span>, <span class="number">3</span>]), yplot, color=(<span class="number">0.5</span>, <span class="number">0.5</span>, i/<span class="number">10</span>))</span><br><span class="line">    plt.pause(<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Proof of Convergence</p>
<p>There exists <span class="math inline">\(w_0\)</span> such that</p>
<p>For samples in class <span class="math inline">\(C_1\)</span> <span class="math inline">\(w_0^Tx(k)&gt;0\)</span></p>
<p>For samples in class <span class="math inline">\(C_2\)</span> <span class="math inline">\(w_0^Tx(k)&lt;0\)</span> <span class="math display">\[
w(n+1)=w(n)+e(n)x(n)=e(1)x(1)+e(2)x(2)+\dotsm+e(n)x(n)\\
w_0^Tw(n+1)=e(1)w_0^Tx(1)+e(2)w_0^Tx(2)+\dotsm+e(n)w_0^Tx(n)\\
e(k)w_0^Tx(k)=|w_0^Tx(k)|&gt;0\\
\alpha=\min\{|w_0^Tx(i)|\}
\]</span> Then we have <span class="math display">\[
w_0^Tw(n+1)\ge n\alpha\\
||w_0||\cdot||w(n+1)||\ge||w_0^Tw(n+1)||\ge n\alpha\\
\]</span> we have <span class="math display">\[
||w(n+1)||\ge\frac{n\alpha}{||w_0||}
\]</span> Let <span class="math inline">\(\beta = \max \{||x(i)||^2\}\)</span></p>
<p>And we have <span class="math display">\[
||w(n+1)||^2\le n \beta
\]</span> Overall, we have <span class="math display">\[
\frac{n^2\alpha^2}{||w_o||^2}\le ||w(n+1)||^2\le n\beta
\]</span> Therefore, n must be finite. Otherwise, the inequality will not be satisfied.</p>
<h2 id="linear-regression-problem">3. Linear Regression Problem</h2>
<p><span class="math display">\[
E(w)=\frac{1}{2}e(w)^2\\
e(w)=d-x^Tw\\
g(w)=\frac{\part E}{\part w}=\frac{\part E}{\part e}\frac{\part e}{\part w}=-e(w)x^T\\
\]</span></p>
<p>Then we have <span class="math display">\[
w(n+1)=w(n)+\eta e(n)x(n)
\]</span></p>
<p>Example</p>
<p>Training samples {(1, 3.5), (1, 3.5), (1.5, 2.5), (3, 2), (3.5, 1), (4, 1)}</p>
<p>Initial weight <span class="math inline">\(w(1)=[2, 0]^T\)</span></p>
<p>Learning rate <span class="math inline">\(\eta\)</span> is 0.1.</p>
<p>==Iteration 1==</p>
<p>Forward Propagation <span class="math display">\[
e(1)=d^{(1)}-w(1)^Tx^{(1)}=3.5-2=1.5
\]</span> Backward Propagation <span class="math display">\[
w(2)=w(1)+\eta x^{(1)}e(1)=[2.15\quad 0.15]^T
\]</span> ==Iteration 2==</p>
<p>Forward Propagation <span class="math display">\[
e(2)=d^{(2)}-w(2)^Tx^{(2)}=0.125
\]</span> Backward Propagation <span class="math display">\[
w(3)=w(2)+\eta x^{(2)}e(2)=[2.1625\quad 0.1688]^T
\]</span></p>
<h2 id="multilayer-perceptrons">4. Multilayer Perceptrons</h2>
<p>MLP generally adopts a smooth nonlinear activation function, such as the following logistic function</p>
<p>Sigmoid Function <span class="math display">\[
\varphi(v)=\frac{1}{1+e^{-av}}
\]</span></p>
<p>MLP is fed with an input vector <span class="math inline">\(x(n)\)</span>, and produces an output vector <span class="math inline">\(y(n)\)</span>, where <span class="math inline">\(n\)</span> is the iteration number.</p>
<p>If we have a three layers NN, for the third layer</p>
Let <span class="math inline">\(d(n)\)</span> denote the desired network output, and then the error is <span class="math display">\[
e(n)=d(n)-y(n)=d(n)-x^{(3)}_{out}(n)
\]</span> Then the cost function is <span class="math display">\[
E(n)=\frac{1}{2}\sum_{j=1}^{n_3}(d_j(n)-x_{out,j}^{(3)}(n))^2
\]</span> where <span class="math inline">\(n_3\)</span> means the number of neurons in the third layer <span class="math display">\[
x^{(3)}_{out,j}=\varphi(v^{(3)}_{j})\\
v^{(3)}_{j} = \sum_i w_{ji}^{(3)}x^{(2)}_i
\]</span> Then we have $$
<span class="math display">\[\begin{array}{rcl}
\Delta w^{(3)}_{ji}(n)&amp;=&amp;-\eta\frac{\part E(n)}{\part w_{ji}^{(3)}(n)}\\
&amp;=&amp;\eta(d_j(n)-x^{(3)}_{out,j})\frac{\part x^{(3)}_{out,j}}{\part w_{ji}^{(3)}(n)}\\
&amp;=&amp;\eta(d_j(n)-x^{(3)}_{out,j})\frac{\part x^{(3)}_{out,j}}{\part v_j^{(3)}}
\frac{\part v_j^{(3)}}{\part w_{ji}^{(3)}(n)}\\

&amp;=&amp; \eta \delta_j^{(3)}(n)x_{out,i}^{(2)}(n)
\end{array}\]</span>
<p>$$</p>
<p>where <span class="math display">\[
\delta_j^{(3)}(n)=e_j(n)\dot \varphi^{(3)}(v_j^{(3)}(n))
\]</span> For the second layer,</p>
<p>Let d(n) denote the desired network output, and the error is then <span class="math display">\[
e(n)=d(n)-y(n)=d(n)-x^{(3)}_{out}(n)
\]</span> Then the cost function is <span class="math display">\[
E(n)=\frac{1}{2}\sum_{k=1}^{n_3}(d_k(n)-x_{out,k}^{(3)}(n))^2
\]</span> where <span class="math inline">\(n_3\)</span> means the number of neurons in the third layer $$ x^{(3)}<em>{out,k}=(v^{(3)}</em>{k})\ v^{(3)}_{k} = <em>j w</em>{kj}<sup>{(3)}x</sup>{(2)}_j\</p>
<p>x^{(2)}<em>{out,j}=(v^{(2)}</em>{j})\ v^{(2)}_{j} = <em>i w</em>{ji}<sup>{(2)}x</sup>{(1)}<em>i <span class="math display">\[
Then we have
\]</span> w<sup>{(2)}<em>{ji}(n)=-\ =</em>{k=1}</sup>{n_3}(d_k(n)-x^{(3)}</em>{out,k}) \</p>
<p>=<em>{k=1}<sup>{n_3}(d_k(n)-x</sup>{(3)}</em>{out,k})  \</p>
<p>=<em>{k=1}<sup>{n_3}(d_k(n)-x</sup>{(3)}</em>{out,k}) <sup>{(3)}(v_k</sup>{(3)}) w_{kj}^{(3)} \</p>
<p>=<em>{k=1}<sup>{n_3}(d_k(n)-x</sup>{(3)}</em>{out,k}) <sup>{(3)}(v_k</sup>{(3)}(n)) w_{kj}^{(3)}  \</p>
<p>=<em>{k=1}<sup>{n_3}(d_k(n)-x</sup>{(3)}</em>{out,k}) <sup>{(3)}(v_k</sup>{(3)}(n)) w_{kj}^{(3)} <sup>{(2)}(v_j</sup>{(2)}(n)) x_{out,i}^{(1)}\</p>
<p>=<em>{k=1}<sup>{n_3}<em>k^{(3)} w</em>{kj}</sup>{(3)} <sup>{(2)}(v_j</sup>{(2)}(n)) x</em>{out,i}^{(1)}\</p>
<p>= <em>j<sup>{(2)}(n)x_{out,i}</sup>{(1)}(n) <span class="math display">\[
where
\]</span> <em>j^{(2)}(n)= (</em>{k=1}<sup>{n_3}<em>k^{(3)} w</em>{kj}</sup>{(3)}) <sup>{(2)}(v_j</sup>{(2)}(n))\ <span class="math display">\[
Then we can summary
\]</span> w</em>{ji}<sup>{(s)}(n+1)=w_{ji}</sup>{(s)}(n)+_j<sup>{(s)}(n)x_{out,i}</sup>{(s-1)}(n) <span class="math display">\[
where
\]</span> _j<sup>{(s)}(n)=e_j(n)</sup>{(s)}(v_j^{(s)}(n))\</p>
<p><em>j^{(s)}(n)= (</em>{k=1}<sup>{n_{s+1}}<em>k^{(s+1)}(n) w</em>{kj}</sup>{(s+1)}(n)) <sup>{(s)}(v_j</sup>{(s)}(n)) \ <span class="math display">\[
To increase learning speed, modify the delta rule
\]</span> w_{ji}<sup>{(s)}(k)=w_{ji}</sup>{(s)}(k-1) + <em>j<sup>{(s)}(k)x_{out,i}</sup>{(s-1)}(k)\ w</em>{ji}<sup>{(s)}(n+1)=w_{ji}</sup>{(s)}(n)+w_{ji}^{(s)}(k) $$</p>
<h2 id="mlp-design-and-training-issues">5. MLP Design and Training Issues</h2>
<p>Many design and training issues to consider</p>
<h3 id="sequential-mode-or-batch-mode">1. Sequential mode or batch mode</h3>
<p>For sequential mode, weight updating is performed after each presentation of the training example. N times adjustments in one epoch.</p>
<p>For batch mode, update weight after the presentation of all the training examples. One time adjustment in one epoch.</p>
<p>Sequential learning is generally the preferred one since sequential learning often results in better solutions. Nonlinear networks often have multiple local minima. Batch learning can easily get trapped into the local minimum.</p>
<p>But sequential mode always causes fluctuation because of the noise.</p>
<p>One way to reduce fluctuation is to use mini-batches.</p>
<p>That is to start with a small batch size and increase the size as the training proceeds.</p>
<p>Second Order Method <span class="math display">\[
E(w(n+1))=E(w(n)+\Delta w(n))=E(w(n))+g^T(n)\Delta w(n)+\frac{1}{2}\Delta w^T(n)H(n)\Delta w(n)
\]</span> If we hope to get choose <span class="math inline">\(\Delta w(n)\)</span> to let <span class="math inline">\(E(w(n+1))\)</span> be minimized <span class="math display">\[
\frac{\part E(w(n+1))}{\part \Delta w(n)}=g^T(n)+\Delta w^T(n)H(n)=0\\
\Delta w(n) = -H^{-1}(n)g(n)
\]</span> This is so called Newton's method.</p>
<h3 id="normalize-the-input-variables">2. Normalize the input variables</h3>
<p>In general, any shift of the average input away from zero will bias the updates in a particular direction and this slow down learning.</p>
<p>Normalize <span class="math display">\[
\bar x_i = \frac{\sum_{n=1}^Nx_i(n)}{N}\quad \sigma=\sqrt \frac{\sum_{n=1}^N (x_i(n)-\bar x_i)}{N}\quad x_i&#39;(n) = \frac{x_i(n)-\bar x_i}{\sigma}
\]</span> Let the range are close to [-1, 1]</p>
<h3 id="how-to-choose-activation-functions-in-hidden-layers">3. How to choose activation functions in hidden layers</h3>
<p>For the hidden layers, <span class="math display">\[
\tan sig(x)=\frac{2}{1+e^{-2x}}-1
\]</span> is better than <span class="math display">\[
\log sig(x)=\frac{1}{1+e^{-x}}
\]</span> Every neuron output is closed to [-1, 1]</p>
<p>However, for the output layer, if the problem is pattern recognition problem <span class="math display">\[
\log sig(x)=\frac{1}{1+e^{-x}}
\]</span> is better because the output is 0 or 1</p>
<p>For regression problem, using linear neuron at the output layer would be more flexible.</p>
<h3 id="how-to-choose-target-values-for-pattern-recognition">4. How to choose target values for pattern recognition</h3>
<p>For the sigmoid function, the training process will try to derive the output as close as possible to the target value, that can cause the input to the output layer to be bigger and bigger in the training process. Therefore, it is better to choose [-0.6, 0.6] for tansig and [0.2, 0.8] for logsig.</p>
<h3 id="how-many-hidden-layers-and-how-many-hidden-neurons-in-each-layer">5. How many hidden layers AND how many hidden neurons in each layer</h3>
<p>Adding more layers can reduce the parameters but may make the MLP more prone to local minima traps because of its more complicated structure.</p>
<h3 id="how-to-avoid-over-fitting">6. How to avoid over-fitting</h3>
<p>One way is to limit the number of hidden units. It is important to seek the minimal structure of the MLP</p>
<p>For valid generation, the size of the weights is more important than the size of the network</p>
<p>Use regularization techniques</p>
<h3 id="use-singular-value-decomposition-svd-to-decide-the-minimal-structure">7. Use Singular Value Decomposition (SVD) to decide the minimal structure</h3>
<p>Calculate the outputs of the hidden neurons for each training sample input vector <span class="math inline">\(x(k)\)</span> <span class="math display">\[
h_{ki} = f_i(\sum_{j=1}^M w_{ij}x_j(k)+b_i)\in H
\]</span> Use SVD to get the effective rank of H. The threshold is always chosen as 0.95 or 0.99.</p>
<p>Train the MLP on the N training samples (<span class="math inline">\(X_{N\times M}\)</span>) with sufficiently large number of <span class="math inline">\(n\)</span> hidden neurons until converge.</p>
<p>Calculate the effective rank.</p>
<p>Train the MLP with number of effective rank hidden neurons until converge.</p>
<p>Repeat the procedure until convergence.</p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/12/22/EE5104 Computer Control/8. MPC with Constraint/" rel="next" title="8. MPC with Constraint">
                  <i class="fa fa-chevron-left"></i> 8. MPC with Constraint
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/12/22/EE5904 Neural Network/2.RBFN/" rel="prev" title="">
                   <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#what-is-learning-in-nn"><span class="nav-text">1. What is Learning in NN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#perceptron"><span class="nav-text">2. Perceptron</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linear-regression-problem"><span class="nav-text">3. Linear Regression Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multilayer-perceptrons"><span class="nav-text">4. Multilayer Perceptrons</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mlp-design-and-training-issues"><span class="nav-text">5. MLP Design and Training Issues</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sequential-mode-or-batch-mode"><span class="nav-text">1. Sequential mode or batch mode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#normalize-the-input-variables"><span class="nav-text">2. Normalize the input variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-choose-activation-functions-in-hidden-layers"><span class="nav-text">3. How to choose activation functions in hidden layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-choose-target-values-for-pattern-recognition"><span class="nav-text">4. How to choose target values for pattern recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-many-hidden-layers-and-how-many-hidden-neurons-in-each-layer"><span class="nav-text">5. How many hidden layers AND how many hidden neurons in each layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-avoid-over-fitting"><span class="nav-text">6. How to avoid over-fitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#use-singular-value-decomposition-svd-to-decide-the-minimal-structure"><span class="nav-text">7. Use Singular Value Decomposition (SVD) to decide the minimal structure</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
