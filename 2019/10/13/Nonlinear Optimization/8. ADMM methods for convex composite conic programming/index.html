<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="8. ADMM methods for convex composite conic programming 8.1 A generic 2-block semi-proximal ADMM Let \(\mathcal X\), \(\mathcal Y\), and \(\mathcal Z\) be finite dimensional real Euclidean spaces. Le">
<meta property="og:type" content="article">
<meta property="og:title" content="8. ADMM methods for convex composite conic programming (1)">
<meta property="og:url" content="http://yoursite.com/2019/10/13/Nonlinear Optimization/8. ADMM methods for convex composite conic programming/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:description" content="8. ADMM methods for convex composite conic programming 8.1 A generic 2-block semi-proximal ADMM Let \(\mathcal X\), \(\mathcal Y\), and \(\mathcal Z\) be finite dimensional real Euclidean spaces. Le">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-10-31T14:11:04.837Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="8. ADMM methods for convex composite conic programming (1)">
<meta name="twitter:description" content="8. ADMM methods for convex composite conic programming 8.1 A generic 2-block semi-proximal ADMM Let \(\mathcal X\), \(\mathcal Y\), and \(\mathcal Z\) be finite dimensional real Euclidean spaces. Le">
  <link rel="canonical" href="http://yoursite.com/2019/10/13/Nonlinear Optimization/8. ADMM methods for convex composite conic programming/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>8. ADMM methods for convex composite conic programming (1) | Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/13/Nonlinear Optimization/8. ADMM methods for convex composite conic programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">8. ADMM methods for convex composite conic programming (1)

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-10-13 01:03:55" itemprop="dateCreated datePublished" datetime="2019-10-13T01:03:55+08:00">2019-10-13</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-10-31 22:11:04" itemprop="dateModified" datetime="2019-10-31T22:11:04+08:00">2019-10-31</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2019/10/13/Nonlinear Optimization/8. ADMM methods for convex composite conic programming/" class="post-meta-item leancloud_visitors" data-flag-title="8. ADMM methods for convex composite conic programming (1)" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/10/13/Nonlinear Optimization/8. ADMM methods for convex composite conic programming/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/10/13/Nonlinear Optimization/8. ADMM methods for convex composite conic programming/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="admm-methods-for-convex-composite-conic-programming">8. ADMM methods for convex composite conic programming</h1>
<h2 id="a-generic-2-block-semi-proximal-admm">8.1 A generic 2-block semi-proximal ADMM</h2>
<p>Let <span class="math inline">\(\mathcal X\)</span>, <span class="math inline">\(\mathcal Y\)</span>, and <span class="math inline">\(\mathcal Z\)</span> be finite dimensional real Euclidean spaces. Let <span class="math inline">\(f:\mathcal Y\rightarrow (-\infty,+\infty]\)</span> and <span class="math inline">\(g:\mathcal Z\rightarrow (-\infty,+\infty]\)</span> be closed proper convex functions, <span class="math inline">\(\mathcal A:\mathcal X\rightarrow \mathcal Y\)</span> and <span class="math inline">\(\mathcal B:\mathcal X\rightarrow \mathcal Z\)</span> be linear maps. Let <span class="math inline">\(\partial f\)</span> and <span class="math inline">\(\partial g\)</span> be the subdifferential mappings of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, respectively. Since both <span class="math inline">\(\partial f\)</span> and <span class="math inline">\(\partial g\)</span> are maximally monotone, there exists two self-adjoint and positive semidefinite operators <span class="math inline">\(\Sigma_f\)</span> and <span class="math inline">\(\Sigma_g\)</span> such that for all <span class="math inline">\(y,y’\in\text{dom}(f)\)</span>, <span class="math inline">\(\xi\in\partial f(y)\)</span>, and <span class="math inline">\(\xi&#39;\in\partial f(y&#39;)\)</span>, <span class="math display">\[
\langle \xi-\xi&#39;,y-y&#39;\rangle \ge ||y-y&#39;||^2_{\Sigma_f}
\]</span> and for all <span class="math inline">\(z,z’\in\text{dom}(g)\)</span>, <span class="math inline">\(\zeta\in\partial g(z)\)</span>, and <span class="math inline">\(\zeta&#39;\in\partial g(z&#39;)\)</span>, <span class="math display">\[
\langle \zeta-\zeta&#39;,z-z&#39;\rangle \ge ||z-z&#39;||^2_{\Sigma_g}.
\]</span> Consider the convex optimization problem with the following 2-block separable structure <span class="math display">\[
\text{(P)}\quad \begin{array}{rCl}
\min &amp; f(y)+g(z)\\
\text{subject to}&amp; \mathcal A^*y+\mathcal B^*z=c.
\end{array}
\]</span> The dual problem is given by <span class="math display">\[
\text{(D)}\quad \max\left\{-\langle c,x\rangle-f^*(-\mathcal Ax)-g^*(-\mathcal Bx)\right\}.
\]</span> The augmented Lagrangian function associated with (P) is given as follows, for any <span class="math inline">\((y,z,x)\in\mathcal Y\times \mathcal Z\times \mathcal X\)</span>, <span class="math display">\[
\begin{array}{rcl}
\mathcal L_\sigma(y,z;x)&amp;=&amp;f(y)+g(z)+\langle x,\mathcal A^*y+\mathcal B^*z-c\rangle+\frac{\sigma}{2}||\mathcal A^*y+\mathcal B^*z-c||^2\\
&amp;=&amp; f(y)+g(z)+\frac{\sigma}{2}||\mathcal A^*y+\mathcal B^*z-c+\sigma^{-1}x||^2-\frac{1}{2\sigma}||x||^2
\end{array}
\]</span> <strong>Algorithm</strong></p>
<p>Let <span class="math inline">\(\sigma&gt;0\)</span> and <span class="math inline">\(\tau \in(0,\infty)\)</span> be given parameters. Let <span class="math inline">\(\mathcal S\)</span> and <span class="math inline">\(\mathcal T\)</span> be given self-adjoint positive semidefinite (not necessarily positive definite) linear operators defined on <span class="math inline">\(\mathcal Y\)</span> and <span class="math inline">\(\mathcal Z\)</span>, respectively. Choose <span class="math inline">\((x^0,y^0,z^0)\in\mathcal X\times \text{dom}(f)\times \text{dom}(g)\)</span>. For <span class="math inline">\(k=0,1,2,\dotsm\)</span>, perform the <span class="math inline">\(k\)</span>th iteration as follows,</p>
<p><strong>Step 1</strong> <span class="math display">\[
\begin{array}{rcl}
y^{k+1}&amp;=&amp;\arg\min_y\mathcal L_\sigma(y,z^k;x^k)+\frac{1}{2}||y-y^k||^2_\mathcal S\\
&amp;=&amp;\arg\min_y\mathcal f(y)+g(z^{k})+\frac{\sigma}{2}||\mathcal A^*y+\mathcal B^*z^{k}-c+\sigma^{-1}x^{k}||^2-\frac{1}{2\sigma}||x^{k}||^2+\frac{1}{2}||y-y^k||^2_\mathcal S\\
&amp;=&amp;\arg\min_y\mathcal f(y)+\frac{\sigma}{2}||\mathcal A^*y+\mathcal B^*z^{k}-c+\sigma^{-1}x^{k}||^2+\frac{1}{2}||y-y^k||^2_\mathcal S.
\end{array}
\]</span></p>
<p><strong>Step 2</strong> <span class="math display">\[
   z^{k+1}=\arg\min_z\mathcal L_\sigma(y^{k+1},z;x^k)+\frac{1}{2}||z-z^k||^2_\mathcal T.
\]</span> <strong>Step 3</strong> <span class="math display">\[
   x^{k+1}=x^k+\tau\sigma(\mathcal A^*y^{k+1}+\mathcal B^* z^{k+1}-c).
\]</span> ### 8.1.1 Literature review</p>
<p>When <span class="math inline">\(\mathcal S=0\)</span> and <span class="math inline">\(\mathcal T=0\)</span>, algorithm sPADMM reduces to the classic 2-block ADMM for solving the problem.</p>
<h3 id="convergence-of-spadmm">8.1.2 Convergence of sPADMM</h3>
<p>For the convergence of the 2-block semi-proximal ADMM, we need the following assumption.</p>
<p><strong>Assumption</strong></p>
<p>There exists <span class="math inline">\((\hat y,\hat z)\in \text{relint}(\text{dom}(f)\times\text{dom}(g))\)</span> such that <span class="math inline">\(\mathcal A^*\hat y+\mathcal B^*\hat z=c\)</span>.</p>
<p>Suppose that the CQ holds. Then we know that <span class="math inline">\((\bar y,\bar z)\in\text{dom}(f)\times \text{dom}(g)\)</span> is an optimal solution to the problem if and only if there exists a Lagrange multiplier <span class="math inline">\(\bar x\in\mathcal X\)</span> such that <span class="math inline">\((\bar x,\bar y,\bar z)\)</span> is a solution to the following KKT system, <span class="math display">\[
0\in\mathcal Ax+\partial f(y)\\
0\in\mathcal Bx+\partial g(z)\\
\mathcal A^*y+\mathcal B^*z-c=0.
\]</span> For simplicity, we define <span class="math display">\[
\begin{cases}
H(y,z)=\mathcal A^*y+\mathcal B^*z\\
\theta (x,y,z;x&#39;,y&#39;,z&#39;)=(\tau \sigma)^{-1}||x-x&#39;||^2+||y-y&#39;||^2_\mathcal S+||z-z&#39;||^2_{\mathcal T}+\sigma||\mathcal B^*(z-z&#39;)||^2.
\end{cases}
\]</span> <strong>Theorem</strong></p>
<p>Suppose that the solution set of problem is nonempty and the assumption holds.</p>
<p>Assume that <span class="math inline">\(\mathcal S\)</span> and <span class="math inline">\(\mathcal T\)</span> are chosen such that the sequence <span class="math inline">\(\{(x^k,y^k,z^k)\}\)</span> generated by sPADMM is well defined.</p>
<p>Let <span class="math inline">\((\bar x,\bar y,\bar z)\)</span> be a solution to the KKT system.</p>
<p>For <span class="math inline">\(k=0,1,\dotsm\)</span>, we denote <span class="math display">\[
\begin{cases}
x_e^k=x^k-\bar x\\
y_e^k=y^k-\bar y\\
z_e^k=z^k-\bar z,
\end{cases}
\]</span> and <span class="math display">\[
\begin{cases}
\psi_{k+1}=\theta(x^{k+1},y^{k+1},z^{k+1};\bar x,\bar y,\bar z)+||z^{k+1}-z^k||^2_\mathcal T\\
\delta_{k+1}=\min(\tau,1+\tau-\tau^2)\sigma||\mathcal B^*(z^{k+1}-z^k)||^2+||z^{k+1}-z^k||^2_\mathcal T\\
t_{k+1}=\delta_{k+1}+||y^{k+1}-y^k||^2_\mathcal S+2||y^{k+1}-\bar y||^2_{\Sigma_f}+2||z^{k+1}-\bar z||^2_{\Sigma_g}.
\end{cases}
\]</span> Then the following results hold,</p>
<ol type="1">
<li>if <span class="math inline">\(\tau\in(0,1]\)</span>, we have for <span class="math inline">\(k\ge 1\)</span> that <span class="math display">\[
\begin{array}{l}
\left[\psi_{k+1}+(1-\tau)\sigma||H(y^{k+1},z^{k+1})-c||^2\right]\\
\quad-\left[\psi_k+(1-\tau)\sigma||H(y^k,z^k)-c||^2\right]\\
\le -\left[t_{k+1}+\sigma||H(y^{k+1},z^{k+1})-c||^2\right];
\end{array}
\]</span></li>
<li>If <span class="math inline">\(\tau &gt;1\)</span>, we have for <span class="math inline">\(k\ge 1\)</span> that <span class="math display">\[
\begin{array}{l}
\left[\psi_{k+1}+(1-\tau^{-1})\sigma||H(y^{k+1},z^{k+1})-c||^2\right]\\
\quad-\left[\psi_k+(1-\tau^{-1})\sigma||H(y^k,z^k)-c||^2\right]\\
\le -\left[t_{k+1}+\tau^{-1}(1+\tau-\tau^{-2})\sigma||H(y^{k+1},z^{k+1})-c||^2\right];
\end{array}
\]</span></li>
<li><p>Suppose the either of the following condition holds,</p>
<ol type="1">
<li><span class="math inline">\(\tau\in(0,(1+\sqrt5)/2)\)</span></li>
<li><span class="math inline">\(\tau \ge (1+\sqrt 5)/2\)</span> and <span class="math inline">\(\displaystyle\sum_{k=0}^\infty ||x^{k+1}-x^k||^2&lt;\infty\)</span>.</li>
</ol>
<p>Then the sequence <span class="math display">\[
    \left\{||x_e^{k+1}||^2+||y_{e}^{k+1}||^2_{\Sigma_f+\mathcal S+\sigma\mathcal A\mathcal A^*}+||z_e^{k+1}||^2_{\Sigma_g+\mathcal T+\sigma\mathcal B\mathcal B^*}\right\}
\]</span> is bounded.</p>
<p>Moreover, if <span class="math inline">\((x^\infty,y^\infty,z^\infty)\)</span> is an accumulation point of <span class="math inline">\(\{(x^k,y^k,z^k)\}\)</span>. Then <span class="math inline">\((x^\infty,y^\infty,z^\infty)\)</span> is a solution of the KKT system, and <span class="math display">\[
    \lim_{k\rightarrow \infty}\left(||x_e^{k+1}||^2+||y_{e}^{k+1}||^2_{\Sigma_f+\mathcal S+\sigma\mathcal A\mathcal A^*}+||z_e^{k+1}||^2_{\Sigma_g+\mathcal T+\sigma\mathcal B\mathcal B^*}\right)=0,
\]</span> where in the definition of <span class="math inline">\((x^k_e,y^k_e,z^k_e)\)</span>, the point is replaced by <span class="math inline">\((x^\infty,y^\infty,z^\infty)\)</span>. Consequently, if both <span class="math inline">\(\Sigma_f+\mathcal S+\sigma\mathcal A\mathcal A^*\)</span> and <span class="math inline">\(\Sigma_g+\mathcal T+\sigma\mathcal B\mathcal B^*\)</span> are positive definite, then the sequence <span class="math inline">\(\{(x^k,y^k,z^k)\}\)</span>, which is automatically well-defined, converge to a unique limit, say <span class="math inline">\((x^\infty,y^\infty,z^\infty)\)</span> with <span class="math inline">\((y^\infty,z^\infty)\)</span> solving (P) and <span class="math inline">\((x^\infty)\)</span> solving (D), respectively.</p></li>
<li><p>If <strong>y-part</strong> is vacuous, then we have that for all <span class="math inline">\(k\ge0\)</span>, <span class="math display">\[
    \begin{array}{l}
    (\tau\sigma)^{-1}||x^{k+1}-\bar x||^2+||z^{k+1}-\bar z||^2_\mathcal T\\
    \le (\tau\sigma)^{-1}||x^{k}-\bar x||^2+||z^{k}-\bar z||^2_\mathcal T\\
    \quad-\left[(2-\tau)\sigma||\mathcal B^*z^{k+1}-c||^2+||z^{k+1}-z^k||^2_\mathcal T+2||z^{k+1}-\bar z||^2_{\Sigma_g}\right].
    \end{array}
 \]</span> The corresponding result in part 3 also holds.</p></li>
</ol>
<p>Proof.</p>
<p>Since both <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are proper closed convex functions and an infinite sequence <span class="math inline">\(\{(x^k,y^k,z^k)\}\)</span> is assumed to be generated by sPADMM, we have for <span class="math inline">\(k&gt;0\)</span>, by using the algorithm, <span class="math display">\[
\begin{array}{l}
0\in\mathcal Ax^{k}+\partial f(y^{k+1})+\mathcal S(y^{k+1}-y^k)\\
0\in\mathcal Bx^{k}+\partial g(z^{k+1})+\mathcal T(z^{k+1}-z^k)\\
x^{k+1}=x^k+\tau\sigma(\mathcal A^*y^{k+1}+\mathcal B^* z^{k+1}-c)\\
\;\:\:\:\;\;\;\:\:=x^k+\tau\sigma(H(y^{k+1},z^{k+1})-c)\\
0=H(y^{k+1},z^{k+1})-c-(\tau\sigma)^{-1}(x^{k+1}-x^k),
\end{array}
\]</span> we have</p>
<p>Not done yet.</p>
<h3 id="admm-for-solving-the-dual-problem-d-of-the-sparse-regression-problem">8.1.3 ADMM for solving the dual problem (D) of the sparse regression problem</h3>
<p>Consider the sparse regression problem, <span class="math display">\[
\min\left\{\frac{1}{2}||Ax-b||^2+\lambda||x||_1\;|\; x\in\mathbb R^n\right\},
\]</span> where <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span> and <span class="math inline">\(b\in\mathbb R^m\)</span> are given data. The equivalent problem can be denoted as <span class="math display">\[
\text{(P)}\quad \begin{array}{rCl}
\min &amp; f(u)+g(x)\\
\text{subject to}&amp; u+Ax=b.
\end{array}
\]</span> where <span class="math inline">\(f(u)=\frac{1}{2}||u||^2\)</span> and <span class="math inline">\(g(x)=\lambda||x||_1\)</span>. Then we consider how to derive the dual problem.</p>
<p>The Lagrangian function can be denoted as <span class="math display">\[
\begin{array}{rcl}
\mathcal L(u,x;\xi)&amp;=&amp;f(u)+g(x)+\langle \xi,b-u-Ax\rangle\\
&amp;=&amp; f(u)-\langle \xi,u\rangle+g(x)-\langle A^T\xi,x\rangle +\langle \xi,b\rangle.
\end{array}
\]</span> The Lagrangian dual function can be denoted as <span class="math display">\[
\begin{array}{rcl}
\min_{u,x}\mathcal L(u,x;\xi) &amp;=&amp; \langle \xi,b\rangle+\min_u \left\{f(u)-\langle \xi,u\right\rangle\}+\min_x\{g(x)-\langle A^T\xi,x\rangle \}\\ 
&amp;=&amp; \langle \xi,b\rangle-\frac{1}{2}||\xi||^2-\delta_{B_\lambda}(A^T\xi),
\end{array}
\]</span> where <span class="math inline">\(B_\lambda =\{v\in\mathbb R^n\;|\; ||v||_\infty\le \lambda\}\)</span>.</p>
<p>The Lagrangian dual problem can be denoted as (we define <span class="math inline">\(v+A^T\xi=0\)</span>) <span class="math display">\[
\text{(D)}\quad \begin{array}{rCl}
\max &amp; \langle \xi,b\rangle-\frac{1}{2}||\xi||^2-\delta_{B_\lambda}(v)\\
\text{subject to}&amp; A^T\xi+v=0
\end{array}\\\iff\\
\text{(D)}\quad \begin{array}{rCl}
\min &amp; -\langle \xi,b\rangle+\frac{1}{2}||\xi||^2+\delta_{B_\lambda}(v)\\
\text{subject to}&amp; A^T\xi+v=0.
\end{array}
\]</span> Now we can apply the 2-block ADMM to solve (D). In this case, we have <span class="math inline">\(f(\xi)=-\langle b,\xi\rangle+\frac{1}{2}||\xi||^2\)</span>, <span class="math inline">\(g(v)=\delta_{B_{\lambda}}(v)\)</span>, <span class="math inline">\(\mathcal B=I\)</span> and <span class="math inline">\(c=0\)</span>. The augmented Lagrangian function is given by</p>
<p><span class="math display">\[
\begin{array}{rcl}
\mathcal L_\sigma(\xi,u;x) &amp;=&amp; -\langle b,\xi\rangle+\frac{1}{2}||\xi||^2+\delta_{B_{\lambda}}(v)+\langle x,A^T\xi+v\rangle+\frac{\sigma}{2}||A^T\xi+v||^2\\
&amp;=&amp;-\langle b,\xi\rangle+\frac{1}{2}||\xi||^2+\delta_{B_{\lambda}}(v)+\frac{\sigma}{2}||A^T\xi+v+\sigma^{-1}x||^2-\frac{1}{2\sigma}||x||^2.
\end{array}
\]</span> Then we can use 2-block ADMM to solve this problem.</p>
<p><strong>Step 1</strong> <span class="math display">\[
\begin{array}{rcl}
\xi^{k+1}&amp;=&amp; \arg\min _\xi \mathcal L_\sigma (\xi,v^k;x^k)\\
&amp;=&amp; \arg\min_\xi \left\{-\langle b,\xi\rangle+\dfrac{1}{2}||\xi||^2+\dfrac{\sigma}{2}||A^T\xi+v+\sigma^{-1}x||^2\right\}.
\end{array}
\]</span> From the optimality condition, we know that <span class="math inline">\(\xi^{k+1}\)</span> is the solution of the subproblem <span class="math display">\[
(I+\sigma A A^T)\xi=b-\sigma A(v^k+\sigma^{-1} x^k).
\]</span> <strong>Step 2</strong> <span class="math display">\[
\begin{array}{rcl}
v^{k+1}&amp;=&amp; \arg\min _v \mathcal L_\sigma (\xi^{k+1},v;x^k)\\
&amp;=&amp; \arg\min_v \left\{\delta_{B_{\lambda}}(v)+\frac{\sigma}{2}||A^T\xi+v+\sigma^{-1}x||^2\right\}\\
&amp;=&amp; \Pi_{B_{\lambda}}(-A^T\xi^{k+1}-\sigma^{-1}x^k).
\end{array}
\]</span> <strong>Step 3</strong> <span class="math display">\[
x^{k+1}=x^k+\tau\sigma (A^* \xi^{k+1}+v^{k+1}),
\]</span> where <span class="math inline">\(\tau \in(0,\frac{1+\sqrt 5}{2})\)</span> is the stepsize. In practice, for faster convergence, we take <span class="math inline">\(\tau = 1.618\)</span>.</p>
<p><strong>Step 4</strong></p>
<p>For a given tolerance <span class="math inline">\(\varepsilon\)</span>, check the relative KKT residual, if <span class="math display">\[
\max\left\{\frac{||R_p^{k+1}||}{1+||b||},\frac{||R_d^{k+1}||}{1+||v^k||},\frac{||R_c^{k+1}||}{1+||x^k||+||v^k||}\right\}\le \varepsilon,
\]</span> stop.</p>
<p>The KKT conditions are <span class="math display">\[
Ax-b+\xi=0,\quad 0\in x+\partial g(v),\quad A^T\xi+v=0.
\]</span></p>
<p>Thus to measure whether the computed iterate <span class="math inline">\((\xi^k,v^k,x^k)\)</span> is a good solution, we compute the KKT residual, <span class="math display">\[
\begin{array}{rcl}
R_p^k&amp;=&amp;||Ax^k+\xi^k-b||,\\
R_d^k&amp;=&amp;||A^T\xi^k+v^k||,\\
R_c^k&amp;=&amp;|| v^k-\Pi_{B_\lambda}(v^k-x^k)||.
\end{array}
\]</span></p>
<p><strong>Remark</strong></p>
<p>If <span class="math inline">\(||A^Tb||_\infty \le \lambda\)</span>, then we have <span class="math inline">\(x^*=0\)</span>, <span class="math inline">\(\xi^*=b\)</span>, and <span class="math inline">\(v^*=-A^Tb\)</span>. Since we have <span class="math inline">\(v^*\in B_\lambda\)</span>, we have <span class="math display">\[
v^*=\Pi_{B_\lambda}(v^*)=\Pi_{B_\lambda}(v^*-x^*).
\]</span></p>
<h3 id="linearized-admm-for-solving-the-primal-problem-p-of-the-sparse-regression-problem">8.1.4 Linearized ADMM for solving the primal problem (P) of the sparse regression problem</h3>
<p>One may attempt to apply the 2-block ADMM to the primal problem (P). But we will see that the subproblem corresponding to <span class="math inline">\(x^{k+1}\)</span> is very difficult to solve.</p>
<p>The augmented Lagrangian function associated with (P) is given by <span class="math display">\[
\begin{array}{rcl}
\mathcal L_\sigma (u,x;y) &amp;=&amp; f(u)+g(x)+\langle y,u+Ax-b\rangle +\frac{\sigma}{2}||u+Ax-b||^2\\
&amp;=&amp; f(u)+g(x)+\frac{\sigma}{2}||u+Ax-b+\sigma^{-1}y||^2-\frac{1}{2\sigma}||y||^2.
\end{array}
\]</span> The linearized 2-block ADMM for solving the problem (P) is given as follows.</p>
<p><strong>Step 1</strong> <span class="math display">\[
\begin{array}{rcl}
u^{k+1}&amp;=&amp; \arg\min _u\mathcal L_\sigma (u,x^k;y^k)\\
&amp;=&amp; \arg\min _u \{\frac{1}{2}||u||^2+\frac{\sigma}{2}||u+Ax^k-b+\sigma^{-1}y^k||^2\}.
\end{array}
\]</span> Then we have <span class="math display">\[
u^{k+1}=\frac{-\sigma}{1+\sigma}(Ax^k-b+\sigma^{-1}y^k).
\]</span> <strong>Step 2</strong> <span class="math display">\[
\begin{array}{rcl}
x^{k+1}&amp;=&amp;\arg \min_{x} \left\{ L_\sigma (u^{k+1},x;y^k)+\frac{\sigma}{2}||x-x^k||^2_\mathcal T\right\}\\
&amp;=&amp; \arg\min_x \left\{\lambda ||x||_1+\frac{\sigma}{2}||u^{k+1}+Ax-b+\sigma^{-1}y^k||^2+\frac{\sigma}{2}||x-x^k||^2_\mathcal T\right\}.
\end{array}
\]</span> Observe that if <span class="math inline">\(\mathcal T=0\)</span>, solving the subproblem is as difficult as solving the original problem. Suppose we choose <span class="math display">\[
\mathcal T=\rho I-A^TA
\]</span> where <span class="math inline">\(\rho\)</span> denotes the largest eigenvalue of <span class="math inline">\(A^TA\)</span>. Such a choice of <span class="math inline">\(\mathcal T\)</span> reduces the semi-proximal ADMM to the so-called linearized ADMM. Then we reduce the subproblem to $$ \begin{array}{rcl} x^{k+1}&amp;=&amp;_x{ \begin{array}{l} ||x||<em>1+||Ax+u<sup>{k+1}-b+</sup>{-1}y<sup>k||</sup>2+||x-x<sup>k||</sup>2</em>T \end{array} }\</p>
<p>&amp;=&amp; _x{ \begin{array}{l} ||x||_1+(x,A<sup>TAx+2x,A</sup>T(u<sup>{k+1}-b+</sup>{-1}y<sup>k)\ +x,(I-A</sup>TA)x-2x,(I-A<sup>TA)x</sup>k ) \end{array} }\</p>
<p>&amp;=&amp; _x{ \begin{array}{l} ||x||_1+(x,x+2x,A<sup>T(u</sup>{k+1}-b+<sup>{-1}y</sup>k)-(I-A<sup>TA)x</sup>k) \end{array} }\</p>
<p>&amp;=&amp; _x{||x||_1+||x-h||<sup>2}(), \end{array} <span class="math display">\[
where
\]</span> h=x</sup>k-^{-1 }A<sup>T(u</sup>{k+1}+Ax<sup>k-b+</sup>{-1}y<sup>k). <span class="math display">\[
**Step 3**
\]</span> y</sup>{k+1}=y<sup>k+()(Ax</sup>{k+1}+u^{k+1}-b). $$</p>
<h2 id="directly-extended-admm-for-multi-block-convex-programming-problem">8.2 Directly extended ADMM for multi-block convex programming problem</h2>
<p>It is natural for one directly extend the 2-block ADMM. But it does not work.</p>
<h2 id="a-symmetric-gauss-seidel-decomposition-theorem">8.3 A symmetric Gauss-Seidel decomposition theorem</h2>
<p>Let <span class="math inline">\(s\ge 2\)</span> be a given integer and <span class="math inline">\(\mathcal X=\mathcal X_1\times \mathcal X_2\times \dotsm\times \mathcal X_s\)</span>. For any <span class="math inline">\(x\in\mathcal X\)</span> we write <span class="math inline">\(x=(x_1,x_2,\dotsm,x_s)\in\mathcal X\)</span>. Let <span class="math inline">\(\mathcal H:\mathcal X\rightarrow \mathcal X\)</span> be a given self-adjoint positive semidefinite linear operator. Consider the following decomposition <span class="math display">\[
\mathcal Hx = 
\left[\begin{array}{rcl}
\mathcal H_{11}&amp;\mathcal H_{12}&amp; \dotsm&amp; \mathcal H_{1s}\\
\mathcal H_{12}^*&amp;\mathcal H_{22}&amp; \dotsm&amp; \mathcal H_{2s}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\mathcal H_{1s}^*&amp;\mathcal H_{2s}^*&amp; \dotsm&amp; \mathcal H_{ss}
\end{array}\right]
\begin{bmatrix}
x_1\\x_2\\ \vdots \\ x_s
\end{bmatrix},
\]</span> where <span class="math inline">\(\mathcal H_{ii}:\mathcal X_i\rightarrow \mathcal X_i,\; i=1,\dotsm,s\)</span> are self-adjoint positive semidefinite linear operator, <span class="math inline">\(\mathcal H_{ij}:\mathcal X_j\rightarrow \mathcal X_i,\; i=1,\dotsm,s-1,j&gt;i\)</span> are linear maps. Here, we further assume that <span class="math display">\[
\mathcal H_{ii}\succ 0,\quad \forall i=1,\dotsm,s.
\]</span></p>
<p>Define <span class="math display">\[
x_{\le i}=(x_1,x_2,\dotsm,x_i),\; x_{\ge i}=(x_i,x_{i+1},\dotsm,x_s),\; i=0,\dotsm,s+1
\]</span> with the convention that <span class="math inline">\(x_{\le 0}=x_{\ge s+1}=\emptyset\)</span>.</p>
<p>Denote the linear operator <span class="math inline">\(\mathcal U,\mathcal D:\mathcal X\rightarrow \mathcal X\)</span> by <span class="math display">\[
\mathcal U=
\begin{bmatrix}
0 &amp; \mathcal H_{12} &amp; \dotsm &amp; \mathcal H_{1s}\\
&amp;\ddots &amp; \dotsm&amp;\vdots\\
&amp;&amp; 0 &amp;\mathcal H_{(s-1)s}\\
&amp;&amp;&amp;0
\end{bmatrix}\quad \mathcal D=\text{diag}(\mathcal H_{11},\mathcal H_{22},\dotsm,\mathcal H_{ss}).
\]</span> Then we have <span class="math inline">\(\mathcal H=\mathcal D+\mathcal U+\mathcal U^*\)</span> and <span class="math inline">\(\mathcal D\succ 0\)</span>. Let <span class="math inline">\(r=(r_1,r_2,\dotsm,r_s)\in\mathcal X\)</span> be given. Define the convex quadratic function <span class="math inline">\(h:\mathcal X\rightarrow \mathbb R\)</span> by <span class="math display">\[
h(x)=\frac{1}{2}\langle x,\mathcal Hx\rangle -\langle r,x\rangle ,\quad x\in \mathcal X.
\]</span> Let <span class="math inline">\(\phi:\mathcal X_1\rightarrow (-\infty,\infty]\)</span> be a given closed proper convex function. Define the following self-adjoint positive semidefinite linear operator <span class="math inline">\(\mathcal T:\mathcal X\rightarrow \mathcal X\)</span> as <span class="math display">\[
\mathcal T=\mathcal U\mathcal D^{-1}\mathcal U^*.
\]</span></p>
<p>Denote <span class="math inline">\(\delta&#39; = (\delta_1&#39;,\dotsm,\delta_s&#39;)\)</span> and <span class="math inline">\(\delta^+=(\delta_1^+,\dotsm,\delta_s^+)\)</span> with <span class="math inline">\(\delta_1&#39;=\delta_1^+\)</span>. Denote <span class="math display">\[
\Delta(\delta&#39;,\delta^+)=\delta&#39;+(\mathcal D+\mathcal U)\mathcal D^{-1}(\delta^+-\delta&#39;).
\]</span> Let <span class="math inline">\(\bar x\)</span> be given. Define <span class="math display">\[
\begin{array}{rcl}
x^+ &amp;=&amp;\arg\min_x\left\{\phi(x_1)+h(x)+\frac{1}{2}||x-\bar x||_\mathcal T^2-\langle x,\Delta (\delta&#39;,\delta^+)\rangle\right\}\\
&amp;=&amp; \arg\min_x\left\{\phi(x_1)+\frac{1}{2}\langle x,\mathcal Hx\rangle -\langle r,x\rangle+\frac{1}{2}||x-\bar x||_\mathcal T^2-\langle x,\Delta (\delta&#39;,\delta^+)\rangle\right\}\\
\end{array}
\]</span> The optimality condition for <span class="math inline">\(x^+\)</span> is <span class="math display">\[
(\mathcal H+\mathcal T)x=r-\gamma +\mathcal T \bar x+\Delta(\delta&#39;,\delta^+),
\]</span> where <span class="math inline">\(\gamma = (\gamma_1,0,\dotsm,0)\)</span>, <span class="math inline">\(\gamma_1 \in \partial \phi(x_1)\)</span>.</p>
<p>The meaning of <span class="math inline">\(\Delta(\delta&#39;,\delta^+)\)</span> is some numerical error when we solve the subproblem, since the subproblem cannot be solved exactly precisely.</p>
<p>The following theorem describe an equivalent symmetric Gauss-Seidel procedure for computing <span class="math inline">\(x^+\)</span>.</p>
<p><strong>Theorem</strong> (sGS Decomposition)</p>
<p>Assume that <span class="math inline">\(\mathcal H_{ii}\succ 0,\; \forall i=1,\dotsm,s\)</span> holds. Then we have <span class="math display">\[
\mathcal H+\mathcal T=(\mathcal D+\mathcal U)\mathcal D^{-1}(\mathcal D+\mathcal U^*)\succ 0.
\]</span> Furthermore, for <span class="math inline">\(i=s,\dotsm,2\)</span>, define <span class="math inline">\(x&#39;_i\in\mathcal X_i\)</span> by <span class="math display">\[
\begin{array}{rcl}
x_i&#39; &amp;=&amp; \arg\min_{x_i\in\mathcal X_i} \{\phi(\bar x_1)+h(\bar x_{\le i-1},x_i,x&#39;_{\ge i+1})-\lang\delta_i&#39;,x_i\rangle\}\\
&amp;=&amp;\mathcal H_{ii}^{-1}(r_i+\delta_i&#39;-\sum_{j=1}^{i-1} \mathcal H_{ji}^*\bar x_j-\sum_{j=i+1}^s \mathcal H_{ij}x&#39;_j).
\end{array}
\]</span> Then the optimal solution <span class="math inline">\(x^+\)</span> defined by <span class="math display">\[
x^+=\arg\min_x\left\{\phi(x_1)+h(x)+\frac{1}{2}||x-\bar x||_\mathcal T^2-\left\langle x,\Delta (\delta&#39;,\delta^+)\right\rangle\right\},
\]</span> can be obtained exactly via <span class="math display">\[
\begin{cases}
x_1^+ &amp; = &amp; \arg\min_{x_1}\phi(x_1)+h(x_1,x&#39;_{\ge 2})-\langle \delta_1^+,x_1\rangle\\
x_i^+&amp;=&amp; \arg\min _{x_i} \phi(x_1^+)+h(x^+_{\le i-1},x_i,x&#39;_{\ge i+1})-\langle \delta_i^+,x_i\rangle\\
&amp;=&amp; \mathcal H_{ii}^{-1}(r_i+\delta_i&#39;-\sum_{j=1}^{i-1} \mathcal H_{ji}^* x_j^+-\sum_{j=i+1}^s \mathcal H_{ij}x&#39;_j), \quad i=2,\dotsm,s.
\end{cases}
\]</span> Proof.</p>
<p>It is obvious <span class="math inline">\(\mathcal H+\mathcal T=(\mathcal D+\mathcal U)\mathcal D^{-1}(\mathcal D+\mathcal U^*)\succ 0\)</span>.</p>
<p>Then <span class="math display">\[
\begin{array}{rcl}
x_i&#39; &amp;=&amp; \arg\min_{x_i\in\mathcal X_i} \{\phi(\bar x_1)+h(\bar x_{\le i-1},x_i,x&#39;_{\ge i+1})-\lang\delta_i&#39;,x_i\rangle\}\\
&amp;=&amp; \arg\min_{x_i\in\mathcal X_i} \left\{\begin{array}{l}
\frac{1}{2}\langle (\bar x_1,\dotsm,\bar x_{i-1},x_i,x&#39;_{i+1},\dotsm,x&#39;_s),\mathcal H(\bar x_1,\dotsm,\bar x_{i-1},x_i,x&#39;_{i+1},\dotsm,x&#39;_s)\rangle \\
-\langle r,(\bar x_1,\dotsm,\bar x_{i-1},x_i,x&#39;_{i+1},\dotsm,x&#39;_s)\rangle\\
-\lang\delta_i&#39;,x_i\rangle
\end{array}\right\}\\
&amp;=&amp; \arg\min_{x_i\in\mathcal X_i} \left\{\begin{array}{l}
\frac{1}{2}(
\sum_{j=1}^{i-1} x_i\mathcal H_{ji}^*\bar x_j+x_i\mathcal H_{ii}x_i+\sum_{j=i+1}^s x_i\mathcal H_{ij}x&#39;_j\\
+\sum_{j=1}^{i-1}  x_i\mathcal H_{ji}^*\bar x_j+\sum_{j=i+1}^s x_i\mathcal H_{ij}x_j&#39;)\\
-\langle\delta_i&#39;+r_i,x_i\rangle
\end{array}\right\}\\
&amp;=&amp; \arg\min_{x_i\in\mathcal X_i} \left\{\begin{array}{l}
\sum_{j=1}^{i-1} x_i\mathcal H_{ji}^*\bar x_j+\frac{1}{2}x_i\mathcal H_{ii}x_i+\sum_{j=i+1}^s x_i\mathcal H_{ij}x&#39;_j-\langle\delta_i&#39;+r_i,x_i\rangle
\end{array}\right\}.
\end{array}
\]</span> The optimality condition is given by <span class="math display">\[
\sum_{j=1}^{i-1} \mathcal H_{ji}^*\bar x_j+\mathcal H_{ii}x_i+\sum_{j=i+1}^s \mathcal H_{ij}x&#39;_j-\delta_i&#39;-r_i=0.
\]</span> Then we have <span class="math display">\[
x_i&#39;=\mathcal H_{ii}^{-1}\left(r_i+\delta_i&#39;-\sum_{j=1}^{i-1} \mathcal H_{ji}^*\bar x_j-\sum_{j=i+1}^s \mathcal H_{ij}x&#39;_j\right).
\]</span> The remaining part is to prove that <span class="math display">\[
\begin{cases}
x_1^+ &amp; = &amp; \arg\min_{x_1}\phi(x_1)+h(x_1,x&#39;_{\ge 2})-\langle \delta_1^+,x_1\rangle\\
x_i^+ &amp; = &amp; \mathcal H_{ii}^{-1}(r_i+\delta_i&#39;-\sum_{j=1}^{i-1} \mathcal H_{ji}^* x_j^+-\sum_{j=i+1}^s \mathcal H_{ij}x&#39;_j), \quad i=2,\dotsm,s
\end{cases}
\]</span> is equivalent to <span class="math display">\[
x^+=\arg\min_x\left\{\phi(x_1)+h(x)+\frac{1}{2}||x-\bar x||_\mathcal T^2-\langle x,\Delta (\delta,\delta^+)\rangle\right\}.
\]</span> We define <span class="math display">\[
\begin{array}{rcl}
x_1&#39;&amp;=&amp;\arg\min_{x_1} \phi(x_1)+h(x_1,x&#39;_{\ge 2})-\langle \delta_1,x_1\rangle\\
&amp;=&amp; \arg\min_{x_1} \phi(x_1)+h(x_1,x&#39;_{\ge 2})-\langle \delta_1^+,x_1\rangle\\
&amp;=&amp; x_1^+.
\end{array}
\]</span> Since we have <span class="math inline">\(\delta_1=\delta^+_1\)</span>, and we check the optimality condition we have <span class="math display">\[
\begin{cases}
\mathcal H_{11} x_1&#39;&amp;=&amp;r_1-\gamma _1+\delta _1&#39;-\sum_{j=2}^{s}\mathcal H_{ij}x_j&#39;\\
\mathcal H_{11}x_1^+&amp;=&amp;r_1-\gamma _1+\delta _1^+-\sum_{j=2}^{s}\mathcal H_{ij}x_j&#39;.
\end{cases}
\]</span> By using $ x_1'=H_{11}^{-1}(r_1-<em>1+<em>1'-</em>{j=2}^{s}H</em>{1j}x_j')$ and <span class="math inline">\(x_i&#39;=\mathcal H_{ii}^{-1}\left(r_i+\delta_i&#39;-\sum_{j=1}^{i-1} \mathcal H_{ji}^*\bar x_j-\sum_{j=i+1}^s \mathcal H_{1j}x&#39;_j\right)\)</span>, we have <span class="math display">\[
\begin{array}{rcccl}
\mathcal H_{11} x_1&#39;&amp;=&amp;r_1&amp;-\gamma _1&amp;+\delta _1&#39;&amp;&amp;-\sum_{j=2}^{s}\mathcal H_{1j}x_j&#39;\\
\mathcal H_{22}x_2&#39;&amp;=&amp;r_2&amp;-0&amp;+\delta_2&#39;&amp;- \mathcal H_{12}^*\bar x_1&amp;-\sum_{j=3}^s \mathcal H_{2j}x&#39;_j\\
&amp;\vdots&amp;\\
\mathcal H_{(s-1)(s-1)}x_{s-1}&#39;&amp;=&amp;r_{s-1}&amp;-0&amp;+\delta_{s-1}&#39;&amp;-\sum_{j=1}^{s-2} \mathcal H_{j({s-1})}^*\bar x_j&amp;- \mathcal H_{(s-1)s}x&#39;_s\\
\mathcal H_{ss} x_s&#39;&amp;=&amp;r_s&amp;-0&amp;+\delta_s&#39;&amp;-\sum_{j=1}^{s-1} \mathcal H_{js}^*\bar x_j.
\end{array}
\]</span> Then we have <span class="math display">\[
\mathcal D x&#39;=r-\gamma+\delta&#39;-\mathcal U^* \bar x-\mathcal Ux&#39;\\
\iff\\
(\mathcal D+\mathcal U)x&#39;=r-\gamma +\delta&#39;-\mathcal U^*\bar x.
\]</span> From <span class="math display">\[
\begin{cases}
x_1^+ &amp; = &amp; \arg\min_{x_1}\phi(x_1)+h(x_1,x&#39;_{\ge 2})-\langle \delta_1^+,x_1\rangle\\
x_i^+ &amp; = &amp; \mathcal H_{ii}^{-1}(r_i+\delta_i&#39;-\sum_{j=1}^{i-1} \mathcal H_{ji}^* x_j^+-\sum_{j=i+1}^s \mathcal H_{ij}x&#39;_j), \quad i=2,\dotsm,s
\end{cases}
\]</span> and <span class="math display">\[
\mathcal H_{11}x_1^+=r_1-\gamma _1+\delta _1^+-\sum_{j=2}^{s}\mathcal H_{ij}x_j&#39;,
\]</span> we have <span class="math display">\[
(\mathcal D+\mathcal U^*)x^+=r-\gamma +\delta^+-\mathcal U x&#39;.
\]</span> Since we have <span class="math inline">\(x&#39;=(\mathcal D+\mathcal U)^{-1}(r-\gamma +\delta&#39;-\mathcal U^*\bar x)\)</span>, we have <span class="math display">\[
\begin{array}{rcl}
(\mathcal D+\mathcal U^*)x^+&amp;=&amp;r-\gamma +\delta^+-\mathcal U (\mathcal D+\mathcal U)^{-1}(r-\gamma +\delta&#39;-\mathcal U^*\bar x)\\
&amp;=&amp; ((\mathcal D+\mathcal U)(\mathcal D+\mathcal U)^{-1} -\mathcal U (\mathcal D+\mathcal U)^{-1})(r-\gamma) +\mathcal U (\mathcal D+\mathcal U)^{-1}\mathcal U^*\bar x+\delta^+-\mathcal U (\mathcal D+\mathcal U)^{-1}\delta&#39;\\
&amp;=&amp; \mathcal D(\mathcal D+\mathcal U)^{-1}(r-\gamma) +\mathcal U (\mathcal D+\mathcal U)^{-1}\mathcal U^*\bar x+\delta^+-\mathcal U (\mathcal D+\mathcal U)^{-1}\delta&#39;.
\end{array}
\]</span> Then we have <span class="math display">\[
\begin{array}{rcl}
(\mathcal D+\mathcal U)\mathcal D^{-1}(\mathcal D+\mathcal U^*)x^+
&amp;=&amp;r-\gamma +(\mathcal D+\mathcal U)\mathcal D^{-1}\mathcal U (\mathcal D+\mathcal U)^{-1}\mathcal U^*\bar x+\delta^+-(\mathcal D+\mathcal U)\mathcal D^{-1}\mathcal U (\mathcal D+\mathcal U)^{-1}\delta&#39;,
\end{array}
\]</span> We have the fact <span class="math display">\[
\begin{array}{rcl}
(\mathcal D+\mathcal U)\mathcal D^{-1}\mathcal U (\mathcal D+\mathcal U)^{-1}&amp;=&amp;
(\mathcal U+\mathcal U \mathcal D^{-1}\mathcal U) (\mathcal D+\mathcal U)^{-1}\\
&amp;=&amp; \mathcal U(\mathcal D+\mathcal U)^{-1}+\mathcal U\mathcal D^{-1}\mathcal U (\mathcal D+\mathcal U)^{-1}\\
&amp;=&amp; \mathcal U(\mathcal D+\mathcal U)^{-1}+\mathcal U(\mathcal D^{-1}-(\mathcal D+\mathcal U)^{-1})\\
&amp;=&amp; \mathcal U\mathcal D^{-1}.\\
\end{array}
\]</span> Then we have <span class="math display">\[
\begin{array}{rcl}
(\mathcal H+\mathcal T)x^+
&amp;=&amp;r-\gamma +\mathcal T\bar x+\delta^++\Delta(\delta&#39;,\delta^+).
\end{array}
\]</span> Q.E.D.</p>
<p><strong>Example</strong> <span class="math display">\[
\min\{p(x_1)+\frac{1}{2}\langle x,\mathcal Px\rangle -\langle g,x\rangle\;|\; \mathcal Ax=d\},
\]</span> where <span class="math inline">\(\mathcal P\)</span> is a positive semidefinite linear operator on <span class="math inline">\(\mathcal X\)</span>, and <span class="math inline">\(\mathcal A:\mathcal X\rightarrow \mathcal Y\)</span> is a given linear map, and <span class="math inline">\(g\in\mathcal X\)</span>, and <span class="math inline">\(d\in\mathcal Y\)</span> are given data. The Lagrangian function is given as <span class="math display">\[
\mathcal L_\sigma (x;y)=p(x_1)+\frac{1}{2}\langle x,\mathcal Px\rangle -\langle g,x\rangle+\frac{\sigma }{2} ||\mathcal Ax-d+\sigma ^{-1}y||^2-\frac{1}{2\sigma}||y||^2.
\]</span> Proximal ALM method is given as</p>
<p><strong>Step 1</strong> <span class="math display">\[
\begin{array}{rcl}
x^{k+1}&amp;=&amp;\arg\min_x\{\mathcal L_\sigma(x;y^k)+\frac{1}{2}||x-x^k||_\mathcal T^2\;|\; x\in\mathcal X\}\\
&amp;=&amp;\arg\min_x\{p(x_1)+
\frac{1}{2}\langle x,\mathcal Px\rangle -\langle g,x\rangle+
\frac{\sigma }{2} ||\mathcal Ax-d+\sigma ^{-1}y||^2
+\frac{1}{2}||x-x^k||_\mathcal T^2\;|\; x\in\mathcal X\}\\
&amp;=&amp;\arg\min_x\{p(x_1)+
\frac{1}{2}\langle x,\mathcal Px\rangle -\langle g,x\rangle
+\frac{\sigma}{2}\langle x,\mathcal A^*\mathcal Ax\rangle
+\sigma\langle x,\mathcal A^*(-d+\sigma^{-1}y)\rangle
+\frac{1}{2}||x-x^k||_\mathcal T^2\;|\; x\in\mathcal X\}\\
&amp;=&amp;\arg\min_x\{p(x_1)
+\frac{1}{2}\langle x,(\mathcal P+\sigma \mathcal A^*\mathcal A)x\rangle
-\langle g+ \mathcal A^*(\sigma d-y),x\rangle
+\frac{1}{2}||x-x^k||_\mathcal T^2\;|\; x\in\mathcal X\}.
\end{array}
\]</span> If we define <span class="math inline">\(b = g+ \mathcal A^*(\sigma d-y)\)</span>, then we have <span class="math display">\[
x^{k+1}=\arg\min _x \left\{p(x_1)
+\frac{1}{2}\langle x,(\mathcal P+\sigma \mathcal A^*\mathcal A)x\rangle
-\langle b,x\rangle
+\frac{1}{2}||x-x^k||_\mathcal T^2\;|\; x\in\mathcal X\right\}.
\]</span> Then if we choose <span class="math inline">\(\mathcal T= \mathcal T_{\mathcal P+\sigma \mathcal A^*\mathcal A}\)</span>, sGS decomposition can be applied.</p>
<p><strong>Step 2</strong></p>
<p>Compute <span class="math inline">\(y^{k+1}=y^k+\tau\sigma (\mathcal Ax^k-d)\)</span>, where <span class="math inline">\(\tau\in(0,2)\)</span> is the dual step-length.</p>
<p><strong>Example</strong></p>
<p>The important class of standard convex quadratic semidefinite programming (QSDP) in the dual form is given by <span class="math display">\[
\min\left\{\frac{1}{2}\langle W,\mathcal HW\rangle -\langle h,\xi\rangle \;|\; Z+\mathcal B^*\xi+\mathcal H W=C,\xi\in\mathbb R^p,Z\in\mathbb S^n_+,W\in\mathcal W\right\},
\]</span> where <span class="math inline">\(\mathcal H:\mathbb S^n\rightarrow \mathbb S^n\)</span> is a self-adjoint positive semidefinite linear operator.</p>
<p>In this problem, we have <span class="math display">\[
\mathcal Ax = \left[
\begin{array}{rcl}
I &amp;\mathcal B^* &amp; \mathcal H
\end{array}
\right]
\left(
\begin{array}{c}
Z\\ \xi\\  W
\end{array}
\right)=C\\
\mathcal P = \text{diag}(0,0,\mathcal H).
\]</span> Then we have <span class="math display">\[
\mathcal Q = \mathcal P+\sigma \mathcal A^*\mathcal A = 
\begin{bmatrix}
0,0,0\\
0,0,0\\
0,0,\mathcal H
\end{bmatrix}+
\sigma 
\begin{bmatrix}
I\\\mathcal B\\\mathcal H
\end{bmatrix}
\begin{bmatrix}
I&amp;\mathcal B^*&amp;\mathcal H
\end{bmatrix}=
\sigma 
\begin{bmatrix}
I &amp; \mathcal B^* &amp;\mathcal H \\
\mathcal B &amp; \mathcal B\mathcal B^*&amp;\mathcal B\mathcal H\\
\mathcal H &amp; \mathcal H\mathcal B^* &amp; \sigma^{-1}\mathcal H+\mathcal H^2
\end{bmatrix}.
\]</span></p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/10/13/Nonlinear Optimization/7.3 Nonlinear conic programming/" rel="next" title="7. Nonlinear conic programming (3)">
                  <i class="fa fa-chevron-left"></i> 7. Nonlinear conic programming (3)
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/10/24/Nonlinear Optimization/8.4 A symmetric Gauss-Seidel based semi-proximal ADMMALM/" rel="prev" title="8. ADMM methods for convex composite conic programming (2)">
                  8. ADMM methods for convex composite conic programming (2) <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#admm-methods-for-convex-composite-conic-programming"><span class="nav-text">8. ADMM methods for convex composite conic programming</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#a-generic-2-block-semi-proximal-admm"><span class="nav-text">8.1 A generic 2-block semi-proximal ADMM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#convergence-of-spadmm"><span class="nav-text">8.1.2 Convergence of sPADMM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#admm-for-solving-the-dual-problem-d-of-the-sparse-regression-problem"><span class="nav-text">8.1.3 ADMM for solving the dual problem (D) of the sparse regression problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#linearized-admm-for-solving-the-primal-problem-p-of-the-sparse-regression-problem"><span class="nav-text">8.1.4 Linearized ADMM for solving the primal problem (P) of the sparse regression problem</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#directly-extended-admm-for-multi-block-convex-programming-problem"><span class="nav-text">8.2 Directly extended ADMM for multi-block convex programming problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a-symmetric-gauss-seidel-decomposition-theorem"><span class="nav-text">8.3 A symmetric Gauss-Seidel decomposition theorem</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
