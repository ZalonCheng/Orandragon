<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Orandragon&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Orandragon&#39;s Blog">
  <link rel="canonical" href="http://yoursite.com/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/Convex Optimization/10. Equality constrained Minimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/12/22/Convex Optimization/10. Equality constrained Minimization/" class="post-title-link" itemprop="url">Chapter 10. Equality constrained Minimization</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:49:01 / Modified: 13:59:42" itemprop="dateCreated datePublished" datetime="2020-12-22T13:49:01+08:00">2020-12-22</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Convex-Optimization/" itemprop="url" rel="index"><span itemprop="name">Convex Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2020/12/22/Convex Optimization/10. Equality constrained Minimization/" class="post-meta-item leancloud_visitors" data-flag-title="Chapter 10. Equality constrained Minimization" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/12/22/Convex Optimization/10. Equality constrained Minimization/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/12/22/Convex Optimization/10. Equality constrained Minimization/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="equality-constrained-minimization">10. Equality constrained minimization</h1>
<h2 id="equality-constrained-minimization-problems">10.1 Equality constrained minimization problems</h2>
<p>In this chapter we describe methods for solving a convex optimization problem with equality constraints, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}
\]</span> We denote <span class="math display">\[
p^\star=\inf\{f(x)|Ax=b\}=f(x^\star)
\]</span> Recall that a point <span class="math inline">\(x^\star\)</span> is optimal for the problem if and only if there is a <span class="math inline">\(v^\star\)</span> such that <span class="math display">\[
Ax^\star =b\quad \nabla f(x^\star)+A^Tv^\star=0
\]</span> Solving the equality constrained optimization problem is therefore equivalent to finding a solution of the KKT conditions shown in the above, which is a n+p equations in the n+p variables <span class="math inline">\(v^\star\)</span> and <span class="math inline">\(x^\star\)</span>. The first one is called <strong>primal feasibility equations</strong>, which are linear. The second one is called <strong>dual feasibility equations</strong>, and are in general non-linear.</p>
<p>The bulk if this chapter is to denoted to extensions of Newton's method that directly handle equality constraints. In many cases, this method is preferable than the methods that reduce an equality constrained problem to unconstrained one. One reason is that the problem structure is destroyed by elimination. Another reason is conceptual, methods that directly handle the constraints can be thought of solving the problem directly.</p>
<h3 id="equality-constrained-convex-quadratic-minimization">10.1.1 Equality constrained convex quadratic minimization</h3>
<p><span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
(1/2)x^TPx+q^Tx+r
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}
\]</span></p>
<p>The optimal conditions are <span class="math display">\[
Ax^\star =b \quad Px^\star +q+A^Tv^\star=0
\]</span> which we can write as <span class="math display">\[
\begin{bmatrix}
P &amp; A^T\\
A  &amp; 0
\end{bmatrix}
\begin{bmatrix}
x^\star \\ v^\star
\end{bmatrix}=
\begin{bmatrix}
-q\\b
\end{bmatrix}
\]</span> This set of <span class="math inline">\(n+p\)</span> linear equations is called <strong>KKT system</strong>. The coefficient matrix is called <strong>KKT matrix</strong>.</p>
<p>If KKT matrix is nonsingular, there is an unique optimal solution. If singular, there is infinite solutions. If not solvable, the problem is infeasible.</p>
<h3 id="eliminating-equality-constraints">10.1.2 Eliminating equality constraints</h3>
<p>We have <span class="math display">\[
\{x|Ax=b\}=\{Fz+\hat x|z\in R^{n-p}\}
\]</span> where <span class="math inline">\(\hat x\)</span> is a particular solution of <span class="math inline">\(Ax=b\)</span> and <span class="math inline">\(F\)</span> is the null space of A.</p>
<h3 id="solving-equality-constrained-problem-via-the-dual">10.1.3 Solving equality constrained problem via the dual</h3>
<p>Another approach is to solving the dual, and then recover the optimal primal variable <span class="math inline">\(x^\star\)</span>.</p>
<p>The dual function of 10.1 is <span class="math display">\[
g(v)=-b^Tv+\inf_x(f(x)+v^TAx) \\
=-b^Tv-f^\star (-A^Tv)
\]</span></p>
<p>where <span class="math inline">\(f^*\)</span> is the conjugate of <span class="math inline">\(f\)</span>.</p>
<h2 id="newtons-method-with-equality-constraints">10.2 Newton's method with equality constraints</h2>
<p>In this section we describe an extension of Newton's method to include equality constraints. The method is almost the same as Newton's method without constraints, except two differences: the initial point must be feasible, and the definition of Newton step is modified to take the equality constraints into account.</p>
<h3 id="the-newton-step">10.2.1 The newton step</h3>
<p>For the equality constrained problem, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}
\]</span> we replace the objective with the second-order Taylor approximation near <span class="math inline">\(x\)</span>, to form the problem <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
\hat f(x+v)=f(x)+\nabla f(x)^Tv+(1/2)v^T\nabla^2f(x)v
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
A(x+v)=b
\end{array}
\end{array}
\]</span> with variable <span class="math inline">\(v\)</span>. This is a convex quadratic minimization problem with equality constraints, and can be solved analytically. We define the newton step <span class="math inline">\(\Delta x_{nt}\)</span> at <span class="math inline">\(x\)</span> as the solution of the convex quadratic problem in the above. Then we have <span class="math display">\[
\begin{bmatrix}
\nabla^2f(x) &amp; A^T\\A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x_{nt}\\w
\end{bmatrix}=
\begin{bmatrix}
-\nabla f(x)\\0
\end{bmatrix}
\]</span> where <span class="math inline">\(w\)</span> is the optimal dual variable for the quadratic problem.</p>
<p><strong>Solution of linearized optimality conditions</strong></p>
<p>Recall that a point <span class="math inline">\(x^\star\)</span> is optimal for the problem if and only if there is a <span class="math inline">\(v^\star\)</span> such that <span class="math display">\[
Ax^\star =b\\ \nabla f(x^\star)+A^Tv^\star=0
\]</span> Newton method can be interpreted by solving the above equations.</p>
<p>We substitute <span class="math inline">\(x+\Delta x_{nt}\)</span> for <span class="math inline">\(x^\star\)</span> and <span class="math inline">\(w\)</span> for <span class="math inline">\(v^\star\)</span>, and replace the gradient term in the second equation by its linearized approximation around <span class="math inline">\(x\)</span>, to obtain the equation, <span class="math display">\[
A(x+\Delta x_{nt})=b\\
\nabla f(x+\Delta x)+A^Tw\approx \nabla f(x)+\nabla^2 f(x)\Delta x_{nt}+A^Tw=0
\]</span></p>
<h2 id="infeasible-start-newton-method">10.3 Infeasible start Newton method</h2>
<p>As in Newton's method, we start with optimality conditions for the equality constrained optimization problem: <span class="math display">\[
Ax^\star=b\quad \nabla f(x^\star)+A^Tv^\star=0
\]</span> Let <span class="math inline">\(x\)</span> denote the current point, which we do not assume to be feasible. We will do second-order Taylor expansion around the point <span class="math inline">\(x\)</span>, and our goal is to find a step <span class="math inline">\(\Delta x\)</span> so that <span class="math inline">\(x+\Delta x\)</span> satisfies (at least approximately) the optimality conditions, i.e. <span class="math inline">\(x+\Delta x\approx x^\star\)</span>. <span class="math display">\[
\nabla f(x+\Delta x)\approx \nabla f(x)+\nabla^2f(x)\Delta x
\]</span> For the gradient, we have <span class="math display">\[
A(x+\Delta x)=b\\
\nabla f(x)+\nabla^2f(x)\Delta x+A^Tw=0
\]</span> This is a linear equations for <span class="math inline">\(\Delta x\)</span> and <span class="math inline">\(w\)</span> <span class="math display">\[
\begin{bmatrix}
\nabla^2f(x) &amp; A^T\\A &amp; 0
\end{bmatrix}
 \begin{bmatrix}
 \Delta x\\ w
 \end{bmatrix}=
 -\begin{bmatrix}
 \nabla f(x)\\Ax-b
 \end{bmatrix}
\]</span></p>
<p><strong>Primal-dual interpretation</strong></p>
<p>We can give an interpretation of the equation in terms of the primal-dual method for the equality constrained problem. By the primal-dual method, we mean one in which we update both the primal variable <span class="math inline">\(x\)</span> and the dual variable <span class="math inline">\(v\)</span>, in order to satisfy the optimality conditions.</p>
<p>We express the optimality conditions as <span class="math inline">\(r(x^\star,v^\star)=0\)</span>, where <span class="math inline">\(r:R^n\times R^p \rightarrow R^n\times R^p\)</span> is defined as <span class="math display">\[
r(x,v)=(r_{dual}(x,v),r_{pri}(x,v))\\
r_{dual}(x,v)=\nabla f(x)+A^Tv\\
r_{pri}(x,v)=Ax-b
\]</span> The first order Taylor approximate of <span class="math inline">\(r\)</span> is <span class="math display">\[
r(y+z)\approx r(y)+Dr(y)z
\]</span> We define <span class="math inline">\(\Delta y_{pd}=(\Delta x_{pd},\Delta v_{pd})\)</span> such that the Taylor approximation vanishes, i.e. <span class="math display">\[
r(y+\Delta y_{pd})=r(y)+Dr(y)\Delta y_{pd}=0
\]</span> Then we have <span class="math display">\[
Dr(y)\Delta y_{pd}=-r(y)
\]</span> Evaluating the derivative of <span class="math inline">\(r\)</span>, we have <span class="math display">\[
\begin{bmatrix}
\nabla^2f(x)&amp;A^T\\A&amp;0
\end{bmatrix}
\begin{bmatrix}
\Delta x_{pd}\\\Delta v_{pd}
\end{bmatrix}
=-
\begin{bmatrix}
r_{dual}\\ r_{pri}
\end{bmatrix}
=-
\begin{bmatrix}
\nabla f(x)+A^Tv\\ Ax-b
\end{bmatrix}
\]</span> Let <span class="math inline">\(w=v+\Delta v_{pd}\)</span>, we have Newton step.</p>
<p><strong>Using infeasible start Newton method to simplify initialization</strong></p>
<p>The main advantage of the infeasible start Newton method is in the initialization required.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/" class="post-title-link" itemprop="url">Untitled</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:48:40" itemprop="dateCreated datePublished" datetime="2020-12-22T13:48:40+08:00">2020-12-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-11-14 15:53:30" itemprop="dateModified" datetime="2019-11-14T15:53:30+08:00">2019-11-14</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MA6268-Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">MA6268 Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/" class="post-meta-item leancloud_visitors" data-flag-title="" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="newtons-method">9. Newton's method</h1>
<h2 id="newtons-method-for-smooth-equations">9.1 Newton's method for smooth equations</h2>
<p>Suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal X\)</span> is a <strong>continuously differentiable</strong> function, where <span class="math inline">\(\mathcal X\)</span> is a finite dimensional real Euclidean space. We are interested in find a point <span class="math inline">\(\bar x\in\mathcal X\)</span> such that <span class="math inline">\(F(\bar X)=0\)</span>.</p>
<p>If <span class="math inline">\(F(\cdot)\)</span> is a linear function, and <span class="math inline">\(F(x)=\mathcal A(x)-b\)</span>. Then <span class="math inline">\(\bar x= \mathcal A^{-1} b\)</span> if <span class="math inline">\(\mathcal A\)</span> is invertible. If <span class="math inline">\(F\)</span> is nonlinear, we have the following iterative scheme called Newton's method.</p>
<p><strong>Algorithm</strong> (Newton method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span>,</p>
<ol type="1">
<li><p>Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations, <span class="math display">\[
F(x^k)+F&#39;(x^k)d=0.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p><strong>Theorem</strong> (Local convergence)</p>
<p>Let <span class="math inline">\(\bar x\in\mathcal X\)</span> be a solution to <span class="math inline">\(F(x)=0\)</span>. Suppose that <span class="math inline">\(F&#39;(x)\)</span> is nonsingular. Then there exists an open neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> such that whenever <span class="math inline">\(x^0\)</span> is chose in <span class="math inline">\(\mathcal N(\bar x)\)</span>, the whole sequence <span class="math inline">\(\{x^k\}\subset \mathcal N(\bar x)\)</span>, <span class="math inline">\(x^k\rightarrow \bar x\)</span> as <span class="math inline">\(k\rightarrow \infty\)</span>, and <span class="math display">\[
\lim_{k\rightarrow \infty}\frac{||x^{k+1}-\bar x||}{||x-\bar x||}=0.
\]</span> Furthermore, if <span class="math inline">\(F&#39;(\bar x)\)</span> is <strong>locally Lipschitz continuous</strong> near <span class="math inline">\(\bar x\)</span>, then there exits a constant <span class="math inline">\(C&gt;0\)</span> such that <span class="math display">\[
||x^{k+1}-\bar x||\le C||x^k-\bar x||^2\quad \forall k\text{ is sufficiently large.}
\]</span></p>
<p>In order to prove the Theorem, the following two lemmas are introduced.</p>
<p><strong>Lemma</strong> (Perturbation Lemma)</p>
<p>Let <span class="math inline">\(A,B\in \mathbb R^{n\times n}\)</span> and assume that <span class="math inline">\(A\)</span> is invertible, with <span class="math inline">\(||A^{-1}||\le \alpha\)</span>. If <span class="math inline">\(||A-B||\le \beta\)</span> and <span class="math inline">\(\alpha\beta &lt;1\)</span>, then <span class="math inline">\(B\)</span> is also invertible, and <span class="math display">\[
||B^{-1}||\le \frac{\alpha}{1-\alpha\beta}.
\]</span> Proof. <span class="math display">\[
B=A[I+A^{-1}(B-A)].
\]</span> We have <span class="math display">\[
||A^{-1}(B-A)||\le \|A^{-1}\|\|B-A\|\le \alpha \beta&lt;1,
\]</span> which means <span class="math inline">\([I-A^{-1}(A-B)]^{-1}\)</span> exists.</p>
<p>By Taylor expansion, it follows <span class="math display">\[
\begin{array}{rcl}
[I+A^{-1}(B-A)]^{-1}&amp;=&amp;I-\sum_{k=1}^\infty[A^{-1}(B-A)]^k\\
\|B^{-1}\| &amp;=&amp; \|[I+A^{-1}(B-A)]^{-1}A^{-1}\|\\
&amp;=&amp; \|[I+A^{-1}(B-A)]^{-1}A^{-1}\|\\
&amp;\le&amp;  \|[I+A^{-1}(B-A)]^{-1}\|\|A^{-1}\|\\
&amp;\le&amp;  \|I-\sum_{k=1}^\infty[A^{-1}(B-A)]^k\|\alpha\\
&amp;\le&amp;  (1+\sum_{k=1}^\infty(\alpha\beta)^k)\alpha\\
&amp;=&amp; \frac{\alpha}{1-\alpha\beta}.
\end{array}
\]</span> <strong>Lemma</strong> (Mean Value Theorem)</p>
<p>Suppose that <span class="math inline">\(g:\mathcal X\rightarrow \mathcal Y\)</span> is a continuously differentiable function, where <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\mathcal Y\)</span> are finite dimensional real Euclidean space. Then <span class="math display">\[
g(y)-g(x)=\int_0^1 g&#39;(x+t(y-x))(y-x)dt\quad \forall x,y\in\mathcal X.
\]</span> Proof of Theorem</p>
<p>Recall that since <span class="math inline">\(F(\cdot)\)</span> us <span class="math inline">\(C^1\)</span> (Consequently <span class="math inline">\(||F&#39;(x)-F&#39;(\bar x)||\rightarrow 0\)</span> as <span class="math inline">\(x\rightarrow \bar x\)</span>), and <span class="math inline">\(F&#39;(\bar x)\)</span> is invertible, by the first lemma, we can find open neighborhood <span class="math inline">\(\mathcal N_1(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> and a constant <span class="math inline">\(M&gt;0\)</span> such that for all <span class="math inline">\(x\in\mathcal N_1(\bar x)\)</span>, <span class="math inline">\(F&#39;(x)\)</span> is invertible and <span class="math display">\[
||(F&#39;(x))^{-1}||\le M.
\]</span> We can find an open neighborhood <span class="math inline">\(\mathcal N_2(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> and a constant <span class="math inline">\(m&gt;0\)</span> such that <span class="math inline">\(Mm&lt;1/4\)</span>, and for all <span class="math inline">\(x\in\mathcal N_2(\bar x)\)</span>, we have <span class="math display">\[
\begin{array}{rcl}
||[F&#39;(x)-F&#39;(\bar x)](x-\bar x)||&amp;\le&amp; m||x-\bar x||\\
||F(x)-F(\bar x)-F&#39;(\bar x)(x-\bar x)||&amp;\le&amp; m||x-\bar x||.
\end{array}
\]</span> Hence we can find a neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> such that for all <span class="math inline">\(x\in\mathcal N(\bar x)\)</span>, we have both the properties. Let <span class="math inline">\(x^0\in\mathcal N(\bar x)\)</span>. Then <span class="math display">\[
\begin{array}{rcl}
||x^1-\bar x||&amp;=&amp; ||x^0-[F&#39;(x^0)]^{-1 }F(x^0)-\bar x||\\
&amp;=&amp; ||[F&#39;(x^0)]^{-1 }[F&#39;(x^0)(x^0-\bar x)-F(x^0)+F(\bar x)]||\\
&amp;=&amp; ||[F&#39;(x^0)]^{-1 }[F&#39;(x^0)(x^0-\bar x)-F(x^0)+F(\bar x)-F&#39;(\bar x)(x^0-\bar x)+F&#39;(\bar x)(x^0-\bar x)]||\\
&amp;=&amp; \|[F&#39;(x^0)]^{-1 }
[F(x^0)-F(\bar x)-F&#39;(\bar x)(x^0-\bar x)+(F&#39;(x^0)-F&#39;(\bar x))(x^0-\bar x)]\|\\
&amp;\le &amp; \|[F&#39;(x^0)]^{-1}\|
\left(\|F(x^0)-F(\bar x)-F&#39;(\bar x)(x^0-\bar x)\|+\|(F&#39;(x^0)-F&#39;(\bar x))(x^0-\bar x)\|\right)\\
&amp;\le &amp; 2mM||x^0-\bar x||\\
&amp;\le &amp; \frac{1}{2}||x^0-\bar x||.
\end{array}
\]</span> Then similarly, we have (linear converging rate) <span class="math display">\[
\|x^{k+1}-\bar x\|\le \frac{1}{2}\|x^k-\bar x\|\quad \forall k\ge 0.
\]</span> Since <span class="math inline">\(x^k\rightarrow x\)</span> as <span class="math inline">\(k\rightarrow \infty\)</span>, <span class="math inline">\(M\)</span> and <span class="math inline">\(m\)</span> can choose arbitrarily small. We have <span class="math display">\[
\|x^{k+1}-\bar x\| =o(\|x^k-\bar x\|).
\]</span> Next we prove the quadratic convergence by using the second lemma, <span class="math display">\[
\begin{array}{rcl}
\|x^{k+1}-\bar x\|  &amp;=&amp; \| [F&#39;(x^k)]^{-1}[F(x^k)-F(\bar x)-F&#39;(x^k)(x^k-\bar x)]\|\\
&amp;\le&amp; M\|[\int_0^1\left(F&#39;(\bar x+t(x^k-\bar x))-F&#39;(x^k)\right)(x^k-\bar x)dt]\|\\
&amp;\le&amp; M[\int_0^1\|\left(F&#39;(\bar x+t(x^k-\bar x))-F&#39;(x^k)\right)(x^k-\bar x)\|dt]\\
&amp;\le&amp; M\|x^k-\bar x\|\int_0^1\|\left(F&#39;(\bar x+t(x^k-\bar x))-F&#39;(x^k)\right)\|dt\\
&amp;\le&amp; MO(1)\|x^k-\bar x\|\int_0^1\|\left(\bar x+t(x^k-\bar x)-x^k\right)\|dt\\
&amp;\le&amp; MO(1)\|x^k-\bar x\|^2\int_0^1(t-1)dt\\
&amp;=&amp; O(1)\|x^k-\bar x\|^2.
\end{array}
\]</span></p>
<h2 id="a-globalized-newtons-method-line-search-model">9.2 A globalized Newton's method (line search model)</h2>
<p><strong>Differentiability class</strong> is a classification of functions according to the properties of their derivatives. The function <span class="math inline">\(f\)</span> is said to be of differentiability class <span class="math inline">\(C^k\)</span> if the derivatives <span class="math inline">\(f&#39;,f&#39;&#39;,\dotsm,f^{(k)}\)</span> exist and are continuous. The function <span class="math inline">\(f\)</span> is said to be of differentiability class <span class="math inline">\(C^\infty\)</span> or smooth if it has derivatives of all orders. <span class="math inline">\(C^0\)</span> consists of all continuous functions. <span class="math inline">\(C^1\)</span> consists of all differentiable functions.</p>
<p>Define <span class="math inline">\(f(x)=\frac{1}{2}||F(x)||^2,\;x\in\mathcal X\)</span>. Obviously, <span class="math inline">\(f\in C^1\)</span>, and <span class="math inline">\(F(\bar x)=0\)</span> if and only if <span class="math inline">\(\bar x\)</span> solves <span class="math inline">\(\min \{f(x)\;|\; x\in\mathcal X\}\)</span> globally with <span class="math inline">\(f(\bar x)=0\)</span>. From the fact that <span class="math inline">\(f\in C^1\)</span>, we have for all <span class="math inline">\(d\in\mathcal X\)</span> with <span class="math inline">\(||d||\)</span> sufficiently small, <span class="math display">\[
f(x+d)-f(x)=\langle \nabla f(x),d\rangle +o(||d||).
\]</span> <strong>Definition</strong> (Descent Direction)</p>
<p>Suppose that <span class="math inline">\(g:\mathcal X\rightarrow \mathcal Y\)</span> is a continuously differentiable function. We say that <span class="math inline">\(d\in\mathcal X\)</span> is a descent direction of <span class="math inline">\(g\)</span> at <span class="math inline">\(x\)</span> if <span class="math inline">\(\langle \nabla g(x),d\rangle &lt;0\)</span>.</p>
<p><strong>Fact</strong></p>
<p>If <span class="math inline">\(d\)</span> is a descent direction, then there exists <span class="math inline">\(\bar \lambda &gt;0\)</span> such that for all <span class="math inline">\(\lambda\in(0,\bar \lambda)\)</span>, we have <span class="math inline">\(g(x+\lambda d)&lt;g(x)\)</span>, which can be proved by using Taylor's expansion.</p>
<p><strong>Algorithm</strong> (Globalized Newton's method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, <span class="math inline">\(\rho\in(0,1)\)</span> and <span class="math inline">\(\delta\in(0,1/2)\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span></p>
<ol type="1">
<li><p>Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations <span class="math display">\[
F(x^k)+F&#39;(x^k)d=0.
\]</span></p></li>
<li><p>Let <span class="math inline">\(m_k\)</span> be the smallest nonnegative integer <span class="math inline">\(m\)</span> such that <span class="math display">\[
f(x^k+\rho^m d^k)\le (1-2\delta \rho^m)f(x^k).
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+\rho^{m_k}d^k\)</span>.</p></li>
</ol>
<p><strong>Theorem</strong> (Global Convergence)</p>
<p>Assume that <span class="math inline">\(\{||[F&#39;(x^k)]^{-1}||\}\)</span> is uniformly bounded. Then the globalized Newton's method is <strong>well-defined</strong>. Suppose that <span class="math inline">\(\bar x\)</span>, if exists, is an accumulation point of <span class="math inline">\(\{x^k\}\)</span>. Then <span class="math inline">\(\bar x\)</span> solves <span class="math inline">\(F(x)=0\)</span> and <span class="math inline">\(\{x^k\}\)</span> converges to <span class="math inline">\(\bar x\)</span> super-linearly. Furthermore, if <span class="math inline">\(F&#39;(\cdot)\)</span> is locally Lipschitz continuous near <span class="math inline">\(\bar x\)</span>. Then the rate of convergence is quadratic.</p>
<h2 id="newtons-method-for-smooth-unconstrained-minimization-problems">9.3 Newton's method for smooth unconstrained minimization problems</h2>
<p>Consider <span class="math display">\[
\min\{f(x)\;|\; x\in\mathcal X\}
\]</span> where <span class="math inline">\(f:\mathcal X\rightarrow \mathbb R\)</span> is twice continuously differentiable. To get a local minimizer, we solve the smooth equation <span class="math display">\[
\nabla f(x)=0.
\]</span> One can apply Newton's method to compute a root of the above equation.</p>
<p><strong>Algorithm</strong> (Newton's method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span>,</p>
<ol type="1">
<li><p>Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations <span class="math display">\[
\nabla f(x^k)+\nabla ^2f(x^k)d=0.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p>Note that if <span class="math inline">\(H^k=\nabla^2 f(x^k)\)</span> is positive definite, then <span class="math inline">\(d^k\)</span> is a descent direction; otherwise, <span class="math inline">\(d^k\)</span> is not necessarily a descent direction. To overcome the aforementioned defect, we need to modify the computation of <span class="math inline">\(d^k\)</span>.</p>
<p><strong>Algorithm</strong> (Practical Newton's method when <span class="math inline">\(\nabla^2 f(x^k)\not \succ 0\)</span>)</p>
<ol type="1">
<li><p>Pick <span class="math inline">\(\epsilon&gt;0\)</span> such that <span class="math inline">\(\nabla^2 f(x^k)+\epsilon_k I\succ 0\)</span>. Compute <span class="math inline">\(d^k\)</span> by solving the linear equations, <span class="math display">\[
(\nabla^2 f(x^k)+\epsilon_k I)d=-\nabla f(x^k).
\]</span> Then do line search.</p></li>
<li><p>Trust region model: compute <span class="math inline">\(d^k\)</span> by solving <span class="math display">\[
\begin{array}{rrcl}
\min &amp;Q(d)&amp;=&amp; f(x^k)+\langle \nabla f(x^k),d\rangle+\frac{1}{2}\langle d,\nabla f^2(x^k)d\rangle\\
\text{subject to} &amp;||d||&amp;\le &amp;\Delta_k. 
\end{array}
\]</span></p></li>
</ol>
<h2 id="semismooth-newtons-methods">9.4 Semismooth Newton's methods</h2>
<p><strong>Definition</strong> (Lipschitz continuous)</p>
<p>A function <span class="math inline">\(F:\mathcal E_1\rightarrow \mathcal E_2\)</span> is said to be <strong>locally Lipschitz continuous</strong> if for any open set <span class="math inline">\(\mathcal O \subset \mathcal E_1\)</span>, there exists a constant <span class="math inline">\(L\)</span> (depending on <span class="math inline">\(\mathcal O\)</span>) such that <span class="math display">\[
\|F(x)-F(y)\| \le L||x-y||\quad \forall x,y\in\mathcal O.
\]</span> If <span class="math inline">\(\mathcal O=\mathcal E_1\)</span>, then <span class="math inline">\(F\)</span> is said to be <strong>globally Lipschitz continuous</strong>.</p>
<p><strong>Proposition</strong></p>
<p>Each convex function is locally Lipschitz continuous.</p>
<p><strong>Theorem</strong> (Rademacher’s Theorem)</p>
<p>Suppose that <span class="math inline">\(F:\mathbb R^n\rightarrow \mathbb R^m\)</span> is locally Lipschitz continuous on an open set <span class="math inline">\(\mathcal O \in\mathbb R^n\)</span>. Then <span class="math inline">\(F\)</span> is almost everywhere (In the sense of Lebesgue measure) (Fréchet-)differentiable in <span class="math inline">\(\mathcal O\)</span>.</p>
<p>Randemacher's Theorem leads to the definition of the generalized Jacobian in the sense of Clarke. Let <span class="math display">\[
D_F =\{x\in\mathbb R^n\;|\; F\text{ is differentiable at }x\}.
\]</span> Then the <strong>generalized Jacobian</strong> of <span class="math inline">\(F\)</span> at <span class="math inline">\(x\)</span> can be defined by <span class="math display">\[
\partial F(x)=\text{conv}(\partial_B F(x))
\]</span> where <span class="math inline">\(\partial_B F(x)=\{\lim F&#39;(x^k)\;|\; x^k\in D_F,x_n\rightarrow x\}\)</span>.</p>
<p>Let <span class="math inline">\(\partial_C\)</span> be defined by <span class="math display">\[
\partial _C F(x)=\partial F_1(x)\times \partial F_2(x)\times \dotsm\times \partial F_m(x)
\]</span> It can be proved that <span class="math display">\[
\partial_B F(x)\subset \partial F(x)\subset \partial _CF(x)\quad \forall x\in\mathbb R^n.
\]</span> <strong>Example</strong></p>
Consider <span class="math inline">\(F(x)=|x|\)</span>. Then $$ _B F(x)=
<span class="math display">\[\begin{cases}
\{1\}&amp;\text{if }x&gt;0\\
\{-1,1\}&amp;\text{if }x=0\\
\{-1\}&amp;\text{if }x&lt;0\\
\end{cases}\quad\]</span>
F(x)=
<span class="math display">\[\begin{cases}
\{1\}&amp;\text{if }x&gt;0\\ 
[-1,1]&amp;\text{if }x=0\\
\{-1\}&amp;\text{if }x&lt;0\\
\end{cases}\]</span>
<p>$$ <strong>Proposition</strong></p>
<ol type="1">
<li><p><span class="math inline">\(\partial_B F(x)\)</span> is a nonempty compact subset of <span class="math inline">\(\mathbb R^{m\times n}\)</span>.</p></li>
<li><p><span class="math inline">\(\partial_B F(\cdot)\)</span> is upper semi-continuous at <span class="math inline">\(x\)</span>: for any <span class="math inline">\(\epsilon&gt;0\)</span> there exits <span class="math inline">\(\delta&gt;0\)</span> such that for all <span class="math inline">\(y\in x+\delta B_n\)</span>, <span class="math display">\[
\partial F(y)\subset \partial F(x)+\epsilon B_{m\times n}
\]</span> where <span class="math inline">\(B_n\subset \mathbb R^n\)</span> and <span class="math inline">\(B_{m\times n}\subset \mathbb R^{m\times n}\)</span> are unit open balls centered at the origin.</p></li>
<li><p>If <span class="math inline">\(F&#39;(x)\)</span> exists, then <span class="math inline">\(F&#39;(x)\in \partial_B F(x)\)</span>.</p></li>
</ol>
<p>Note that the above properties also hold for <span class="math inline">\(\partial F(\cdot)\)</span> and <span class="math inline">\(\partial_C F(\cdot)\)</span>.</p>
<p><strong>Remark</strong></p>
<p><strong>Upper semi-continuity</strong> of set-valued maps is generalized from the <strong>continuity</strong> of the function (Not the upper semi-continuity of the function).</p>
<p>Let us see what the continuous function is. For the continuity definition (<span class="math inline">\(\epsilon-\delta\)</span> definition), it is saying that <span class="math inline">\(\forall \epsilon&gt;0,\;\exists\delta&gt;0\)</span> such that <span class="math display">\[
||f(y)-f(x)||\le \epsilon\quad\forall y\in B_{\delta}(x).
\]</span> Therefore, it is <span class="math display">\[
f(y)\in f(x)+\epsilon B_m\quad \forall y\in x+\delta B_n,
\]</span> where <span class="math inline">\(B_m\)</span> is the unit open ball centered at the origin. Then it follows <span class="math display">\[
\{f(y)\}\subset \{f(x)\}+\epsilon B_m.
\]</span> Similarly, for the set-valued maps, it follows, <span class="math display">\[
\partial F(y)\subset \partial F(x)+\epsilon B_{m\times n}.
\]</span> Suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is locally Lipschitz continuous, where <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\mathcal Y\)</span> are finite dimensional real Euclidean spaces. Recall that any finite dimensional real Euclidean space is isometric to <span class="math inline">\(\mathbb R^n\)</span> for some integer <span class="math inline">\(n\)</span>. Hence, Rademacher's Theorem also holds in such a finite dimensional real Euclidean space <span class="math inline">\(\mathcal X\)</span>. Denote the set of all linear operators from <span class="math inline">\(\mathcal X\)</span> to <span class="math inline">\(\mathcal Y\)</span> by <span class="math inline">\(\mathcal L(\mathcal X,\mathcal Y)\)</span>. Then <span class="math inline">\(\mathcal L(\mathcal X,\mathcal Y)\)</span> is also a finite dimensional real Euclidean space. Let <span class="math inline">\(T_F:\mathcal X\rightrightarrows\mathcal L(\mathcal X,\mathcal Y)\)</span> be a multifunction satisfying</p>
<ol type="1">
<li><span class="math inline">\(T_F(x)\)</span> is a nonempty compact subset of <span class="math inline">\(\mathcal L(\mathcal X,\mathcal Y)\)</span></li>
<li><span class="math inline">\(T_F(\cdot)\)</span> is upper semi-continuous at <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(\partial_B F(x)v\subset T_F(x)v\subset \partial _C F(x)v\)</span> for all <span class="math inline">\(v\in\mathcal X\)</span>.</li>
</ol>
<p>Now we suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal X\)</span> is a locally Lipschitz continuous function.</p>
<p><strong>Algorithm</strong> (Local semismooth Newton's method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k\in T_F(x^k)\)</span>. Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations <span class="math display">\[
F(x^k)+V_k d=0.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p><strong>Definition</strong> (Semismoothness)</p>
<p>Let <span class="math inline">\(\Omega\)</span> be an open subset of <span class="math inline">\(\mathcal E\)</span>. Let <span class="math inline">\(f:\Omega\rightarrow \mathbb R\)</span> be a locally Lipschitz continuous function (not necessarily convex).</p>
<ol type="1">
<li><p><span class="math inline">\(f\)</span> is said to be <strong>semismooth</strong> at <span class="math inline">\(x\)</span> if it is directionally differentiable at <span class="math inline">\(x\)</span> and <span class="math display">\[
f(x+h)-f(x)-\langle \nabla f(x+h),h\rangle =o(||h||)\quad \forall h\rightarrow 0,x+h\in D_F.
\]</span></p></li>
<li><p>Moreover, <span class="math inline">\(f\)</span> is said to be <strong>strongly semismooth</strong> at <span class="math inline">\(x\)</span> if <span class="math display">\[
f(x+h)-f(x)-\langle \nabla f(x+h),h\rangle =O(||h||^2)\quad \forall h\rightarrow 0,x+h\in D_F.
\]</span></p></li>
</ol>
<p><strong>Theorem</strong></p>
<p>Any convex function <span class="math inline">\(f:\mathcal E\rightarrow \mathbb R\)</span> is semismooth.</p>
<p><strong>Definition</strong> (Directional differentiability)</p>
<p>A function <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is said to be directionally differentiable at <span class="math inline">\(x\)</span> if for any <span class="math inline">\(h\in\mathcal X\)</span>, <span class="math inline">\(F&#39;(x;h)\)</span> exists, where <span class="math display">\[
F&#39;(x;h)=\lim_{t\downarrow 0} \frac{F(x+th)-F(x)}{t}.
\]</span> It is possible to have a function f of two variables <span class="math inline">\(x,y\)</span> and a point <span class="math inline">\((x_0,y_0)\)</span> in the domain of <span class="math inline">\(f\)</span> such that <span class="math inline">\(f\)</span> has a directional derivative in every direction at <span class="math inline">\((x_0,y_0)\)</span>, but f is not differentiable at <span class="math inline">\((x_0,y_0)\)</span>, i.e., the gradient vector of <span class="math inline">\(f\)</span> at <span class="math inline">\((x_0,y_0)\)</span> does not exist. If a function is differentiable then all its directional derivatives exist and they can be easily computed from the derivative.</p>
<p><strong>Example</strong></p>
<p>Define <span class="math inline">\(f:\mathbb R^2\rightarrow \mathbb R\)</span> by <span class="math inline">\(f(x,y)=\frac{x^2y}{x^4+y^2}\)</span> when <span class="math inline">\((x,y)\neq (0,0)\)</span> and <span class="math inline">\(f(0,0)=0\)</span>, otherwise.</p>
<p>This function is not continuous at <span class="math inline">\((0,0)\)</span> and thus not differentiable at <span class="math inline">\((0,0)\)</span>.</p>
<p>Let <span class="math inline">\(U=(u_1,u_2)\in\mathbb R^2\)</span>, and <span class="math inline">\(||U||=1\)</span>. <span class="math display">\[
\lim_{t\downarrow 0}\frac{f(0+tU)-f(0)}{t}=\lim_{t\downarrow 0}\frac{u_1^2u_2}{t^2u_1^4+u_2^2}=0,\text{if }u_2=0,\text{and }\frac{u_1^2}{u_2},\text{if }u_2\neq 0.
\]</span> <strong>Definition</strong> (B-differentiability)</p>
<p>A function <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is said to be B-differentiable at <span class="math inline">\(x\)</span> if there exits a function <span class="math inline">\(BF(x):\mathcal X\rightarrow \mathcal Y\)</span>, called the B-derivative of <span class="math inline">\(F\)</span> at <span class="math inline">\(x\)</span>, which is positively homogeneous of degree 1, i.e., <span class="math inline">\(BF(x)(\lambda h)=\lambda BF(x)h\)</span> for all <span class="math inline">\(h\in\mathcal X\)</span> and <span class="math inline">\(\lambda \ge 0\)</span>, such that <span class="math display">\[
\lim_{h\rightarrow 0}\frac{F(x+h)-F(x)-BF(x)h}{||h||}=0.
\]</span> <strong>Proposition</strong></p>
<p>Suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is locally Lipschitz continuous. Then <span class="math inline">\(F\)</span> is B-differentiable at <span class="math inline">\(x\)</span> if and only if it is directionally differentiable at <span class="math inline">\(x\)</span>. In this case, the B-derivative and the directional derivate are identical, i.e., for any <span class="math inline">\(h\in\mathcal X\)</span>, <span class="math display">\[
BF(x)h=F&#39;(x;h).
\]</span> <strong>Definition</strong> (G-semismoothness)</p>
<p>A locally Lipschitz function <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is said to be G-semismooth at <span class="math inline">\(x\)</span> if for any <span class="math inline">\(h\rightarrow 0\)</span>, we have for any <span class="math inline">\(V\in\partial F(x+h)\)</span>, <span class="math display">\[
F(x+h)-F(x)-Vh=o(||h||).
\]</span> If the above condition is replaced by <span class="math display">\[
F(x+h)-F(x)-Vh=O(||h||^2),
\]</span> then <span class="math inline">\(F\)</span> is said to be strongly G-semismooth at <span class="math inline">\(x\)</span>.</p>
<p>In the above definition, if <span class="math inline">\(F\)</span> is also directionally differentiable at <span class="math inline">\(x\)</span>, then <span class="math inline">\(F\)</span> is said to be strongly semismooth at <span class="math inline">\(x\)</span>.</p>
<p><strong>Theorem</strong> (Local Convergence)</p>
<p>Let <span class="math inline">\(\bar x\in\mathcal X\)</span> be a solution to <span class="math inline">\(F(x)=0\)</span>. Suppose that</p>
<ol type="1">
<li><span class="math inline">\(T_F(\bar x)\)</span> is nonsingular, i.e., all <span class="math inline">\(V\in T_F(\bar x)\)</span> are nonsingular.</li>
<li><span class="math inline">\(F\)</span> is G-semismooth at <span class="math inline">\(\bar x\)</span>.</li>
</ol>
<p>Then there exists an open neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> such that if <span class="math inline">\(x^0\in\mathcal N(\bar x)\)</span>, the whole sequence <span class="math inline">\(\{x^k\}\)</span> generated by Newton's method is well-defined and converges to <span class="math inline">\(\bar x\)</span> supperlinearly, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=o(||x^k-\bar x||).
\]</span> Furthermore, if <span class="math inline">\(F\)</span> is strongly G-semismooth at <span class="math inline">\(\bar x\)</span>, then the convergence is quadratic, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=O(||x^k-\bar x||^2).
\]</span> When the dimension <span class="math inline">\(n\)</span> is very large, from the computational point of view, it is not advisable to solve the Newton linear system exactly. This motivation leads to an inexact version of Newton's method.</p>
<p><strong>Algorithm</strong> (Inexact local semismooth Newton method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, <span class="math inline">\(\tau\in(0,1]\)</span>, and <span class="math inline">\(\eta\in(0,1)\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k\in T_F(x^k)\)</span>. Compute an approximate Newton direction <span class="math inline">\(d^k\)</span> of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span> by solving the linear equations <span class="math display">\[
F(x^k)+V_kd=0,
\]</span> such that <span class="math display">\[
||F(x^k)+V_kd^k||\le \eta_k||F(x^k)||,
\]</span> where <span class="math inline">\(\eta_k=\min\{\eta,||F(x^k)||^\tau\}\)</span>.</p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p><strong>Remark</strong></p>
<ol type="1">
<li><span class="math inline">\(F(x^k)+V_kd=0\)</span> can be solved by conjugate gradient method or Gauss elimination.</li>
<li><span class="math inline">\(\eta_k=\min\{\eta,||F(x^k)||^\tau\}\)</span> means when the residual error <span class="math inline">\(||F(x^k)||\)</span> is too big, we just use <span class="math inline">\(\eta\)</span>.</li>
</ol>
<p><strong>Theorem</strong> (Local Convergence)</p>
<p>Let <span class="math inline">\(\bar x\in\mathcal X\)</span> be a solution to <span class="math inline">\(F(x)=0\)</span>. Suppose that</p>
<ol type="1">
<li><span class="math inline">\(T_F(\bar x)\)</span> is nonsingular, i.e., all <span class="math inline">\(V\in T_F(\bar x)\)</span> are nonsingular.</li>
<li><span class="math inline">\(F\)</span> is G-semismooth at <span class="math inline">\(\bar x\)</span>.</li>
</ol>
<p>Then there exists an open neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> such that if <span class="math inline">\(x^0\in\mathcal N(\bar x)\)</span>, the whole sequence <span class="math inline">\(\{x^k\}\)</span> generated by Newton's method is well-defined and converges to <span class="math inline">\(\bar x\)</span> supperlinearly, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=o(||x^k-\bar x||).
\]</span> Furthermore, if <span class="math inline">\(F\)</span> is strongly G-semismooth at <span class="math inline">\(\bar x\)</span>, then the convergence is of the order <span class="math inline">\(1+\tau\)</span>, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=O(||x^k-\bar x||^{1+\tau}).
\]</span> When the dimension <span class="math inline">\(n\)</span> is very large, from the computational point of view, it is not advisable to solve the Newton linear system exactly. This motivation leads to an inexact version of Newton's method.</p>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Set-valued map</th>
<th style="text-align: center;">Smooth single-valued function</th>
<th style="text-align: center;">Non-smooth single-valued function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Upper semi-continuity</td>
<td style="text-align: center;">Continuity</td>
<td style="text-align: center;">Continuity</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Semismoothness</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G-semismoothness</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Directional differentiability</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B-differentiability</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2 id="globalization-of-semismooth-newton-methods-for-equation">9.5 Globalization of semismooth Newton methods for equation</h2>
<p>Define <span class="math display">\[
f(x)=\frac{1}{2}||F(x)||^2.
\]</span> In some special cases, <span class="math inline">\(f\)</span> can be continuously differentiable though <span class="math inline">\(F\)</span> itself is not smooth. For example, for a closed convex cone <span class="math inline">\(\mathcal C\subset \mathbb R^n\)</span>, <span class="math inline">\(F(x)=\Pi_{\mathcal C}(x)\)</span>, <span class="math inline">\(x\in\mathbb R^n\)</span>, is not differentiable, while <span class="math inline">\(f\)</span> is continuously differentiable on <span class="math inline">\(\mathbb R^n\)</span>. For the case when <span class="math inline">\(f\)</span> is continuously differentiable, we state an inexact version of the damped semismooth Newton method.</p>
<p><strong>Algorithm</strong> (iDSN: Inexact damped semismooth Newton method)</p>
<p>Given <span class="math inline">\(x^0 \in\mathcal X\)</span>, <span class="math inline">\(\tau \in(0,1]\)</span>, <span class="math inline">\(\eta \in(0,1)\)</span>, and <span class="math inline">\(\rho\in(0,1/2)\)</span>, <span class="math inline">\(\sigma\in(0,1/8)\)</span>, perform the following steps for <span class="math inline">\(k=0,1\dotsm,\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k \in\partial F(x^k)\)</span>. Compute an approximate Newton direction <span class="math inline">\(d^k\)</span> of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span> by solving the linear equations, <span class="math display">\[
F(x^k)+V_kd=0
\]</span> such that <span class="math display">\[
|| F(x^k)+V_kd^k||\le \eta_k||F(x^k)||
\]</span> where <span class="math inline">\(\eta_k =\min\{\eta,||F(x^k)||^\tau\}\)</span>. If the above condition is not achievable or if the condition <span class="math display">\[
\langle \nabla f(x^k),d^k\rangle \le -\beta||d^k||^p\quad \text{($\beta&gt;0$ and $p&gt;1$ are given constants)}
\]</span> is not satisfied, let <span class="math inline">\(d=-\nabla f(x^k)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(m_k\)</span> be the smallest nonnegative integer <span class="math inline">\(m\)</span> such that <span class="math display">\[
f(x^k+\rho^m d^k)-f(x^k)\le \sigma \rho^m\langle \nabla f(x^k),d^k\rangle.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+\rho^{m_k} d^k\)</span>.</p></li>
</ol>
<h2 id="a-semismooth-newton-method-for-minimization-of-sc1-functions">9.6 A semismooth Newton method for minimization of SC1 functions</h2>
<p>We consider the unconstrained optimization problem <span class="math display">\[
\min\{f(x)\;|\; x\in\mathbb R^n\}
\]</span> where <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> is an <span class="math inline">\(LC^1\)</span> function (a continuously differentiable function whose gradient is locally Lipschitz continuous). Furthermore, an <span class="math inline">\(LC^1\)</span> function is said to be <span class="math inline">\(SC^1\)</span> if its gradient is semismooth. We denote the generalized Hessian, i.e., the generalized Jacobian of <span class="math inline">\(\nabla f(x)\)</span> at <span class="math inline">\(x\)</span> by <span class="math inline">\(\partial ^2 f(x)\)</span>. For the <span class="math inline">\(LC^1\)</span> functions, a second-order Taylor-like expansion is available, as shown in the proposition.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> be an <span class="math inline">\(LC^1\)</span> function on the open set <span class="math inline">\(\mathcal O\)</span> and suppose the closed line segment <span class="math inline">\([x;y]\)</span> is contained in <span class="math inline">\(\mathcal O\)</span>. Then <span class="math display">\[
f(y)=f(x)+\langle \nabla f(x),y-x\rangle+\frac{1}{2}\langle y-x,V(y-x)\rangle,
\]</span> where <span class="math inline">\(V\in\partial ^2 f(z)\)</span>, for some <span class="math inline">\(z\in(x;y)\)</span>.</p>
<p><strong>Algorithm</strong> (GSN: globalized semismooth Newton method)</p>
<p>Given <span class="math inline">\(x^0 \in\mathcal X\)</span>, <span class="math inline">\(\tau \in(0,1]\)</span>, <span class="math inline">\(\eta,\beta \in(0,1)\)</span>, and <span class="math inline">\(\rho,\sigma\in(0,1/2)\)</span>, perform the following steps for <span class="math inline">\(k=0,1\dotsm,\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k \in\partial^2 f(x^k)\)</span>. Compute an approximate Newton direction <span class="math inline">\(d^k\)</span> of <span class="math inline">\(\nabla f\)</span> at <span class="math inline">\(x^k\)</span> by solving the linear equations, <span class="math display">\[
\nabla f(x^k)+V_kd=0
\]</span> such that <span class="math display">\[
|| \nabla f(x^k)+V_kd^k||\le \eta_k||\nabla f(x^k)||
\]</span> where <span class="math inline">\(\eta_k =\min\{\eta,||F(x^k)||^\tau\}\)</span>. If the above condition is not achievable or if the condition <span class="math display">\[
\langle \nabla f(x^k),d^k\rangle \le -\beta_k||d^k||^p\quad \text{($p&gt;1$ are given constants)}
\]</span> is not satisfied, let <span class="math display">\[
d^k=-B_k^{-1}\nabla f(x^k).
\]</span> In the above, <span class="math inline">\(\beta_k=\min\{\beta,\|\nabla f(x^k)\|\}\)</span> and <span class="math inline">\(B_k\)</span> is any real symmetric positive definite matrix.</p></li>
<li><p>Let <span class="math inline">\(m_k\)</span> be the smallest nonnegative integer <span class="math inline">\(m\)</span> such that <span class="math display">\[
f(x^k+\rho^m d^k)-f(x^k)\le \sigma \rho^m\langle \nabla f(x^k),d^k\rangle.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+\rho^{m_k} d^k\)</span>.</p></li>
</ol>
<h2 id="an-inexact-semismooth-newton-method-for-convex-sc1-minimization-problems">9.7 An inexact semismooth Newton method for convex SC1 minimization problems</h2>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">71</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
