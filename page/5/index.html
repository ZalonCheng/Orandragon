<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Orandragon&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Orandragon&#39;s Blog">
  <link rel="canonical" href="http://yoursite.com/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/" class="post-title-link" itemprop="url">Untitled</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:48:40" itemprop="dateCreated datePublished" datetime="2020-12-22T13:48:40+08:00">2020-12-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-11-14 15:53:30" itemprop="dateModified" datetime="2019-11-14T15:53:30+08:00">2019-11-14</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MA6268-Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">MA6268 Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/" class="post-meta-item leancloud_visitors" data-flag-title="" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/12/22/MA6268 Nonlinear Optimization/9. Newton's method/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="newtons-method">9. Newton's method</h1>
<h2 id="newtons-method-for-smooth-equations">9.1 Newton's method for smooth equations</h2>
<p>Suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal X\)</span> is a <strong>continuously differentiable</strong> function, where <span class="math inline">\(\mathcal X\)</span> is a finite dimensional real Euclidean space. We are interested in find a point <span class="math inline">\(\bar x\in\mathcal X\)</span> such that <span class="math inline">\(F(\bar X)=0\)</span>.</p>
<p>If <span class="math inline">\(F(\cdot)\)</span> is a linear function, and <span class="math inline">\(F(x)=\mathcal A(x)-b\)</span>. Then <span class="math inline">\(\bar x= \mathcal A^{-1} b\)</span> if <span class="math inline">\(\mathcal A\)</span> is invertible. If <span class="math inline">\(F\)</span> is nonlinear, we have the following iterative scheme called Newton's method.</p>
<p><strong>Algorithm</strong> (Newton method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span>,</p>
<ol type="1">
<li><p>Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations, <span class="math display">\[
F(x^k)+F&#39;(x^k)d=0.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p><strong>Theorem</strong> (Local convergence)</p>
<p>Let <span class="math inline">\(\bar x\in\mathcal X\)</span> be a solution to <span class="math inline">\(F(x)=0\)</span>. Suppose that <span class="math inline">\(F&#39;(x)\)</span> is nonsingular. Then there exists an open neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> such that whenever <span class="math inline">\(x^0\)</span> is chose in <span class="math inline">\(\mathcal N(\bar x)\)</span>, the whole sequence <span class="math inline">\(\{x^k\}\subset \mathcal N(\bar x)\)</span>, <span class="math inline">\(x^k\rightarrow \bar x\)</span> as <span class="math inline">\(k\rightarrow \infty\)</span>, and <span class="math display">\[
\lim_{k\rightarrow \infty}\frac{||x^{k+1}-\bar x||}{||x-\bar x||}=0.
\]</span> Furthermore, if <span class="math inline">\(F&#39;(\bar x)\)</span> is <strong>locally Lipschitz continuous</strong> near <span class="math inline">\(\bar x\)</span>, then there exits a constant <span class="math inline">\(C&gt;0\)</span> such that <span class="math display">\[
||x^{k+1}-\bar x||\le C||x^k-\bar x||^2\quad \forall k\text{ is sufficiently large.}
\]</span></p>
<p>In order to prove the Theorem, the following two lemmas are introduced.</p>
<p><strong>Lemma</strong> (Perturbation Lemma)</p>
<p>Let <span class="math inline">\(A,B\in \mathbb R^{n\times n}\)</span> and assume that <span class="math inline">\(A\)</span> is invertible, with <span class="math inline">\(||A^{-1}||\le \alpha\)</span>. If <span class="math inline">\(||A-B||\le \beta\)</span> and <span class="math inline">\(\alpha\beta &lt;1\)</span>, then <span class="math inline">\(B\)</span> is also invertible, and <span class="math display">\[
||B^{-1}||\le \frac{\alpha}{1-\alpha\beta}.
\]</span> Proof. <span class="math display">\[
B=A[I+A^{-1}(B-A)].
\]</span> We have <span class="math display">\[
||A^{-1}(B-A)||\le \|A^{-1}\|\|B-A\|\le \alpha \beta&lt;1,
\]</span> which means <span class="math inline">\([I-A^{-1}(A-B)]^{-1}\)</span> exists.</p>
<p>By Taylor expansion, it follows <span class="math display">\[
\begin{array}{rcl}
[I+A^{-1}(B-A)]^{-1}&amp;=&amp;I-\sum_{k=1}^\infty[A^{-1}(B-A)]^k\\
\|B^{-1}\| &amp;=&amp; \|[I+A^{-1}(B-A)]^{-1}A^{-1}\|\\
&amp;=&amp; \|[I+A^{-1}(B-A)]^{-1}A^{-1}\|\\
&amp;\le&amp;  \|[I+A^{-1}(B-A)]^{-1}\|\|A^{-1}\|\\
&amp;\le&amp;  \|I-\sum_{k=1}^\infty[A^{-1}(B-A)]^k\|\alpha\\
&amp;\le&amp;  (1+\sum_{k=1}^\infty(\alpha\beta)^k)\alpha\\
&amp;=&amp; \frac{\alpha}{1-\alpha\beta}.
\end{array}
\]</span> <strong>Lemma</strong> (Mean Value Theorem)</p>
<p>Suppose that <span class="math inline">\(g:\mathcal X\rightarrow \mathcal Y\)</span> is a continuously differentiable function, where <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\mathcal Y\)</span> are finite dimensional real Euclidean space. Then <span class="math display">\[
g(y)-g(x)=\int_0^1 g&#39;(x+t(y-x))(y-x)dt\quad \forall x,y\in\mathcal X.
\]</span> Proof of Theorem</p>
<p>Recall that since <span class="math inline">\(F(\cdot)\)</span> us <span class="math inline">\(C^1\)</span> (Consequently <span class="math inline">\(||F&#39;(x)-F&#39;(\bar x)||\rightarrow 0\)</span> as <span class="math inline">\(x\rightarrow \bar x\)</span>), and <span class="math inline">\(F&#39;(\bar x)\)</span> is invertible, by the first lemma, we can find open neighborhood <span class="math inline">\(\mathcal N_1(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> and a constant <span class="math inline">\(M&gt;0\)</span> such that for all <span class="math inline">\(x\in\mathcal N_1(\bar x)\)</span>, <span class="math inline">\(F&#39;(x)\)</span> is invertible and <span class="math display">\[
||(F&#39;(x))^{-1}||\le M.
\]</span> We can find an open neighborhood <span class="math inline">\(\mathcal N_2(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> and a constant <span class="math inline">\(m&gt;0\)</span> such that <span class="math inline">\(Mm&lt;1/4\)</span>, and for all <span class="math inline">\(x\in\mathcal N_2(\bar x)\)</span>, we have <span class="math display">\[
\begin{array}{rcl}
||[F&#39;(x)-F&#39;(\bar x)](x-\bar x)||&amp;\le&amp; m||x-\bar x||\\
||F(x)-F(\bar x)-F&#39;(\bar x)(x-\bar x)||&amp;\le&amp; m||x-\bar x||.
\end{array}
\]</span> Hence we can find a neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> such that for all <span class="math inline">\(x\in\mathcal N(\bar x)\)</span>, we have both the properties. Let <span class="math inline">\(x^0\in\mathcal N(\bar x)\)</span>. Then <span class="math display">\[
\begin{array}{rcl}
||x^1-\bar x||&amp;=&amp; ||x^0-[F&#39;(x^0)]^{-1 }F(x^0)-\bar x||\\
&amp;=&amp; ||[F&#39;(x^0)]^{-1 }[F&#39;(x^0)(x^0-\bar x)-F(x^0)+F(\bar x)]||\\
&amp;=&amp; ||[F&#39;(x^0)]^{-1 }[F&#39;(x^0)(x^0-\bar x)-F(x^0)+F(\bar x)-F&#39;(\bar x)(x^0-\bar x)+F&#39;(\bar x)(x^0-\bar x)]||\\
&amp;=&amp; \|[F&#39;(x^0)]^{-1 }
[F(x^0)-F(\bar x)-F&#39;(\bar x)(x^0-\bar x)+(F&#39;(x^0)-F&#39;(\bar x))(x^0-\bar x)]\|\\
&amp;\le &amp; \|[F&#39;(x^0)]^{-1}\|
\left(\|F(x^0)-F(\bar x)-F&#39;(\bar x)(x^0-\bar x)\|+\|(F&#39;(x^0)-F&#39;(\bar x))(x^0-\bar x)\|\right)\\
&amp;\le &amp; 2mM||x^0-\bar x||\\
&amp;\le &amp; \frac{1}{2}||x^0-\bar x||.
\end{array}
\]</span> Then similarly, we have (linear converging rate) <span class="math display">\[
\|x^{k+1}-\bar x\|\le \frac{1}{2}\|x^k-\bar x\|\quad \forall k\ge 0.
\]</span> Since <span class="math inline">\(x^k\rightarrow x\)</span> as <span class="math inline">\(k\rightarrow \infty\)</span>, <span class="math inline">\(M\)</span> and <span class="math inline">\(m\)</span> can choose arbitrarily small. We have <span class="math display">\[
\|x^{k+1}-\bar x\| =o(\|x^k-\bar x\|).
\]</span> Next we prove the quadratic convergence by using the second lemma, <span class="math display">\[
\begin{array}{rcl}
\|x^{k+1}-\bar x\|  &amp;=&amp; \| [F&#39;(x^k)]^{-1}[F(x^k)-F(\bar x)-F&#39;(x^k)(x^k-\bar x)]\|\\
&amp;\le&amp; M\|[\int_0^1\left(F&#39;(\bar x+t(x^k-\bar x))-F&#39;(x^k)\right)(x^k-\bar x)dt]\|\\
&amp;\le&amp; M[\int_0^1\|\left(F&#39;(\bar x+t(x^k-\bar x))-F&#39;(x^k)\right)(x^k-\bar x)\|dt]\\
&amp;\le&amp; M\|x^k-\bar x\|\int_0^1\|\left(F&#39;(\bar x+t(x^k-\bar x))-F&#39;(x^k)\right)\|dt\\
&amp;\le&amp; MO(1)\|x^k-\bar x\|\int_0^1\|\left(\bar x+t(x^k-\bar x)-x^k\right)\|dt\\
&amp;\le&amp; MO(1)\|x^k-\bar x\|^2\int_0^1(t-1)dt\\
&amp;=&amp; O(1)\|x^k-\bar x\|^2.
\end{array}
\]</span></p>
<h2 id="a-globalized-newtons-method-line-search-model">9.2 A globalized Newton's method (line search model)</h2>
<p><strong>Differentiability class</strong> is a classification of functions according to the properties of their derivatives. The function <span class="math inline">\(f\)</span> is said to be of differentiability class <span class="math inline">\(C^k\)</span> if the derivatives <span class="math inline">\(f&#39;,f&#39;&#39;,\dotsm,f^{(k)}\)</span> exist and are continuous. The function <span class="math inline">\(f\)</span> is said to be of differentiability class <span class="math inline">\(C^\infty\)</span> or smooth if it has derivatives of all orders. <span class="math inline">\(C^0\)</span> consists of all continuous functions. <span class="math inline">\(C^1\)</span> consists of all differentiable functions.</p>
<p>Define <span class="math inline">\(f(x)=\frac{1}{2}||F(x)||^2,\;x\in\mathcal X\)</span>. Obviously, <span class="math inline">\(f\in C^1\)</span>, and <span class="math inline">\(F(\bar x)=0\)</span> if and only if <span class="math inline">\(\bar x\)</span> solves <span class="math inline">\(\min \{f(x)\;|\; x\in\mathcal X\}\)</span> globally with <span class="math inline">\(f(\bar x)=0\)</span>. From the fact that <span class="math inline">\(f\in C^1\)</span>, we have for all <span class="math inline">\(d\in\mathcal X\)</span> with <span class="math inline">\(||d||\)</span> sufficiently small, <span class="math display">\[
f(x+d)-f(x)=\langle \nabla f(x),d\rangle +o(||d||).
\]</span> <strong>Definition</strong> (Descent Direction)</p>
<p>Suppose that <span class="math inline">\(g:\mathcal X\rightarrow \mathcal Y\)</span> is a continuously differentiable function. We say that <span class="math inline">\(d\in\mathcal X\)</span> is a descent direction of <span class="math inline">\(g\)</span> at <span class="math inline">\(x\)</span> if <span class="math inline">\(\langle \nabla g(x),d\rangle &lt;0\)</span>.</p>
<p><strong>Fact</strong></p>
<p>If <span class="math inline">\(d\)</span> is a descent direction, then there exists <span class="math inline">\(\bar \lambda &gt;0\)</span> such that for all <span class="math inline">\(\lambda\in(0,\bar \lambda)\)</span>, we have <span class="math inline">\(g(x+\lambda d)&lt;g(x)\)</span>, which can be proved by using Taylor's expansion.</p>
<p><strong>Algorithm</strong> (Globalized Newton's method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, <span class="math inline">\(\rho\in(0,1)\)</span> and <span class="math inline">\(\delta\in(0,1/2)\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span></p>
<ol type="1">
<li><p>Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations <span class="math display">\[
F(x^k)+F&#39;(x^k)d=0.
\]</span></p></li>
<li><p>Let <span class="math inline">\(m_k\)</span> be the smallest nonnegative integer <span class="math inline">\(m\)</span> such that <span class="math display">\[
f(x^k+\rho^m d^k)\le (1-2\delta \rho^m)f(x^k).
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+\rho^{m_k}d^k\)</span>.</p></li>
</ol>
<p><strong>Theorem</strong> (Global Convergence)</p>
<p>Assume that <span class="math inline">\(\{||[F&#39;(x^k)]^{-1}||\}\)</span> is uniformly bounded. Then the globalized Newton's method is <strong>well-defined</strong>. Suppose that <span class="math inline">\(\bar x\)</span>, if exists, is an accumulation point of <span class="math inline">\(\{x^k\}\)</span>. Then <span class="math inline">\(\bar x\)</span> solves <span class="math inline">\(F(x)=0\)</span> and <span class="math inline">\(\{x^k\}\)</span> converges to <span class="math inline">\(\bar x\)</span> super-linearly. Furthermore, if <span class="math inline">\(F&#39;(\cdot)\)</span> is locally Lipschitz continuous near <span class="math inline">\(\bar x\)</span>. Then the rate of convergence is quadratic.</p>
<h2 id="newtons-method-for-smooth-unconstrained-minimization-problems">9.3 Newton's method for smooth unconstrained minimization problems</h2>
<p>Consider <span class="math display">\[
\min\{f(x)\;|\; x\in\mathcal X\}
\]</span> where <span class="math inline">\(f:\mathcal X\rightarrow \mathbb R\)</span> is twice continuously differentiable. To get a local minimizer, we solve the smooth equation <span class="math display">\[
\nabla f(x)=0.
\]</span> One can apply Newton's method to compute a root of the above equation.</p>
<p><strong>Algorithm</strong> (Newton's method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span>,</p>
<ol type="1">
<li><p>Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations <span class="math display">\[
\nabla f(x^k)+\nabla ^2f(x^k)d=0.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p>Note that if <span class="math inline">\(H^k=\nabla^2 f(x^k)\)</span> is positive definite, then <span class="math inline">\(d^k\)</span> is a descent direction; otherwise, <span class="math inline">\(d^k\)</span> is not necessarily a descent direction. To overcome the aforementioned defect, we need to modify the computation of <span class="math inline">\(d^k\)</span>.</p>
<p><strong>Algorithm</strong> (Practical Newton's method when <span class="math inline">\(\nabla^2 f(x^k)\not \succ 0\)</span>)</p>
<ol type="1">
<li><p>Pick <span class="math inline">\(\epsilon&gt;0\)</span> such that <span class="math inline">\(\nabla^2 f(x^k)+\epsilon_k I\succ 0\)</span>. Compute <span class="math inline">\(d^k\)</span> by solving the linear equations, <span class="math display">\[
(\nabla^2 f(x^k)+\epsilon_k I)d=-\nabla f(x^k).
\]</span> Then do line search.</p></li>
<li><p>Trust region model: compute <span class="math inline">\(d^k\)</span> by solving <span class="math display">\[
\begin{array}{rrcl}
\min &amp;Q(d)&amp;=&amp; f(x^k)+\langle \nabla f(x^k),d\rangle+\frac{1}{2}\langle d,\nabla f^2(x^k)d\rangle\\
\text{subject to} &amp;||d||&amp;\le &amp;\Delta_k. 
\end{array}
\]</span></p></li>
</ol>
<h2 id="semismooth-newtons-methods">9.4 Semismooth Newton's methods</h2>
<p><strong>Definition</strong> (Lipschitz continuous)</p>
<p>A function <span class="math inline">\(F:\mathcal E_1\rightarrow \mathcal E_2\)</span> is said to be <strong>locally Lipschitz continuous</strong> if for any open set <span class="math inline">\(\mathcal O \subset \mathcal E_1\)</span>, there exists a constant <span class="math inline">\(L\)</span> (depending on <span class="math inline">\(\mathcal O\)</span>) such that <span class="math display">\[
\|F(x)-F(y)\| \le L||x-y||\quad \forall x,y\in\mathcal O.
\]</span> If <span class="math inline">\(\mathcal O=\mathcal E_1\)</span>, then <span class="math inline">\(F\)</span> is said to be <strong>globally Lipschitz continuous</strong>.</p>
<p><strong>Proposition</strong></p>
<p>Each convex function is locally Lipschitz continuous.</p>
<p><strong>Theorem</strong> (Rademacher’s Theorem)</p>
<p>Suppose that <span class="math inline">\(F:\mathbb R^n\rightarrow \mathbb R^m\)</span> is locally Lipschitz continuous on an open set <span class="math inline">\(\mathcal O \in\mathbb R^n\)</span>. Then <span class="math inline">\(F\)</span> is almost everywhere (In the sense of Lebesgue measure) (Fréchet-)differentiable in <span class="math inline">\(\mathcal O\)</span>.</p>
<p>Randemacher's Theorem leads to the definition of the generalized Jacobian in the sense of Clarke. Let <span class="math display">\[
D_F =\{x\in\mathbb R^n\;|\; F\text{ is differentiable at }x\}.
\]</span> Then the <strong>generalized Jacobian</strong> of <span class="math inline">\(F\)</span> at <span class="math inline">\(x\)</span> can be defined by <span class="math display">\[
\partial F(x)=\text{conv}(\partial_B F(x))
\]</span> where <span class="math inline">\(\partial_B F(x)=\{\lim F&#39;(x^k)\;|\; x^k\in D_F,x_n\rightarrow x\}\)</span>.</p>
<p>Let <span class="math inline">\(\partial_C\)</span> be defined by <span class="math display">\[
\partial _C F(x)=\partial F_1(x)\times \partial F_2(x)\times \dotsm\times \partial F_m(x)
\]</span> It can be proved that <span class="math display">\[
\partial_B F(x)\subset \partial F(x)\subset \partial _CF(x)\quad \forall x\in\mathbb R^n.
\]</span> <strong>Example</strong></p>
Consider <span class="math inline">\(F(x)=|x|\)</span>. Then $$ _B F(x)=
<span class="math display">\[\begin{cases}
\{1\}&amp;\text{if }x&gt;0\\
\{-1,1\}&amp;\text{if }x=0\\
\{-1\}&amp;\text{if }x&lt;0\\
\end{cases}\quad\]</span>
F(x)=
<span class="math display">\[\begin{cases}
\{1\}&amp;\text{if }x&gt;0\\ 
[-1,1]&amp;\text{if }x=0\\
\{-1\}&amp;\text{if }x&lt;0\\
\end{cases}\]</span>
<p>$$ <strong>Proposition</strong></p>
<ol type="1">
<li><p><span class="math inline">\(\partial_B F(x)\)</span> is a nonempty compact subset of <span class="math inline">\(\mathbb R^{m\times n}\)</span>.</p></li>
<li><p><span class="math inline">\(\partial_B F(\cdot)\)</span> is upper semi-continuous at <span class="math inline">\(x\)</span>: for any <span class="math inline">\(\epsilon&gt;0\)</span> there exits <span class="math inline">\(\delta&gt;0\)</span> such that for all <span class="math inline">\(y\in x+\delta B_n\)</span>, <span class="math display">\[
\partial F(y)\subset \partial F(x)+\epsilon B_{m\times n}
\]</span> where <span class="math inline">\(B_n\subset \mathbb R^n\)</span> and <span class="math inline">\(B_{m\times n}\subset \mathbb R^{m\times n}\)</span> are unit open balls centered at the origin.</p></li>
<li><p>If <span class="math inline">\(F&#39;(x)\)</span> exists, then <span class="math inline">\(F&#39;(x)\in \partial_B F(x)\)</span>.</p></li>
</ol>
<p>Note that the above properties also hold for <span class="math inline">\(\partial F(\cdot)\)</span> and <span class="math inline">\(\partial_C F(\cdot)\)</span>.</p>
<p><strong>Remark</strong></p>
<p><strong>Upper semi-continuity</strong> of set-valued maps is generalized from the <strong>continuity</strong> of the function (Not the upper semi-continuity of the function).</p>
<p>Let us see what the continuous function is. For the continuity definition (<span class="math inline">\(\epsilon-\delta\)</span> definition), it is saying that <span class="math inline">\(\forall \epsilon&gt;0,\;\exists\delta&gt;0\)</span> such that <span class="math display">\[
||f(y)-f(x)||\le \epsilon\quad\forall y\in B_{\delta}(x).
\]</span> Therefore, it is <span class="math display">\[
f(y)\in f(x)+\epsilon B_m\quad \forall y\in x+\delta B_n,
\]</span> where <span class="math inline">\(B_m\)</span> is the unit open ball centered at the origin. Then it follows <span class="math display">\[
\{f(y)\}\subset \{f(x)\}+\epsilon B_m.
\]</span> Similarly, for the set-valued maps, it follows, <span class="math display">\[
\partial F(y)\subset \partial F(x)+\epsilon B_{m\times n}.
\]</span> Suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is locally Lipschitz continuous, where <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\mathcal Y\)</span> are finite dimensional real Euclidean spaces. Recall that any finite dimensional real Euclidean space is isometric to <span class="math inline">\(\mathbb R^n\)</span> for some integer <span class="math inline">\(n\)</span>. Hence, Rademacher's Theorem also holds in such a finite dimensional real Euclidean space <span class="math inline">\(\mathcal X\)</span>. Denote the set of all linear operators from <span class="math inline">\(\mathcal X\)</span> to <span class="math inline">\(\mathcal Y\)</span> by <span class="math inline">\(\mathcal L(\mathcal X,\mathcal Y)\)</span>. Then <span class="math inline">\(\mathcal L(\mathcal X,\mathcal Y)\)</span> is also a finite dimensional real Euclidean space. Let <span class="math inline">\(T_F:\mathcal X\rightrightarrows\mathcal L(\mathcal X,\mathcal Y)\)</span> be a multifunction satisfying</p>
<ol type="1">
<li><span class="math inline">\(T_F(x)\)</span> is a nonempty compact subset of <span class="math inline">\(\mathcal L(\mathcal X,\mathcal Y)\)</span></li>
<li><span class="math inline">\(T_F(\cdot)\)</span> is upper semi-continuous at <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(\partial_B F(x)v\subset T_F(x)v\subset \partial _C F(x)v\)</span> for all <span class="math inline">\(v\in\mathcal X\)</span>.</li>
</ol>
<p>Now we suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal X\)</span> is a locally Lipschitz continuous function.</p>
<p><strong>Algorithm</strong> (Local semismooth Newton's method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k\in T_F(x^k)\)</span>. Compute the Newton direction of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span>, denoted as <span class="math inline">\(d^k\)</span>, by solving the linear equations <span class="math display">\[
F(x^k)+V_k d=0.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p><strong>Definition</strong> (Semismoothness)</p>
<p>Let <span class="math inline">\(\Omega\)</span> be an open subset of <span class="math inline">\(\mathcal E\)</span>. Let <span class="math inline">\(f:\Omega\rightarrow \mathbb R\)</span> be a locally Lipschitz continuous function (not necessarily convex).</p>
<ol type="1">
<li><p><span class="math inline">\(f\)</span> is said to be <strong>semismooth</strong> at <span class="math inline">\(x\)</span> if it is directionally differentiable at <span class="math inline">\(x\)</span> and <span class="math display">\[
f(x+h)-f(x)-\langle \nabla f(x+h),h\rangle =o(||h||)\quad \forall h\rightarrow 0,x+h\in D_F.
\]</span></p></li>
<li><p>Moreover, <span class="math inline">\(f\)</span> is said to be <strong>strongly semismooth</strong> at <span class="math inline">\(x\)</span> if <span class="math display">\[
f(x+h)-f(x)-\langle \nabla f(x+h),h\rangle =O(||h||^2)\quad \forall h\rightarrow 0,x+h\in D_F.
\]</span></p></li>
</ol>
<p><strong>Theorem</strong></p>
<p>Any convex function <span class="math inline">\(f:\mathcal E\rightarrow \mathbb R\)</span> is semismooth.</p>
<p><strong>Definition</strong> (Directional differentiability)</p>
<p>A function <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is said to be directionally differentiable at <span class="math inline">\(x\)</span> if for any <span class="math inline">\(h\in\mathcal X\)</span>, <span class="math inline">\(F&#39;(x;h)\)</span> exists, where <span class="math display">\[
F&#39;(x;h)=\lim_{t\downarrow 0} \frac{F(x+th)-F(x)}{t}.
\]</span> It is possible to have a function f of two variables <span class="math inline">\(x,y\)</span> and a point <span class="math inline">\((x_0,y_0)\)</span> in the domain of <span class="math inline">\(f\)</span> such that <span class="math inline">\(f\)</span> has a directional derivative in every direction at <span class="math inline">\((x_0,y_0)\)</span>, but f is not differentiable at <span class="math inline">\((x_0,y_0)\)</span>, i.e., the gradient vector of <span class="math inline">\(f\)</span> at <span class="math inline">\((x_0,y_0)\)</span> does not exist. If a function is differentiable then all its directional derivatives exist and they can be easily computed from the derivative.</p>
<p><strong>Example</strong></p>
<p>Define <span class="math inline">\(f:\mathbb R^2\rightarrow \mathbb R\)</span> by <span class="math inline">\(f(x,y)=\frac{x^2y}{x^4+y^2}\)</span> when <span class="math inline">\((x,y)\neq (0,0)\)</span> and <span class="math inline">\(f(0,0)=0\)</span>, otherwise.</p>
<p>This function is not continuous at <span class="math inline">\((0,0)\)</span> and thus not differentiable at <span class="math inline">\((0,0)\)</span>.</p>
<p>Let <span class="math inline">\(U=(u_1,u_2)\in\mathbb R^2\)</span>, and <span class="math inline">\(||U||=1\)</span>. <span class="math display">\[
\lim_{t\downarrow 0}\frac{f(0+tU)-f(0)}{t}=\lim_{t\downarrow 0}\frac{u_1^2u_2}{t^2u_1^4+u_2^2}=0,\text{if }u_2=0,\text{and }\frac{u_1^2}{u_2},\text{if }u_2\neq 0.
\]</span> <strong>Definition</strong> (B-differentiability)</p>
<p>A function <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is said to be B-differentiable at <span class="math inline">\(x\)</span> if there exits a function <span class="math inline">\(BF(x):\mathcal X\rightarrow \mathcal Y\)</span>, called the B-derivative of <span class="math inline">\(F\)</span> at <span class="math inline">\(x\)</span>, which is positively homogeneous of degree 1, i.e., <span class="math inline">\(BF(x)(\lambda h)=\lambda BF(x)h\)</span> for all <span class="math inline">\(h\in\mathcal X\)</span> and <span class="math inline">\(\lambda \ge 0\)</span>, such that <span class="math display">\[
\lim_{h\rightarrow 0}\frac{F(x+h)-F(x)-BF(x)h}{||h||}=0.
\]</span> <strong>Proposition</strong></p>
<p>Suppose that <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is locally Lipschitz continuous. Then <span class="math inline">\(F\)</span> is B-differentiable at <span class="math inline">\(x\)</span> if and only if it is directionally differentiable at <span class="math inline">\(x\)</span>. In this case, the B-derivative and the directional derivate are identical, i.e., for any <span class="math inline">\(h\in\mathcal X\)</span>, <span class="math display">\[
BF(x)h=F&#39;(x;h).
\]</span> <strong>Definition</strong> (G-semismoothness)</p>
<p>A locally Lipschitz function <span class="math inline">\(F:\mathcal X\rightarrow \mathcal Y\)</span> is said to be G-semismooth at <span class="math inline">\(x\)</span> if for any <span class="math inline">\(h\rightarrow 0\)</span>, we have for any <span class="math inline">\(V\in\partial F(x+h)\)</span>, <span class="math display">\[
F(x+h)-F(x)-Vh=o(||h||).
\]</span> If the above condition is replaced by <span class="math display">\[
F(x+h)-F(x)-Vh=O(||h||^2),
\]</span> then <span class="math inline">\(F\)</span> is said to be strongly G-semismooth at <span class="math inline">\(x\)</span>.</p>
<p>In the above definition, if <span class="math inline">\(F\)</span> is also directionally differentiable at <span class="math inline">\(x\)</span>, then <span class="math inline">\(F\)</span> is said to be strongly semismooth at <span class="math inline">\(x\)</span>.</p>
<p><strong>Theorem</strong> (Local Convergence)</p>
<p>Let <span class="math inline">\(\bar x\in\mathcal X\)</span> be a solution to <span class="math inline">\(F(x)=0\)</span>. Suppose that</p>
<ol type="1">
<li><span class="math inline">\(T_F(\bar x)\)</span> is nonsingular, i.e., all <span class="math inline">\(V\in T_F(\bar x)\)</span> are nonsingular.</li>
<li><span class="math inline">\(F\)</span> is G-semismooth at <span class="math inline">\(\bar x\)</span>.</li>
</ol>
<p>Then there exists an open neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> such that if <span class="math inline">\(x^0\in\mathcal N(\bar x)\)</span>, the whole sequence <span class="math inline">\(\{x^k\}\)</span> generated by Newton's method is well-defined and converges to <span class="math inline">\(\bar x\)</span> supperlinearly, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=o(||x^k-\bar x||).
\]</span> Furthermore, if <span class="math inline">\(F\)</span> is strongly G-semismooth at <span class="math inline">\(\bar x\)</span>, then the convergence is quadratic, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=O(||x^k-\bar x||^2).
\]</span> When the dimension <span class="math inline">\(n\)</span> is very large, from the computational point of view, it is not advisable to solve the Newton linear system exactly. This motivation leads to an inexact version of Newton's method.</p>
<p><strong>Algorithm</strong> (Inexact local semismooth Newton method)</p>
<p>Given <span class="math inline">\(x^0\in\mathcal X\)</span>, <span class="math inline">\(\tau\in(0,1]\)</span>, and <span class="math inline">\(\eta\in(0,1)\)</span>, perform the following steps for <span class="math inline">\(k=0,1,\dotsm\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k\in T_F(x^k)\)</span>. Compute an approximate Newton direction <span class="math inline">\(d^k\)</span> of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span> by solving the linear equations <span class="math display">\[
F(x^k)+V_kd=0,
\]</span> such that <span class="math display">\[
||F(x^k)+V_kd^k||\le \eta_k||F(x^k)||,
\]</span> where <span class="math inline">\(\eta_k=\min\{\eta,||F(x^k)||^\tau\}\)</span>.</p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+d^k\)</span>.</p></li>
</ol>
<p><strong>Remark</strong></p>
<ol type="1">
<li><span class="math inline">\(F(x^k)+V_kd=0\)</span> can be solved by conjugate gradient method or Gauss elimination.</li>
<li><span class="math inline">\(\eta_k=\min\{\eta,||F(x^k)||^\tau\}\)</span> means when the residual error <span class="math inline">\(||F(x^k)||\)</span> is too big, we just use <span class="math inline">\(\eta\)</span>.</li>
</ol>
<p><strong>Theorem</strong> (Local Convergence)</p>
<p>Let <span class="math inline">\(\bar x\in\mathcal X\)</span> be a solution to <span class="math inline">\(F(x)=0\)</span>. Suppose that</p>
<ol type="1">
<li><span class="math inline">\(T_F(\bar x)\)</span> is nonsingular, i.e., all <span class="math inline">\(V\in T_F(\bar x)\)</span> are nonsingular.</li>
<li><span class="math inline">\(F\)</span> is G-semismooth at <span class="math inline">\(\bar x\)</span>.</li>
</ol>
<p>Then there exists an open neighborhood <span class="math inline">\(\mathcal N(\bar x)\)</span> of <span class="math inline">\(\bar x\)</span> such that if <span class="math inline">\(x^0\in\mathcal N(\bar x)\)</span>, the whole sequence <span class="math inline">\(\{x^k\}\)</span> generated by Newton's method is well-defined and converges to <span class="math inline">\(\bar x\)</span> supperlinearly, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=o(||x^k-\bar x||).
\]</span> Furthermore, if <span class="math inline">\(F\)</span> is strongly G-semismooth at <span class="math inline">\(\bar x\)</span>, then the convergence is of the order <span class="math inline">\(1+\tau\)</span>, i.e., for <span class="math inline">\(k\)</span> sufficiently large, <span class="math display">\[
||x^{k+1}-\bar x||=O(||x^k-\bar x||^{1+\tau}).
\]</span> When the dimension <span class="math inline">\(n\)</span> is very large, from the computational point of view, it is not advisable to solve the Newton linear system exactly. This motivation leads to an inexact version of Newton's method.</p>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Set-valued map</th>
<th style="text-align: center;">Smooth single-valued function</th>
<th style="text-align: center;">Non-smooth single-valued function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Upper semi-continuity</td>
<td style="text-align: center;">Continuity</td>
<td style="text-align: center;">Continuity</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Semismoothness</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G-semismoothness</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Directional differentiability</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B-differentiability</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2 id="globalization-of-semismooth-newton-methods-for-equation">9.5 Globalization of semismooth Newton methods for equation</h2>
<p>Define <span class="math display">\[
f(x)=\frac{1}{2}||F(x)||^2.
\]</span> In some special cases, <span class="math inline">\(f\)</span> can be continuously differentiable though <span class="math inline">\(F\)</span> itself is not smooth. For example, for a closed convex cone <span class="math inline">\(\mathcal C\subset \mathbb R^n\)</span>, <span class="math inline">\(F(x)=\Pi_{\mathcal C}(x)\)</span>, <span class="math inline">\(x\in\mathbb R^n\)</span>, is not differentiable, while <span class="math inline">\(f\)</span> is continuously differentiable on <span class="math inline">\(\mathbb R^n\)</span>. For the case when <span class="math inline">\(f\)</span> is continuously differentiable, we state an inexact version of the damped semismooth Newton method.</p>
<p><strong>Algorithm</strong> (iDSN: Inexact damped semismooth Newton method)</p>
<p>Given <span class="math inline">\(x^0 \in\mathcal X\)</span>, <span class="math inline">\(\tau \in(0,1]\)</span>, <span class="math inline">\(\eta \in(0,1)\)</span>, and <span class="math inline">\(\rho\in(0,1/2)\)</span>, <span class="math inline">\(\sigma\in(0,1/8)\)</span>, perform the following steps for <span class="math inline">\(k=0,1\dotsm,\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k \in\partial F(x^k)\)</span>. Compute an approximate Newton direction <span class="math inline">\(d^k\)</span> of <span class="math inline">\(F\)</span> at <span class="math inline">\(x^k\)</span> by solving the linear equations, <span class="math display">\[
F(x^k)+V_kd=0
\]</span> such that <span class="math display">\[
|| F(x^k)+V_kd^k||\le \eta_k||F(x^k)||
\]</span> where <span class="math inline">\(\eta_k =\min\{\eta,||F(x^k)||^\tau\}\)</span>. If the above condition is not achievable or if the condition <span class="math display">\[
\langle \nabla f(x^k),d^k\rangle \le -\beta||d^k||^p\quad \text{($\beta&gt;0$ and $p&gt;1$ are given constants)}
\]</span> is not satisfied, let <span class="math inline">\(d=-\nabla f(x^k)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(m_k\)</span> be the smallest nonnegative integer <span class="math inline">\(m\)</span> such that <span class="math display">\[
f(x^k+\rho^m d^k)-f(x^k)\le \sigma \rho^m\langle \nabla f(x^k),d^k\rangle.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+\rho^{m_k} d^k\)</span>.</p></li>
</ol>
<h2 id="a-semismooth-newton-method-for-minimization-of-sc1-functions">9.6 A semismooth Newton method for minimization of SC1 functions</h2>
<p>We consider the unconstrained optimization problem <span class="math display">\[
\min\{f(x)\;|\; x\in\mathbb R^n\}
\]</span> where <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> is an <span class="math inline">\(LC^1\)</span> function (a continuously differentiable function whose gradient is locally Lipschitz continuous). Furthermore, an <span class="math inline">\(LC^1\)</span> function is said to be <span class="math inline">\(SC^1\)</span> if its gradient is semismooth. We denote the generalized Hessian, i.e., the generalized Jacobian of <span class="math inline">\(\nabla f(x)\)</span> at <span class="math inline">\(x\)</span> by <span class="math inline">\(\partial ^2 f(x)\)</span>. For the <span class="math inline">\(LC^1\)</span> functions, a second-order Taylor-like expansion is available, as shown in the proposition.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> be an <span class="math inline">\(LC^1\)</span> function on the open set <span class="math inline">\(\mathcal O\)</span> and suppose the closed line segment <span class="math inline">\([x;y]\)</span> is contained in <span class="math inline">\(\mathcal O\)</span>. Then <span class="math display">\[
f(y)=f(x)+\langle \nabla f(x),y-x\rangle+\frac{1}{2}\langle y-x,V(y-x)\rangle,
\]</span> where <span class="math inline">\(V\in\partial ^2 f(z)\)</span>, for some <span class="math inline">\(z\in(x;y)\)</span>.</p>
<p><strong>Algorithm</strong> (GSN: globalized semismooth Newton method)</p>
<p>Given <span class="math inline">\(x^0 \in\mathcal X\)</span>, <span class="math inline">\(\tau \in(0,1]\)</span>, <span class="math inline">\(\eta,\beta \in(0,1)\)</span>, and <span class="math inline">\(\rho,\sigma\in(0,1/2)\)</span>, perform the following steps for <span class="math inline">\(k=0,1\dotsm,\)</span></p>
<ol type="1">
<li><p>Pick an element <span class="math inline">\(V_k \in\partial^2 f(x^k)\)</span>. Compute an approximate Newton direction <span class="math inline">\(d^k\)</span> of <span class="math inline">\(\nabla f\)</span> at <span class="math inline">\(x^k\)</span> by solving the linear equations, <span class="math display">\[
\nabla f(x^k)+V_kd=0
\]</span> such that <span class="math display">\[
|| \nabla f(x^k)+V_kd^k||\le \eta_k||\nabla f(x^k)||
\]</span> where <span class="math inline">\(\eta_k =\min\{\eta,||F(x^k)||^\tau\}\)</span>. If the above condition is not achievable or if the condition <span class="math display">\[
\langle \nabla f(x^k),d^k\rangle \le -\beta_k||d^k||^p\quad \text{($p&gt;1$ are given constants)}
\]</span> is not satisfied, let <span class="math display">\[
d^k=-B_k^{-1}\nabla f(x^k).
\]</span> In the above, <span class="math inline">\(\beta_k=\min\{\beta,\|\nabla f(x^k)\|\}\)</span> and <span class="math inline">\(B_k\)</span> is any real symmetric positive definite matrix.</p></li>
<li><p>Let <span class="math inline">\(m_k\)</span> be the smallest nonnegative integer <span class="math inline">\(m\)</span> such that <span class="math display">\[
f(x^k+\rho^m d^k)-f(x^k)\le \sigma \rho^m\langle \nabla f(x^k),d^k\rangle.
\]</span></p></li>
<li><p>Set <span class="math inline">\(x^{k+1}=x^k+\rho^{m_k} d^k\)</span>.</p></li>
</ol>
<h2 id="an-inexact-semismooth-newton-method-for-convex-sc1-minimization-problems">9.7 An inexact semismooth Newton method for convex SC1 minimization problems</h2>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/MA6268 Nonlinear Optimization/8.4 A symmetric Gauss-Seidel based semi-proximal ADMMALM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/12/22/MA6268 Nonlinear Optimization/8.4 A symmetric Gauss-Seidel based semi-proximal ADMMALM/" class="post-title-link" itemprop="url">8. ADMM methods for convex composite conic programming (2)</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:48:40" itemprop="dateCreated datePublished" datetime="2020-12-22T13:48:40+08:00">2020-12-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-11-21 20:27:26" itemprop="dateModified" datetime="2019-11-21T20:27:26+08:00">2019-11-21</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MA6268-Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">MA6268 Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2020/12/22/MA6268 Nonlinear Optimization/8.4 A symmetric Gauss-Seidel based semi-proximal ADMMALM/" class="post-meta-item leancloud_visitors" data-flag-title="8. ADMM methods for convex composite conic programming (2)" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/12/22/MA6268 Nonlinear Optimization/8.4 A symmetric Gauss-Seidel based semi-proximal ADMMALM/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/12/22/MA6268 Nonlinear Optimization/8.4 A symmetric Gauss-Seidel based semi-proximal ADMMALM/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="a-symmetric-gauss-seidel-based-semi-proximal-admmalm">8.4 A symmetric Gauss-Seidel based semi-proximal ADMM/ALM</h2>
<p>Let us consider the following convex composite quadratic programming, <span class="math display">\[
\text{(P)}\quad \begin{array}{rCl}
\min &amp; \theta(y_1)+f(y_1,y_2,\dotsm,y_p)+\varphi(z_1)+g(z_1,z_2,\dotsm,z_q)\\
\text{subject to}&amp; \mathcal A_1^* y_1+\mathcal A_2^*y_2+\dotsm+\mathcal A_p^*y_p+\mathcal B_1^*z_1+\mathcal B_2^*z_2+\dotsm+\mathcal B_q^*z_q=c,
\end{array}
\]</span> where</p>
<ol type="1">
<li><span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are given nonnegative integers, <span class="math inline">\(c\in\mathcal X\)</span> is a given vector.</li>
<li><span class="math inline">\(\theta:\mathcal Y_1\rightarrow (-\infty,\infty]\)</span> and <span class="math inline">\(\varphi:\mathcal Z_1\rightarrow (-\infty,\infty]\)</span> are simple closed proper convex functions in the sense that their proximal mapping can be relatively <strong>easy</strong> to compute.</li>
<li><span class="math inline">\(f:\mathcal Y_1\times \mathcal Y_2\times \dotsm\times \mathcal Y_p\rightarrow \mathbb R\)</span> and <span class="math inline">\(g:\mathcal Z_1\times \mathcal Z_2\times \dotsm\times \mathcal Z_q\rightarrow \mathbb R\)</span> are convex quadratic functions.</li>
<li><span class="math inline">\(\mathcal A_i:\mathcal X\rightarrow \mathcal Y_i,\;i=1,\dotsm,p\)</span> and <span class="math inline">\(\mathcal B_j:\mathcal X\rightarrow \mathcal Z_j,\;j=1,\dotsm,q\)</span> are linear maps.</li>
<li><span class="math inline">\(\mathcal Y_1.\dotsm,\mathcal Y_p\)</span>, <span class="math inline">\(\mathcal Z_1,\dotsm,\mathcal Z_q\)</span> and <span class="math inline">\(\mathcal X\)</span> are real finite dimensional Euclidean spaces each equipped with an inner product <span class="math inline">\(\langle \cdot,\cdot\rangle\)</span> and its induced norm <span class="math inline">\(||\cdot||\)</span>.</li>
</ol>
<p>Define <span class="math inline">\(\mathcal Y=\mathcal Y_1\times \mathcal Y_2\times \dotsm\times \mathcal Y_p\)</span> and <span class="math inline">\(\mathcal Z=\mathcal Z_1\times \mathcal Z_2\times \dotsm\times \mathcal Z_q\)</span>. Define <span class="math inline">\(y=(y_1,\dotsm,y_p)\in\mathcal Y\)</span> and <span class="math inline">\(z=(z_1,\dotsm,z_q)\in\mathcal Z\)</span>. Define the linear maps <span class="math inline">\(\mathcal A:\mathcal X\rightarrow \mathcal Y\)</span> and <span class="math inline">\(\mathcal B:\mathcal X\rightarrow \mathcal Z\)</span> such that the adjoint maps are given by <span class="math display">\[
\mathcal A^* y=\sum_{i=1}^p \mathcal A_i^* y_i,\;\forall y\in\mathcal Y\\
\mathcal B^* y=\sum_{j=1}^p \mathcal B_j^* z_j,\;\forall z\in\mathcal Z.
\]</span> Then the problem can be denoted in a more compact form, <span class="math display">\[
\text{(P)}\quad \begin{array}{rCl}
\min &amp; \theta(y_1)+f(y)+\varphi(z_1)+g(z)\\
\text{subject to}&amp; \mathcal A^*y+\mathcal B^*z=c.
\end{array}
\]</span> Assume the convex quadratic functions <span class="math inline">\(f:\mathcal Y\rightarrow \mathbb R\)</span> and <span class="math inline">\(g:\mathcal Z\rightarrow \mathbb R\)</span> are given by <span class="math display">\[
f(y)=\frac{1}{2}\langle y,\mathcal Py\rangle -\langle b_f,y\rangle\\
g(z)=\frac{1}{2}\langle z,\mathcal Qz\rangle -\langle b_g,z\rangle.
\]</span> Here <span class="math inline">\(\mathcal P\)</span> and <span class="math inline">\(\mathcal Q\)</span> are self-adjoint positive semidefinite linear operator defined on <span class="math inline">\(\mathcal Y\)</span> and <span class="math inline">\(\mathcal Z\)</span>, respectively. For later discussion, we write <span class="math display">\[
\mathcal P=\left(
\begin{array}{cccc}
\mathcal P_{11} &amp; \mathcal P_{12} &amp; \dotsm &amp; \mathcal P_{1p}\\
\mathcal P_{12}^* &amp; \mathcal P_{22} &amp; \dotsm &amp; \mathcal P_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\mathcal P_{p1}^* &amp; \mathcal P_{p2}^* &amp; \dotsm &amp; \mathcal P_{pp}
\end{array}
\right)\quad 
\mathcal Q=\left(
\begin{array}{cccc}
\mathcal Q_{11} &amp; \mathcal Q_{12} &amp; \dotsm &amp; \mathcal Q_{1p}\\
\mathcal Q_{12}^* &amp; \mathcal Q_{22} &amp; \dotsm &amp; \mathcal Q_{2p}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
\mathcal Q_{q1}^* &amp; \mathcal Q_{q2}^* &amp; \dotsm &amp; \mathcal Q_{qq}
\end{array}
\right),
\]</span> where <span class="math inline">\(\mathcal P_{ij}:\mathcal Y_j\rightarrow \mathcal Y_i\)</span> for <span class="math inline">\(i=1,\dotsm,p\)</span>, <span class="math inline">\(j\le i\)</span> and <span class="math inline">\(\mathcal Q_{mn}:\mathcal Z_n\rightarrow \mathcal Z_m\)</span> for <span class="math inline">\(m=1,\dotsm,q\)</span>, <span class="math inline">\(n\le m\)</span> are linear operators. For simplicity, we further write <span class="math display">\[
\theta_f(y)=\theta(y_1)+f(y)\\
\varphi_g(z)=\varphi (z_1)+g(z).
\]</span> Let <span class="math inline">\(\sigma &gt;0\)</span> be given. The augmented Lagrangian function is given as, for any <span class="math inline">\((y,z,x)\in\mathcal Y,\mathcal Z,\mathcal X\)</span>, <span class="math display">\[
\mathcal L_\sigma (y,z;x)=\theta_f(y)+\varphi_g(z)+\langle x,\mathcal A^* y+\mathcal B^*z-c\rangle +\frac{\sigma}{2}||\mathcal A^* y+\mathcal B^* z-c||^2.
\]</span> We first introduce two self-adjoint semidefinite linear operators <span class="math inline">\(\mathcal S_1\)</span> and <span class="math inline">\(\mathcal T_1\)</span> to handle the convex, possibly nonsmoothed, functions <span class="math inline">\(\theta(y_1)\)</span> and <span class="math inline">\(\varphi(z_1)\)</span>. Let <span class="math inline">\(\mathcal F_1\)</span> and <span class="math inline">\(\mathcal S_1\)</span> be self-adjoint operator on <span class="math inline">\(\mathcal Y_1\)</span> such that <span class="math display">\[
\mathcal F_1=\mathcal S_1+\sigma^{-1}\mathcal P_{11}+\mathcal A_1\mathcal A_1^*\succ 0
\]</span> and the following well-defined optimization problem (subproblem), <span class="math display">\[
\min _{y_1} \theta(y_1)+\frac{\sigma}{2}||y_1-\bar y_1||^2_{\mathcal F_1}
\]</span> can be easily solved for any <span class="math inline">\(\bar y_1\in\text{dom}(\theta)\)</span>. Similarly, define the self-adjoint semidefinite linear operator <span class="math inline">\(\mathcal G_1\)</span> and <span class="math inline">\(\mathcal T_1\)</span> on <span class="math inline">\(\mathcal Z_1\)</span> such that <span class="math display">\[
\mathcal G_1=\mathcal T_1+\sigma ^{-1}\mathcal Q_{11}+\mathcal B_1\mathcal B_1^*\succ0
\]</span> and the optimal solution to the following problem, <span class="math display">\[
\min_{z_1} \varphi(z_1)+\frac{\sigma}{2}||z_1-\bar z_1||^2_{\mathcal G_1}
\]</span> can be easily obtained for any <span class="math inline">\(\bar z_1\in\text{dom}(\varphi)\)</span>. Then for <span class="math inline">\(i=2,\dotsm,p\)</span>, let <span class="math inline">\(\mathcal F_i\)</span> be a self-adjoint positive definite linear operator such that it is a majorization of <span class="math inline">\(\sigma^{-1}\mathcal P_{ii}+\mathcal A_i\mathcal A_i^*\)</span>, i.e., <span class="math display">\[
\mathcal F_i\succeq \sigma^{-1}\mathcal P_{ii}+\mathcal A_i\mathcal A_i^*,\quad i=1,\dotsm,p.
\]</span> In practice, we would choose <span class="math inline">\(\mathcal F_i\)</span> in such a way that its inverse can be computed at a moderate cost. Define <span class="math display">\[
\mathcal S_i=\mathcal F_i-\sigma^{-1}\mathcal P_{ii}-\mathcal A_i\mathcal A_i^*\succeq 0,\quad i=1,\dotsm,p.
\]</span> Note that for numerical efficiency, we need the self-adjoint positive semidefinite linear operator <span class="math inline">\(\mathcal S_i\)</span> to be as small as possible for each <span class="math inline">\(i=1,\dotsm,p\)</span>. Similarly, for <span class="math inline">\(j=2,\dotsm,p\)</span>, let <span class="math inline">\(\mathcal G_j\)</span> be a self-adjoint positive definite linear operator on <span class="math inline">\(\mathcal Z_j\)</span> that majorizes <span class="math inline">\(\sigma^{-1}\mathcal Q_{jj}+\mathcal B_j\mathcal B_j^*\)</span> in such a way that <span class="math inline">\(\mathcal G^{-1}\)</span> can be computed relatively easily. Define <span class="math display">\[
\mathcal T_j=\mathcal G_j-\sigma ^{-1}\mathcal Q_{jj}-\mathcal B_j\mathcal B_K^*\succeq 0,\quad j=1,\dotsm,q.
\]</span> <strong>My thinking</strong></p>
<p>Firstly, let us see how we can use sPADMM to solve this problem. We only analyze one of the blocks of the sPADMM here. <span class="math display">\[
\begin{array}{rcl}y^{k+1}&amp;=&amp;\arg\min_y\left\{\mathcal L_\sigma(y,z^k;x^k)+\frac{1}{2}||y-y^k||_{\mathcal S_\mathcal M}\right\}\\&amp;=&amp; \arg\min_y\left\{\theta(y_1)+\frac{1}{2}\langle y,\mathcal Py\rangle -\langle b_f,y\rangle+\frac{\sigma}{2}||\mathcal A^*y+\mathcal B^*z^{k}-c+\sigma^{-1}x^{k}||^2+\frac{1}{2}||y-y^k||_{\mathcal S_\mathcal M}\right\}\\&amp;=&amp; \arg\min_y\left\{\theta(y_1)+\frac{1}{2}\langle y,(\mathcal P+\sigma \mathcal A\mathcal A^*)y\rangle-\langle b,y\rangle+\frac{1}{2}||y-y^k||_{\mathcal S_\mathcal M}\right\}\\\end{array}.
\]</span> Define <span class="math display">\[
\mathcal H=\left(\begin{array}{cccc}\mathcal P_{11} &amp; \mathcal P_{12} &amp; \dotsm &amp; \mathcal P_{1p}\\\mathcal P_{12}^* &amp; \mathcal P_{22} &amp; \dotsm &amp; \mathcal P_{2p}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\mathcal P_{p1}^* &amp; \mathcal P_{p2}^* &amp; \dotsm &amp; \mathcal P_{pp}\end{array}\right)+\sigma \left(\begin{array}{cccc}\mathcal A_1\mathcal A_1^* &amp; \mathcal A_1\mathcal A_2^* &amp; \dotsm &amp; \mathcal A_1\mathcal A_p^*\\\mathcal A_2\mathcal A_1^* &amp; \mathcal A_2\mathcal A_2^* &amp; \dotsm &amp; \mathcal A_2\mathcal A_p^*\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\mathcal A_p\mathcal A_1^* &amp; \mathcal A_p\mathcal A_2^* &amp; \dotsm &amp; \mathcal A_p\mathcal A_p^*\\\end{array}\right)\\=\left(\begin{array}{cccc}\mathcal P_{11}+\sigma \mathcal A_1\mathcal A_1^* &amp; \mathcal P_{12}+\sigma \mathcal A_1\mathcal A_2^* &amp; \dotsm &amp; \mathcal P_{1p}+\sigma \mathcal A_1\mathcal A_p^*\\\mathcal P_{21}+\sigma \mathcal A_2\mathcal A_1^* &amp; \mathcal P_{22}+\sigma \mathcal A_2\mathcal A_2^* &amp; \dotsm &amp; \mathcal P_{2p}+\sigma \mathcal A_2\mathcal A_p^*\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\mathcal P_{p1}+\sigma \mathcal A_p\mathcal A_1^* &amp; \mathcal P_{p2}+\sigma \mathcal A_p\mathcal A_2^* &amp; \dotsm &amp; \mathcal P_{pp}+\sigma \mathcal A_p\mathcal A_p^*\\\end{array}\right).
\]</span> And define <span class="math display">\[
\mathcal U&#39;=\left(\begin{array}{cccc}0 &amp; \mathcal P_{12}+\sigma \mathcal A_1\mathcal A_2^* &amp; \dotsm &amp; \mathcal P_{1p}+\sigma \mathcal A_1\mathcal A_p^*\\0 &amp; 0 &amp; \dotsm &amp; \mathcal P_{2p}+\sigma \mathcal A_2\mathcal A_p^*\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0 &amp; 0 &amp; \dotsm &amp; 0\\\end{array}\right)\\\mathcal D&#39;=\left(\begin{array}{cccc}\mathcal P_{11}+\sigma \mathcal A_1\mathcal A_1^* &amp; 0 &amp; \dotsm &amp; 0\\0 &amp; \mathcal P_{22}+\sigma \mathcal A_2\mathcal A_2^* &amp; \dotsm &amp; 0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0 &amp; 0 &amp; \dotsm &amp; \mathcal P_{pp}+\sigma \mathcal A_p\mathcal A_p^*\\\end{array}\right).
\]</span> Then sGS method can be applied to solve this subproblem if <span class="math inline">\(\mathcal S_\mathcal M\)</span> is chosen as <span class="math inline">\({\mathcal S_\mathcal M} = \mathcal U&#39;(\mathcal D&#39;)^{-1} (\mathcal U&#39;)^*\)</span>.</p>
But if this decomposition is applied, the subproblem to <span class="math inline">\(y^{k+1}_1\)</span> will be very difficult to solve because of the non-smooth function <span class="math inline">\(\theta(y_1)\)</span>. The subproblem is denoted as $$
<span class="math display">\[\begin{array}{rcl}
y_{1}^{k+1}&amp;=&amp;\operatorname{argmin}_{y_{1}} \mathcal{L}_{\sigma}\left(\left(y_{1}, \bar{y}_{\geq 2}^{k}\right), z^{k} ; x^{k}\right),\\
&amp;=&amp; \operatorname{argmin}_{y_{1}} \theta(y_1)+f(y)+\varphi_g(z)+\langle x,\mathcal A^* y+\mathcal B^*z-c\rangle +\frac{\sigma}{2}||\mathcal A^* y+\mathcal B^* z-c||^2\\

&amp;=&amp; \operatorname{argmin}_{y_{1}} \theta(y_1)+\frac{1}{2}\langle y,\mathcal Py\rangle -\langle b_f,y\rangle+\langle x,\mathcal A^* y\rangle +\frac{\sigma}{2}||\mathcal A^* y+\mathcal B^* z-c||^2\\

&amp;=&amp; \operatorname{argmin}_{y_{1}} \theta(y_1)+\frac{1}{2}\langle y,(\mathcal P+\sigma \mathcal A^*\mathcal A)y\rangle -\langle b,y\rangle
\end{array}\]</span>
<span class="math display">\[
which is a non-smooth function. To solve this problem is as difficult as to solve the original problem. Therefore we hope to introduce the regulation part to each of the subproblems, especially the subproblem to  $y^{k+1}_1$, to make the subproblems easy to solve. The form we hope the subproblem to be is
\]</span>
<span class="math display">\[\begin{array}{rcl}
y_{1}^{k+1}
&amp;=&amp; \operatorname{argmin}_{y_{1}} \theta(y_1)+||y_1-y_1^k||^2_\mathcal {SS},
\end{array}\]</span>
<p>$$ where <span class="math inline">\(\mathcal {SS}\)</span> is chosen from some positive definite symmetric matrices such that this problem can become a proximal operator to the function <span class="math inline">\(\theta\)</span>. Now let us see how we can achieve this goal.</p>
<p>We consider the following problem by introducing another regulation part <span class="math inline">\(\mathcal S\)</span>. <span class="math display">\[
\begin{array}{rcl}
y^{k+1}&amp;=&amp; \arg\min_y\left\{\theta(y_1)+\frac{1}{2}\langle y,(\mathcal P+\sigma \mathcal A\mathcal A^*)y\rangle-\langle b,y\rangle
+\underbrace{\frac{1}{2}||y-y^k||_{\mathcal S}}_{\text{New Part}}+\frac{1}{2}||y-y^k||_{\mathcal S_\mathcal M}\right\}\\\end{array},
\]</span> where <span class="math inline">\(\mathcal S=\text{diag}(\mathcal S_1,\mathcal S_2,\dotsm,\mathcal S_p)\)</span>.</p>
<p>Then we need to modify <span class="math inline">\(\mathcal D\)</span> such that the sGS can be applied. Therefore, <span class="math inline">\(\mathcal D&#39;\)</span> is redefined as <span class="math display">\[
\mathcal D&#39;=\left(\begin{array}{cccc}\mathcal S_1+\mathcal P_{11}+\sigma \mathcal A_1\mathcal A_1^* &amp; 0 &amp; \dotsm &amp; 0\\0 &amp; \mathcal S_2+\mathcal P_{22}+\sigma \mathcal A_2\mathcal A_2^* &amp; \dotsm &amp; 0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0 &amp; 0 &amp; \dotsm &amp; \mathcal S_p+\mathcal P_{pp}+\sigma \mathcal A_p\mathcal A_p^*\\\end{array}\right).
\]</span> If we hope to be consistent with the above text, we need to let the <span class="math inline">\(\sigma\)</span> go outside.</p>
<p>Define <span class="math inline">\(\sigma \mathcal D=\mathcal D&#39;\)</span>, and <span class="math inline">\(\sigma \mathcal U=\mathcal U&#39;\)</span>. Then we have <span class="math display">\[
\mathcal D=\left(\begin{array}{cccc}\mathcal S_1+\sigma^{-1}\mathcal P_{11}+ \mathcal A_1\mathcal A_1^* &amp; 0 &amp; \dotsm &amp; 0\\0 &amp; \mathcal S_2+\sigma^{-1}\mathcal P_{22}+ \mathcal A_2\mathcal A_2^* &amp; \dotsm &amp; 0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0 &amp; 0 &amp; \dotsm &amp; \mathcal S_p+\sigma^{-1}\mathcal P_{pp}+ \mathcal A_p\mathcal A_p^*\\\end{array}\right)\\
\mathcal U=\left(\begin{array}{cccc}
0 &amp; \sigma^{-1}\mathcal P_{12}+ \mathcal A_1\mathcal A_2^* &amp; \dotsm &amp; \sigma^{-1}\mathcal P_{1p}+ \mathcal A_1\mathcal A_p^*\\
0 &amp; 0 &amp; \dotsm &amp; \sigma^{-1}\mathcal P_{2p}+ \mathcal A_2\mathcal A_p^*\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0 &amp; 0 &amp; \dotsm &amp; 0
\end{array}\right).
\]</span> If we choose <span class="math inline">\(\mathcal S_i=\mathcal F_i-\sigma^{-1}\mathcal P_{ii}-\mathcal A_i\mathcal A_i^*\succeq 0,\quad i=1,\dotsm,p\)</span>, then we have <span class="math display">\[
\mathcal D=
\begin{bmatrix}
\mathcal F_1 &amp; 0 &amp; \dotsm &amp; 0\\
0 &amp; \mathcal F_2 &amp; \dotsm &amp; 0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0 &amp; 0 &amp; \dotsm &amp; \mathcal F_p
\end{bmatrix}.
\]</span> Then we have <span class="math inline">\(\mathcal S_\mathcal M=\mathcal U\mathcal D^{-1}\mathcal U^*\)</span>, and the subproblems we need to solve can be denoted as <span class="math display">\[
\begin{array}{rcl}
\bar y_i^k &amp;=&amp;\arg\min _{y_i}\mathcal L_\sigma ((y^k_{\le i-1},y_i,\bar y^k_{\ge i+1}),z^k;x^k)+\frac{\sigma}{2}||y_i-y_i^k||_{\mathcal S_i}^2\\
y_1^{k+1}&amp;=&amp;\arg\min_{y_1}\mathcal L_\sigma((y_1,\bar y^k_{\ge 2},z_k;x^k)+\frac{\sigma}{2}||y_1-y_1^k||_{\mathcal S_1}^2. 
\end{array}
\]</span></p>
<p><strong>Proposition</strong></p>
<p>For any <span class="math inline">\(k\ge 0\)</span>, the point <span class="math inline">\((x^{k+1},y^{k+1},z^{k+1})\)</span> obtained by sGS-sPADMM for solving the problem can be generated exactly according to the following iteration, <span class="math display">\[
\begin{array}{rcl}
y^{k+1}&amp;=&amp; \arg\min_y\{\mathcal L_\sigma (y,z^k;x^k)+\frac{\sigma}{2}||y-y^k||^2_{\mathcal S+\mathcal S_\mathcal M}\}\\
z^{k+1}&amp;=&amp; \arg\min_z\{\mathcal L_\sigma (y^{k+1},z;x^k)+\frac{\sigma}{2}||y-y^k||^2_{\mathcal T+\mathcal T_\mathcal N}\}\\
x^{k+1}&amp;=&amp; x^k+\tau\sigma (\mathcal A^* y^{k+1}+\mathcal B^* z^{k+1}-c).
\end{array}
\]</span> Proof.</p>
<p>It is straightforward to prove it.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/35/">35</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
