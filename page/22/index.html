<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="keywords" content="Optimization, Machine Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Cheng-Zilong">
<meta property="og:url" content="http://yoursite.com/page/22/index.html">
<meta property="og:site_name" content="Cheng-Zilong">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cheng-Zilong">
  <link rel="canonical" href="http://yoursite.com/page/22/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Cheng-Zilong</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cheng-Zilong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Learning Notes</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/13/Convex Optimization/5. Duality/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/07/13/Convex Optimization/5. Duality/" class="post-title-link" itemprop="url">Chapter 5. Duality</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-07-13 13:49:01 / Modified: 13:57:57" itemprop="dateCreated datePublished" datetime="2019-07-13T13:49:01+08:00">2019-07-13</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Convex-Optimization/" itemprop="url" rel="index"><span itemprop="name">Convex Optimization</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="duality">5. Duality</h1>
<h2 id="the-lagrange-dual-function">5.1 The Lagrange dual function</h2>
<h3 id="the-lagrangian">5.1.1 The Lagrangian</h3>
<p>We consider the optimization problem (we do not assume convex) in the standard form <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f_0(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
f_i(x)\le 0\\h_i(x)=0
\end{array}
\end{array}.
\]</span> The basic idea in Lagrangian duality is to take the constraints in (5.1) into account by augmenting the objective function with a weighted sum of the constraint functions. <span class="math display">\[
L(x,\lambda,v)=f_0(x)+\sum_{i=1}^m \lambda_if_i(x)+\sum_{i=1}^p v_i h_i(x).
\]</span> The vectors <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(v\)</span> are called <strong>dual variables</strong> or <strong>Lagrange Multiplier vectors</strong>.</p>
<h3 id="the-lagrange-dual-function-1">5.1.2 The Lagrange dual function</h3>
<p>We define the Lagrange dual function as the minimum value of the Lagrangian over x <span class="math display">\[
g(\lambda,v)=\inf_{x\in \mathcal D}\left(f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^p v_ih_i(x)\right).
\]</span> Since the dual function is the pointwise infimum of a family of affine functions of <span class="math inline">\((\lambda,v)\)</span>, it is concave even when the original problem is not convex.</p>
<h3 id="lower-bounds-on-optimal-value">5.1.3 Lower bounds on optimal value</h3>
<p>The dual function yields the lower bounds on the optimal value <span class="math inline">\(p^\star\)</span> of the problem. For any <span class="math inline">\(\lambda \succeq 0\)</span> and any <span class="math inline">\(v\)</span> we have <span class="math display">\[
g(\lambda,v)\le p^\star
\]</span> This important property is easily verified, suppose <span class="math inline">\(\tilde x\)</span> is a feasible point for the problem, i.e. <span class="math inline">\(f_i(\tilde x)\le 0\)</span> and <span class="math inline">\(h_i(\tilde x)=0\)</span>. Then we have <span class="math display">\[
\sum_{i=1}^m \lambda_if_i(\tilde x)+\sum_{i=1}^p v_ih_i(\tilde x)\le 0.
\]</span> Then we have <span class="math display">\[
g(\lambda,v)=\inf_{x\in \mathcal D}\left(f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^p v_ih_i(x)\right)\le L(\tilde x,\lambda,v)\le f_0(\tilde x).
\]</span> The equality holds, but is vacuous when <span class="math inline">\(g(\lambda,v) = -\infty\)</span>. The dual function gives a nontrivial lower bound on <span class="math inline">\(p^\star\)</span> only when <span class="math inline">\(\lambda \succeq 0\)</span> and <span class="math inline">\((\lambda,v) \in \text{dom} g\)</span>.</p>
<p>We refer to a pair <span class="math inline">\((\lambda, v)\)</span> with <span class="math inline">\(\lambda \succeq 0\)</span> and <span class="math inline">\((\lambda,v) \in \text{dom} g\)</span> as dual feasible to let <span class="math inline">\(g(\lambda,v)&gt; -\infty\)</span>.</p>
<h3 id="linear-approximation-interpretation">5.1.4 Linear approximation interpretation</h3>
<p>The Lagrangian and lower bound property can be given a simple interpretation.</p>
<p>We first rewrite the origin problem as an unconstrained problem <span class="math display">\[
\text{minimize }f_0(x)+\sum_{i=1}^mI_-(f_i(x))+\sum_{i=1}^pI_0(h_i(x))
\]</span> where <span class="math display">\[
I_-(u)=\begin{cases}0&amp;u\le 0\\\infty &amp;u&gt;0\end{cases}
\]</span> and similar for <span class="math inline">\(I_0(u)\)</span>.The two indicator functions can be interpreted as expressing out displeasure associated with a constraint function. Then we can use <span class="math inline">\(\lambda_if_i(x)\)</span> and <span class="math inline">\(v_ih_i(x)\)</span> to replace the indicator functions to show the displeasure. Then the objective becomes the Lagrangian <span class="math display">\[
\text{minimize }f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^p v_ih_i(x)
\]</span> and the dual function value <span class="math inline">\(g(\lambda,v)\)</span> is the optimal value of the problem.</p>
<h3 id="examples">5.1.5 Examples</h3>
<p><strong>Least-squares solution of linear equations</strong> <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
x^Tx
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}.
\]</span> Then the Lagrangian <span class="math inline">\(L(x,v)=x^Tx +v^T(Ax-b)\)</span>.</p>
<p>The dual function <span class="math inline">\(g(v)=\inf_x L(x,v)=\inf_x {(x^Tx +v^T(Ax-b))}\)</span>.</p>
<p>We hope to derive the analytical expression <span class="math display">\[
\nabla_xL(x,v)=2x+A^Tv=0\\
x=-\frac{1}{2}A^Tv\\
g(v)=\frac{1}{4}v^TAA^Tv+v^T(-\frac{1}{2}AA^Tv-b)=-\frac{1}{4}v^TAA^Tv-v^Tb,
\]</span> which is a concave quadratic function.</p>
<p><strong>Standard form LP</strong> <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
c^Tx
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b\\x\succeq 0
\end{array}
\end{array}.
\]</span> The Lagrangian <span class="math inline">\(L(x,\lambda,v)= c^Tx -\lambda^T x+v^T(Ax-b)\)</span>.</p>
<p>The dual function <span class="math inline">\(g(\lambda,v) = \inf_x L(x,\lambda,v)\)</span>, <span class="math display">\[
\inf_x(c^Tx -\lambda^T x+v^T(Ax-b))=-v^Tb+\inf_x(c^T-\lambda^T+v^TA)x.
\]</span> Therefore <span class="math display">\[
g(\lambda,v)=
\begin{cases}
-v^Tb&amp;c^T-\lambda^T+v^TA=0\\
-\infty&amp;\text{otherwise.}
\end{cases}
\]</span></p>
<h3 id="the-lagrange-dual-function-and-conjugate-functions">5.1.6 The Lagrange dual function and conjugate functions</h3>
<p>Recall that the conjugate <span class="math inline">\(f^*\)</span> of a function <span class="math inline">\(f\)</span> is given by <span class="math display">\[
f^*(y)=\sup_{x\in domf}(y^Tx-f(x)).
\]</span> The conjugate function and Lagrange dual function are closely related.</p>
<p>Consider an optimization problem with linear inequality and equality constraints, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f_0(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax\preceq b\\Cx=d
\end{array}
\end{array}.
\]</span> We can write the dual function as <span class="math display">\[
g(\lambda,v)=\inf _x(f_0(x)+\lambda^T(Ax-b)+v^T(Cx-d))\\
=-b^T\lambda-d^Tv+\inf_x(f_0(x)+(A^T\lambda+C^Tv)^Tx)\\
=-b^T\lambda-d^Tv-f_0^*(-A^T\lambda-C^Tv).
\]</span></p>
<h2 id="the-lagrange-dual-problem">5.2 The Lagrange dual problem</h2>
<p>For each pair <span class="math inline">\((\lambda,v)\)</span> with <span class="math inline">\(\lambda\ge 0\)</span>, the Lagrange dual function gives us a lower bound on the optimal value <span class="math inline">\(p^\star\)</span> of the optimization problem. We hope to know the best lower bound , <span class="math display">\[
\begin{array}{rl}
\text{maxmize} &amp;g(\lambda,v)\\
\text{subject to}&amp;\lambda\succeq 0.
\end{array}
\]</span> This problem is called <strong>Lagrange dual problem</strong>. The original problem is sometimes called the <strong>primal problem</strong>. We refer to <span class="math inline">\((\lambda^\star,v^\star)\)</span> as dual optimal or optimal Lagrange multipliers.</p>
<h3 id="making-dual-constraint-explicit">5.2.1 Making dual constraint explicit</h3>
<p>We have known that the Lagrange dual function for the standard form LP <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
c^Tx
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b\\x\succeq0
\end{array}
\end{array},
\]</span> is given by <span class="math display">\[
g(\lambda,v)=\begin{cases}-b^Tv&amp;A^Tv-\lambda+c=0\\-\infty &amp; \text{otherwise}.\end{cases}
\]</span> We can form an equivalent problem by making these equality constraints explicit <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
b^Tv
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
A^Tv-\lambda+c=0\\ \lambda\succeq 0
\end{array}
\end{array}.
\]</span> The problem can be further simplified as <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
b^Tv
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
A^Tv+c\succeq 0
\end{array}
\end{array}.
\]</span></p>
<h3 id="weak-duality">5.2.2 Weak duality</h3>
<p>The optimal value of the Lagrange dual problem, which we denote as <span class="math inline">\(d^\star\)</span>. We have the simple but important inequality <span class="math display">\[
d^\star \le p^\star
\]</span> which holds even if the original problem is not convex. This property is called <strong>weak duality</strong>.</p>
<p>The weak duality inequality holds when <span class="math inline">\(d^\star\)</span> and <span class="math inline">\(p^\star\)</span> are infinite. For example, if the primal problem is unbounded below, so that <span class="math inline">\(p^\star=-\infty\)</span>, we must have <span class="math inline">\(d^\star =-\infty\)</span>, i.e. the Lagrange dual problem is infeasible. Conversely, if the dual problem is unbounded above, the primal problem is infeasible.</p>
<p>We refer to the difference <span class="math inline">\(p^\star -d^\star\)</span> as the optimal <strong>duality gap</strong>.</p>
<h3 id="strong-duality-and-slaters-constraint-qualification">5.2.3 Strong duality and Slater's constraint qualification</h3>
<p>If the equality <span class="math display">\[
d^\star=p^\star 
\]</span> holds, we say that strong duality holds.</p>
<p>Strong duality does not hold in general. But if the primal problem is convex, we usually have strong duality. There are many results that establish conditions on the problem under which strong duality holds. These conditions are called constraint qualifications.</p>
<p><strong>Slater's Condition</strong></p>
<p>There exists an <span class="math inline">\(x\in \text{relint }\mathcal D\)</span> such that if the first <span class="math inline">\(k\)</span> constraint functions <span class="math inline">\(f_1,\dotsm,f_k\)</span> are affine, then <span class="math display">\[
\begin{array}{rcl}
f_i(x)&amp;\le&amp;0,&amp;i=1,\dotsm,k\\
f_i(x)&amp;&lt;&amp;0,&amp;i=k+1,\dotsm,m\\
Ax&amp;=&amp;b.
\end{array}
\]</span> Slater's theorem states that strong duality holds if Slater's condition holds.</p>
<h3 id="examples-1">5.2.4 Examples</h3>
<p>Least-squares solution of equations, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
x^Tx
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}.
\]</span> The associated dual problem is <span class="math display">\[
\text{maxmize }-(1/4)v^TAA^Tv-b^Tv.
\]</span> We have <span class="math display">\[
p^\star=d^\star.
\]</span></p>
<h3 id="mixed-strategies-for-matrix-games">5.2.5 Mixed strategies for matrix games</h3>
<p>In this section, we use strong duality to derive a basic result for zero-sum matrix games. The players use randomized strategies, then the expected payoff from player 1 to player 2 is then <span class="math display">\[
\sum_{k=1}^n\sum_{l=1}^m u_kv_lP_{kl}=u^TPv.
\]</span> Player 1 wishes to choose u to minimize <span class="math inline">\(u^TPv\)</span> while the player 2 wishes to maximize.</p>
<p>If player 1 knows the strategy of player 2, (player 2 will choose <span class="math inline">\(v\)</span> to maximize <span class="math inline">\(u^TPv\)</span>, which results in the following) <span class="math display">\[
\sup\{u^TPv| v\succeq 0,1^Tv=1\}=\max_{i=1,\dotsm,m} (P^Tu)_i.
\]</span> The best thing player 1 can do is to minimize the worst case, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
\max_{i=1,\dotsm,m} (P^Tu)_i
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
u\succeq 0&amp;1^Tu=1
\end{array}
\end{array}.
\]</span> Similarly, if player 2 knows the strategy of player 1, we have <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
\min_{i=1,\dotsm,m} (Pv)_i
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
v\succeq 0&amp;1^Tv=1
\end{array}
\end{array}.
\]</span></p>
<p>It can be shown that the dual gap is zero.</p>
<h2 id="geometric-interpretation">5.3 Geometric interpretation</h2>
<h3 id="weak-and-strong-duality-via-set-of-values">5.3.1 Weak and strong duality via set of values</h3>
<p>We can give a simple geometric interpretation of the dual function in terms of the set <span class="math display">\[
\mathcal G=\{f_1(x),\dotsm,f_m(x),h_1(x),\dotsm,h_p(x),f_0(x)\in\mathbb R^m\times \mathbb R^p\times \mathbb R\;|\;x\in \mathcal D\},
\]</span> which is the set of values taken on by the constraint and objective functions. The optimal value <span class="math inline">\(p^*\)</span> can be expressed in terms of <span class="math inline">\(\mathcal G\)</span> as <span class="math display">\[
p^*=\inf\{t\;|\; (u,v,t)\in \mathcal G,\; u\preceq 0,\; v=0\}.
\]</span> If we only consider one inequality constraint, we have the Lagrangian function, <span class="math display">\[
L(\lambda,x)=t(x)+\lambda u(x),\quad u(x)\le 0,\lambda\ge0.
\]</span> Then the primal problem is to minimize <span class="math inline">\(t(x)\)</span>. The dual problem is to maximize <span class="math inline">\(g(\lambda)=\inf\{\lambda u+t\}\)</span>=<span class="math inline">\(\inf ((\lambda,1)^T(u,t))\)</span>, which is a supporting plane with slope <span class="math inline">\(-\lambda\)</span>. <span class="math inline">\(u=0\)</span> gives <span class="math inline">\(g(\lambda)\)</span>.</p>
<h3 id="proof-of-strong-duality-under-constraint-qualification">5.3.2 Proof of strong duality under constraint qualification</h3>
<h3 id="multicriterion-interpretation">5.3.3 Multicriterion interpretation</h3>
<h2 id="saddle-point-interpretation">5.4 Saddle-point interpretation</h2>
<p>You can refer to the notes for <strong>Nonlinear Optimization</strong>.</p>
<h2 id="optimality-conditions">5.5 Optimality conditions</h2>
<h3 id="certificate-of-suboptimality-and-stopping-criteria">5.5.1 Certificate of suboptimality and stopping criteria</h3>
<p>If we can find a dual feasible <span class="math inline">\((\lambda,v)\)</span>, we establish a lower bound on the optimal value of the primal problem: <span class="math inline">\(p^\star \ge g(\lambda,v)\)</span>. Dual feasible points allow us to bound how suboptimal a given feasible point is, without knowing the exact value of <span class="math inline">\(p^\star\)</span> <span class="math display">\[
f_0(x)-p^\star\le f_0(x)-g(\lambda,v).
\]</span> In particular, this establishes that <span class="math inline">\(x\)</span> is <span class="math inline">\(\epsilon-\)</span>suboptimal with <span class="math inline">\(\epsilon= f_0(x)-g(\lambda,v)\)</span>.</p>
<p>This can be used as a stop criteria.</p>
<h3 id="complementary-slackness">5.5.2 Complementary slackness</h3>
<p>If the strong duality holds, Let <span class="math inline">\(x^\star\)</span> be a primal optimal and <span class="math inline">\((\lambda^\star, v^\star)\)</span> be a dual optimal point. This means that <span class="math display">\[
\begin{array}{rcl}
f_0(x^\star)&amp;=&amp;g(\lambda^\star,v^\star)\\
&amp;=&amp;\displaystyle{\inf_x(f_0(x)+\sum_{i=1}^m\lambda_i^\star f_i(x)+\sum_{i=1}^pv_i^\star h_i(x)}\\
&amp;\le&amp; \displaystyle{f_0(x^\star)+\sum_{i=1}^m\lambda_i^\star f_i(x^\star)+\sum_{i=1}^pv_i^\star h_i(x^\star)}\\
&amp;\le&amp; f_0(x^\star).
\end{array}
\]</span> We conclude that two inequalities in this chain hold with equality.</p>
<p>That means <span class="math display">\[
\begin{array}{rcl}
h_i(x^\star)&amp;=&amp;0\\
\displaystyle\sum_{i=1}^m \lambda_i^\star f_i(x^\star)&amp;=&amp;0.
\end{array}
\]</span> The second equality means <span class="math display">\[
\lambda_i^\star &gt;0\implies f_i(x^\star)=0\\
f_i(x^\star)&lt;0 \implies \lambda_i^\star=0
\]</span></p>
<h3 id="kkt-optimality-conditions">5.5.3 KKT optimality conditions</h3>
<p>We now assume that the functions are differentiable, but we make no assumptions yet about convexity.</p>
<p><strong>KKT condition for problems</strong></p>
<p>Since <span class="math inline">\(x^\star\)</span> minimizes <span class="math inline">\(L(x,\lambda^\star,v^\star)\)</span>, the gradient must vanish. Thus we have <span class="math display">\[
\begin{array}{rcl}
f_i(x^\star)&amp;\le&amp; 0\\
h_i(x^\star) &amp;=&amp;0\\
\lambda_i^\star&amp;\ge&amp; 0\\
\lambda_i^\star f_i(x^\star)&amp;=&amp;0\\
\nabla f_0(x^\star)+\sum_{i=1}^m \lambda_i^\star \nabla f_i(x^\star)+\sum_{i=1}^p v_i^\star \nabla h_i(x^\star)&amp;=&amp;0
\end{array}
\]</span> which are called the Karush-Kuhn-Tucker (KKT) conditions.</p>
<p>To summarize, for any optimization problem with differentiable objective and constraint functions for which strong duality obtains, any pair of primal and dual optimal points must satisfy the KKT conditions.</p>
<p>When the primal problem is convex, the KKT conditions are also sufficient for the points to be primal and dual optimal.</p>
<h3 id="mechanic-interpretation-of-kkt-conditions">5.5.4 Mechanic interpretation of KKT conditions</h3>
<p>The potential energy in the springs as a function of the block positions, is given by <span class="math display">\[
f_0(x_1,x_2)=\frac{1}{2}k_1x_1^2+\frac{1}{2}k_2(x_2-x_1)^2+\frac{1}{2}k_3(l-x_2)^2.
\]</span> The equilibrium position <span class="math inline">\(x^\star\)</span> is the position that minimizes the potential energy subject to the inequalities <span class="math display">\[
w/2-x_1\le 0\\
w+x_1-x_2\le 0\\
w/2-l+x_2\le 0.
\]</span> The KKT conditions for the problem consist of the constraints, <span class="math inline">\(\lambda_i\ge 0\)</span>, the complementary slackness conditions <span class="math display">\[
\lambda_1(w/2-x_1)= 0\\
\lambda_2(w+x_1-x_2)= 0\\
\lambda_3(w/2-l+x_2)= 0,
\]</span> and the zero gradient condition <span class="math display">\[
\begin{bmatrix}
k_1x_1-k_2(x_2-x_1)\\k_2(x_2-x_1)-k_3(l-x_2)
\end{bmatrix}+
\lambda_1
\begin{bmatrix}
-1\\0
\end{bmatrix}+
\lambda_2
\begin{bmatrix}
1\\-1
\end{bmatrix}+
\lambda_3
\begin{bmatrix}
0\\1
\end{bmatrix}=0.
\]</span> The gradient condition can be interpreted as the force balance equations for the two blocks.</p>
<p><span class="math inline">\(\lambda_1\)</span> is the force from the left wall, <span class="math inline">\(\lambda_2\)</span> is the force between two blocks' contact, <span class="math inline">\(\lambda_3\)</span> is the force from the right wall. If no contact, the force will be zero, which is decided by the slackness conditions.</p>
<h3 id="solving-the-primal-problem-via-the-dual">5.5.5 Solving the primal problem via the dual</h3>
<p>Suppose we have strong duality and an optimal <span class="math inline">\((\lambda^\star, v^\star)\)</span> is known. Suppose the solution of <span class="math display">\[
\text{minimize }f_0(x)+\sum_{i=1}^m\lambda_i^\star f_i(x)+\sum_{i=1}^pv_i^\star h_i(x),
\]</span> is unique. Then if the solution is primal feasible, it must be primal optimal. If it is not primal feasible, then no primal optimal point can exist.</p>
<h2 id="perturbation-and-sensitivity-analysis">5.6 Perturbation and sensitivity analysis</h2>
<p>You can refer to the notes for <strong>Nonlinear Optimization</strong>.</p>
<h2 id="examples-2">5.7 Examples</h2>
<p>In this section, we show by examples that simple equivalent reformulations of a problem can lead to very different dual problems. We will consider the following reformulations.</p>
<ol type="1">
<li>Introducing new variables and associated equality constraints.</li>
<li>Replacing the objective with an increasing function of the original objective.</li>
<li>Making explicit constraints implicit.</li>
</ol>
<h3 id="introducing-new-variables">5.7.1 Introducing new variables</h3>
<p>Consider an unconstrained problem of the form <span class="math display">\[
\text{minimize}\quad f_0(Ax+b).
\]</span> Its Lagrange dual function is the constant <span class="math inline">\(p^*\)</span>. So while we do have the strong duality, the Lagrangian dual is neither useful nor interesting.</p>
<p>Now reformulate the problem as <span class="math display">\[
\begin{array}{rl}
\text{minimize}&amp; f_0(y)\\
\text{subject to}&amp; Ax+b=y.
\end{array}
\]</span> Then the Lagrangian of the reformulated problem is <span class="math display">\[
L(x,y,v)=f_0(y)+v^T(Ax+b-y).
\]</span> To find the dual function, we minimize <span class="math inline">\(L\)</span> over <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. <span class="math display">\[
\begin{array}{rcl}
g(v)&amp;=&amp;b^Tv+\inf_{x,y} \{f_0(y)+v^T(Ax-y)\}\\
&amp;=&amp;b^Tv+\inf_{y}\{f_0(y)-v^Ty\}\quad(v^TA=0)\\
&amp;=&amp;b^Tv-f_0^*(v)\quad(v^TA=0)\\
\end{array}
\]</span> Therefore, the dual problem is <span class="math display">\[
\begin{array}{rl}
\text{minimize}&amp; b^Tv-f_0^*(v)\\
\text{subject to}&amp; A^Tv=0.
\end{array}
\]</span> Thus, the dual problem of the reformulated problem is considerably more useful than the dual of the original problem.</p>
<p>This idea of introducing new equality constraints can be applied to the constraint functions as well.</p>
<h3 id="transforming-the-objective">5.7.2 Transforming the objective</h3>
<p>We consider the minimum norm problem <span class="math display">\[
\text{minimize}\quad ||Ax+b||.
\]</span> We reformulate the problem as <span class="math display">\[
\begin{array}{rl}
\text{minimize}&amp; \frac{1}{2}||y||^2\\
\text{subject to}&amp; Ax-b=y.
\end{array}
\]</span> Then the dual of the problem is <span class="math display">\[
\begin{array}{rl}
\text{minimize}&amp; -\frac{1}{2}||v||_*^2+b^Tv\\
\text{subject to}&amp; A^Tv=0.
\end{array}
\]</span></p>
<h3 id="implicit-constraints">5.7.3 Implicit constraints</h3>
<h2 id="theorems-of-alternatives">5.8 Theorems of alternatives</h2>
<h3 id="weak-alternatives-via-the-dual-function">5.8.1 Weak alternatives via the dual function</h3>
<p>In this section, we apply Lagrange duality theory to the problem of determining feasibility of a system of inequalities and equalities. We can think of this problem as the standard problem with objective function <span class="math inline">\(f_0=0\)</span>, <span class="math display">\[
\begin{array}{rl}
\text{minimize}&amp; 0\\
\text{subject to}&amp; f_i(x)\le 0, &amp;i=1,\dotsm,m\\
&amp;h_i(x)=0,&amp;i=1,\dotsm,p.
\end{array}
\]</span> This problem has optimal value <span class="math display">\[
p^*=\begin{cases}0&amp;\text{feasible}\\ \infty&amp;\text{infeasible}.\end{cases}
\]</span> <strong>The dual function</strong> <span class="math display">\[
g(\lambda,v)=\inf_{x\in\mathcal D}\left(\sum_{i=1}^m \lambda_if_i(x)+\sum_{i=1}^pv_ih_i(x)\right),
\]</span> If the inequality system <span class="math display">\[
\lambda \succeq 0,\quad g(\lambda,v)&gt;0,
\]</span> is feasible. Then the primal problem is infeasible. If the primal problem is feasible, then the dual problem is infeasible. Two systems of inequalities (equalities) are called weak alternatives if at most one of the two is feasible.</p>
<h3 id="strong-alternatives">5.8.2 Strong alternatives</h3>
<h2 id="generalized-inequalities">5.9 Generalized inequalities</h2>
<p>In this section, we examine how Lagrange duality extends to a problem with generalized inequality constraints, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f_0(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
f_i(x)\preceq_{K_i} 0&amp;i=1,\dotsm,m\\
h_i(x)=0 &amp; i=1,\dotsm,p
\end{array}
\end{array},
\]</span> where <span class="math inline">\(K_i\subseteq \mathbb R^{k_i}\)</span> are proper cones.</p>
<h3 id="the-lagrange-dual">5.9.1 The Lagrange dual</h3>
<p>With the generalized inequality <span class="math inline">\(f_i(x)\preceq_{k_i}0\)</span>, we associate a Lagrange multiplier vector <span class="math inline">\(\lambda_i\in\mathbb R^{k_i}\)</span> and define the associated Lagrangian as <span class="math display">\[
L(x,\lambda,\mu)=f_0(x)+\lambda_1^Tf_1(x)+\dotsm+\lambda_m^T f_m(x)+v_1h_1(x)+\dotsm+v_ph_p(x),
\]</span> where <span class="math inline">\(\lambda=(\lambda_1,\dotsm,\lambda_m)\)</span> and <span class="math inline">\(v=(v_1,\dotsm,v_p)\)</span>. The dual function is defined as <span class="math display">\[
g(\lambda,v)=\inf_{x\in\mathcal D}L(x,\lambda,v).
\]</span> As in a problem with scalar inequalities, the dual function gives the lower bounds on <span class="math inline">\(p^*\)</span>, the optimal value of the primal problem. For a problem with scalar inequalities, we require <span class="math inline">\(\lambda_i\ge 0\)</span>. Here the nonnegativity requirement on the dual variables is replaced by the condition, <span class="math display">\[
\lambda_i\succeq_{K_i^*} 0,\quad \forall i=1,\dotsm,m,
\]</span> where <span class="math inline">\(K_{i}^*\)</span> denotes the dual cone of <span class="math inline">\(K_i\)</span>. In other words, the Lagrange multipliers associated with the inequalities must be <strong>dual nonnegative</strong>.</p>
<p><strong>Weak duality</strong> follows immediately from the definition of dual cone. If <span class="math inline">\(\lambda_i\succeq_{k_i^*}0\)</span> and <span class="math inline">\(f_i(\tilde x)\preceq_{k_i}0\)</span>, then <span class="math inline">\(\lambda_if_i(\tilde x)\le 0\)</span>.</p>
<p>The Lagrange dual optimization problem is <span class="math display">\[
\begin{array}{|l|ll|}
\text{maximize}\quad &amp; 
\begin{array}{lll}
g(\lambda,v)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
\lambda_i\succeq_{k_i^*}0,&amp;i=1,\dotsm,m
\end{array}
\end{array}.
\]</span> We always have weak duality whether or not the primal problem is convex.</p>
<p><strong>Slater’s condition and strong duality</strong></p>
<p>Strong duality holds when the primal problem is convex and satisfies an appropriate constraint problem. For example, a generalized version of Slater’s condition for the problem <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f_0(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
f_i(x)\preceq_{K_i} 0&amp;i=1,\dotsm,m\\
h_i(x)=0 &amp; i=1,\dotsm,p
\end{array}
\end{array},
\]</span> where <span class="math inline">\(f_0(x)\)</span> is convex and <span class="math inline">\(f_i\)</span> is <span class="math inline">\(K_i\)</span>-convex, is that there exists an <span class="math inline">\(x\in\text{relint}\mathcal D\)</span> with <span class="math inline">\(Ax=b\)</span> and <span class="math inline">\(f_i(x)\prec_{k_i}0,\forall i=1,\dotsm,m\)</span>. This condition implies strong duality.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/12/Introduction to Hilbert Spaces with Applications/1.5 Banach Spaces/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/07/12/Introduction to Hilbert Spaces with Applications/1.5 Banach Spaces/" class="post-title-link" itemprop="url">Chapter 1-2. Banach Spaces</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-07-12 13:48:00 / Modified: 14:58:30" itemprop="dateCreated datePublished" datetime="2019-07-12T13:48:00+08:00">2019-07-12</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Introduction-to-Hilbert-Spaces-with-Applications/" itemprop="url" rel="index"><span itemprop="name">Introduction to Hilbert Spaces with Applications</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="banach-spaces">1.5 Banach Spaces</h2>
<p>Every Cauchy sequence of <strong>numbers</strong> converges. Every absolutely convergent series of <strong>numbers</strong> converges. Similar properties of a normed space would be of great importance. Not all normed spaces have the above properties. Those which do are called Banach Spaces.</p>
<p>==Definition 1.5.1== Cauchy Sequence</p>
<p>A sequence of vectors <span class="math inline">\((x_n)\)</span> in a normed space is called a Cauchy sequence if for every <span class="math inline">\(\varepsilon&gt;0\)</span> there exists a number M such that <span class="math inline">\(||x_m-x_n||&lt;\varepsilon\)</span> for all <span class="math inline">\(m,n&gt;M\)</span>.</p>
<p>==Theorem 1.5.1==</p>
<p>The following conditions are equivalent:</p>
<ol type="a">
<li><p><span class="math inline">\((x_n)\)</span> is a Cauchy sequence.</p></li>
<li><p><span class="math inline">\(||x_{p_n}-x_{q_n}||\rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>, for every pair of increasing sequences of positive integers <span class="math inline">\((p_n)\)</span> and <span class="math inline">\((q_n)\)</span></p></li>
<li><p><span class="math inline">\(||x_{p_{n+1}}-x_{p_n}||\rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span> for every increasing sequence of positive integers <span class="math inline">\((p_n)\)</span></p></li>
</ol>
<p>Proof. clearly <span class="math inline">\((a)\rightarrow (b)\rightarrow (c)\)</span></p>
<p>we only need to prove <span class="math inline">\((c)\rightarrow (a)\)</span></p>
<p>If <span class="math inline">\((x_n)\)</span> is not a Cauchy sequence, then <span class="math inline">\(\exists \epsilon&gt;0,\forall M&gt;0,\exists m,n&gt;M\)</span> such that <span class="math inline">\(||x_m-x_n||\ge \epsilon\)</span>.</p>
<p>That means <span class="math inline">\(\exists \epsilon&gt;0\)</span>, there are infinitely many <span class="math inline">\(m,n\)</span> such that <span class="math inline">\(||x_m-x_n||\ge \epsilon\)</span></p>
<p>Then there exists increasing sequence of positive integers <span class="math inline">\((p_n)\)</span> such that <span class="math inline">\(||x_{p_{n+1}}-x_{p_n}||\not \rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span></p>
<p>We can easily find such sequence. Q.E.D.</p>
<p>Observe that every <strong>convergent sequence</strong> is a <strong>Cauchy sequence</strong>.</p>
<p>If we have <span class="math inline">\(||x_n-x||\rightarrow 0\)</span>, then <span class="math inline">\(||x_{p_n}-x_{q_n}||\le||x_{p_n}-x||+||x_{q_{n}}-x||\rightarrow 0\)</span></p>
<p>The converse is not true. A sequence of the rational number may converge to an irrational number.</p>
<p>==Lemma 1.5.1==</p>
<p>If <span class="math inline">\((x_n)\)</span> is a Cauchy sequence in a normed space, then the sequence of norms <span class="math inline">\((||x_n||)\)</span> converges.</p>
<p>Proof.</p>
<p><span class="math inline">\(x_1,x_2,\dotsm \in E\)</span> and for every <span class="math inline">\(\varepsilon&gt;0\)</span> there exists a number M such that <span class="math inline">\(||x_m-x_n||&lt;\varepsilon\)</span> for all <span class="math inline">\(m,n&gt;M\)</span>. <span class="math display">\[
|\space||x_m||-||x_n||\space|\le||x_m-x_n||\rightarrow 0
\]</span> ==Definition 1.5.2== Banach Space</p>
<p>A normed space <span class="math inline">\(E\)</span> is called complete if every Cauchy sequence in <span class="math inline">\(E\)</span> converges to an element of <span class="math inline">\(E\)</span>. A complete normed space is called a Banach Space.</p>
<p>==Example 1.5.1== The space <span class="math inline">\(l^2\)</span> is complete.</p>
<p>To prove the completeness, we need to prove for any Cauchy sequence <span class="math inline">\(z_1,z_2,\dotsm \in E\)</span> satisfying that</p>
<p><span class="math inline">\(\forall \epsilon &gt;0\)</span>, there exists a number <span class="math inline">\(n_0\)</span> such that for all <span class="math inline">\(m,n\ge n_0\)</span> <span class="math display">\[
||z_m-z_n||^2=\sum_{k=1}^\infty |z_{m,k}-z_{n,k}|^2&lt;\epsilon^2
\]</span> that implies <span class="math inline">\(\forall k\in \mathbb N,\forall\epsilon&gt;0,\exists n_0&gt;0\)</span> <span class="math display">\[
|z_{m,k}-z_{n,k}|&lt;\epsilon\quad \forall m,n&gt;n_0
\]</span> that mean for every k, the sequence <span class="math inline">\((z_{n,k})\)</span> is Cauchy sequence and convergent. Then we denote <span class="math display">\[
z_k=\lim_{n\rightarrow \infty} z_{n,k}\quad k=1,2,\dotsm\quad z=(z_k)
\]</span> We need to prove that there exists an element <span class="math inline">\(z\in E\)</span> such that <span class="math inline">\(\forall \varepsilon&gt;0,\exists M&gt;0,\forall m&gt;M\)</span> such that <span class="math display">\[
||z_m-z||=\sum_{k=1}^\infty |z_{m,k}-z_k|^2&lt;\varepsilon^2
\]</span> By letting $n$, we can prove the second equation by the first equation.</p>
<p>Then we need to prove that <span class="math inline">\(z\in E\)</span> <span class="math display">\[
\sqrt{\sum_{k=1}^\infty|z_k|^2}=\sqrt{\sum_{k=1}^\infty|z_{k}-z_{m,k}+z_{m,k}|^2}\\
\le \sqrt{\sum_{k=1}^\infty|z_{k}-z_{m,k}|^2}+\sqrt{\sum_{k=1}^\infty |z_{m,k}|^2}
&lt; \infty
\]</span> That means <span class="math inline">\(z_m \rightarrow z\)</span></p>
<p>Q.E.D.</p>
<p>==Example 1.5.2== Another important example of a Banach space is the space <span class="math inline">\(\mathscr C([a,b])\)</span> of continuous functions on an interval <span class="math inline">\([a,b]\)</span>.</p>
<p>To prove the completeness, we need to prove for any Cauchy sequence <span class="math inline">\(f_1,f_2,\dotsm \in E\)</span> satisfying that</p>
<p><span class="math inline">\(\forall \epsilon &gt;0\)</span>, there exists a number <span class="math inline">\(n_0\)</span> such that for all <span class="math inline">\(m,n\ge n_0\)</span> <span class="math display">\[
||f_m-f_n||&lt;\epsilon\\
\max_{x\in[a,b]} |f_m(x)-f_n(x)|&lt;\epsilon
\]</span> that implies <span class="math display">\[
|f_m(x)-f_n(x)|&lt;\epsilon \quad \forall n,m\ge n_0,\forall x\in[a,b]
\]</span> that means <span class="math inline">\((f_n(x))\)</span> is Cauchy sequence <span class="math inline">\(\forall x\in[a,b]\)</span>.</p>
<p>We denote <span class="math display">\[
f(x)=\lim_{n\rightarrow \infty } f_n(x),\forall x\in[a,b]
\]</span> Let $n$ , we have <span class="math display">\[
|f_m(x)-f(x)|&lt;\epsilon,\forall m\ge n_0 ,\forall x\in[a,b]\\
\max_{x\in[a,b]}|f_m(x)-f(x)|&lt;\epsilon \\
||f_m-f|&lt;\epsilon
\]</span> Then we prove <span class="math inline">\(f\in E\)</span></p>
<p>Let <span class="math inline">\(\forall x_0\in[a,b]\)</span>. Since <span class="math inline">\(f_{n_0}\)</span> is continuous on <span class="math inline">\([a,b]\)</span>, there exists <span class="math inline">\(\delta &gt;0\)</span> such that <span class="math inline">\(|f_{n_0}(x_0)-f_{n_0}(y)|&lt;\epsilon\)</span> for every <span class="math inline">\(y\in[a,b]\)</span> and <span class="math inline">\(|x_0-y|&lt;\delta\)</span> <span class="math display">\[
|f(x_0)-f(y)|= |f(x_0)-f_{n_0}(x_0)+f_{n_0}(x_0)-f_{n_0}(y)+f_{n_0}(y)-f(y)|\\
\le |f(x_0)-f_{n_0}(x_0)|+|f_{n_0}(x_0)-f_{n_0}(y)|+|f_{n_0}(y)-f(y)|\\
&lt; 3\epsilon
\]</span> whenever <span class="math inline">\(|x_0-y|&lt;\delta\)</span>, that means <span class="math inline">\(f\)</span> is continuous.</p>
<p>Q.E.D.</p>
<p>==Definition 1.5.3== Convergent and Absolutely convergent series</p>
<p>A series <span class="math inline">\(\sum_{n=1}^\infty x_n\)</span> converges in a normed space <span class="math inline">\(E\)</span> if the sequence of partial sums converges in <span class="math inline">\(E\)</span>.</p>
<p>i.e. there exists <span class="math inline">\(x\in E\)</span> such that <span class="math inline">\(||x_1+x_2+\dotsm+x_n-x||\rightarrow 0\)</span> as $n$ .</p>
<p>In that case, we write <span class="math inline">\(\sum_{n=1}^\infty x_n=x\)</span>. If <span class="math inline">\(\sum_{n=1}^\infty ||x_n||&lt;\infty\)</span>, then the series is called absolute convergent.</p>
<p>==Theorem 1.5.2==</p>
<p>A normed space is complete if and only if every absolutely convergent series converges.</p>
<p><span class="math inline">\(\implies\)</span></p>
<p>We will prove this by proving absolutely convergent series is a Cauchy series.</p>
<p>We define a absolutely convergent series. Suppose <span class="math inline">\(x_n\in E\)</span> and <span class="math inline">\(\sum_{n=1}^\infty||x_n||&lt;\infty\)</span> and denote <span class="math display">\[
s_n=\sum_{k=1}^nx_k
\]</span> Because the sequence of partial sums converges in E, for every $&gt;0 $, there exists <span class="math inline">\(k&gt;0\)</span> such that <span class="math display">\[
\sum_{n=k+1}^\infty ||x_n||&lt;\epsilon
\]</span> To show <span class="math inline">\((s_n)\)</span> is a Cauchy sequence, <span class="math inline">\(\forall \epsilon&gt;0,\exists M,\forall m,n&gt;M\)</span> such that <span class="math display">\[
||s_m-s_n||=||x_{n+1}+x_{n+2}+\dotsm+x_m||\le \sum_{r=n+1}^\infty ||x_r||&lt;\epsilon
\]</span> (without loss of generality, we assume <span class="math inline">\(m&gt;n\)</span>)</p>
<p>Since E is complete, <span class="math inline">\(s_n\)</span> converges.</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>We need to prove if every absolutely convergent series in a norm space converges, then the normed space is complete.</p>
<p>Let <span class="math inline">\((x_n)\)</span> be an Cauchy sequence in E and therefore <span class="math inline">\(\forall \epsilon&gt;0,\exists p_k\in N,\forall m,n&gt;p_k\)</span> such that <span class="math display">\[
||x_m-x_n||&lt;2^{-k}
\]</span> without loss of generality, we can assume <span class="math inline">\((p_k)\)</span> is strictly increasing.</p>
<p>Then the series <span class="math inline">\(\sum_{k=1}^\infty (x_{p_{k+1}}-x_{p_k})\)</span> is absolutely convergent and therefore, convergent and therefore, the sequence <span class="math display">\[
x_{p_k}=x_{p_1}+(x_{p_2}-x_{p_1})+(x_{p_3}-x_{p_2})+\dotsm+(x_{p_k}-x_{p_{k-1}})
\]</span> converges to an element <span class="math inline">\(x\in E\)</span></p>
<p>Then <span class="math display">\[
||x_n-x||\le ||x_n-x_{p_n}||+||x_{p_n}-x||\rightarrow 0
\]</span> Q.E.D.</p>
<p>==Theorem 1.5.3==</p>
<p>A closed vector subspace of a Banach space is a Banach space itself.</p>
<h2 id="linear-mappings">1.6 Linear Mappings</h2>
<p>Let <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> be vector spaces, and let L be a mapping from <span class="math inline">\(E_1\)</span> into <span class="math inline">\(E_2\)</span></p>
<p>If <span class="math inline">\(y=L(x)\)</span>, then y is called the ==image== of x. If A is subset of <span class="math inline">\(E_1\)</span>, then <span class="math inline">\(L(A)\)</span> denotes the image of the set <span class="math inline">\(A\)</span>.</p>
<p>If B is a subset of <span class="math inline">\(E_2\)</span>, then <span class="math inline">\(L^{-1}(B)\)</span> denote the ==inverse image== of B. i.e. <span class="math inline">\(L^{-1}(B)\)</span> is the set of all vectors in <span class="math inline">\(E_1\)</span> whose images are elements of B.</p>
<p>We consider the mapping which are defined on a proper subset of a vector space. Then the ==domain== of <span class="math inline">\(L\)</span> will be denoted as <span class="math inline">\(\mathscr D(L)\)</span>. The set <span class="math inline">\(L(\mathscr D(L))\)</span> is called the ==range== of the L and denoted by <span class="math inline">\(\mathscr R (L)\)</span></p>
<p>By the ==null space== of L, denoted by <span class="math inline">\(\mathscr N(L)\)</span>, we mean the set of all vectors <span class="math inline">\(x\in \mathscr D(L)\)</span> such that <span class="math inline">\(L(x)=0\)</span></p>
<p>By the ==graph== if L, denoted by <span class="math inline">\(\mathscr G(L)\)</span>, we mean the subset of <span class="math inline">\(E_1\times E_2\)</span> defined as follows: <span class="math display">\[
\mathscr G(L)=\{(x,y):x\in\mathscr D(L),y=L(x)\}
\]</span> ==Definition 1.6.1== Linear Mappings</p>
<p>A mapping <span class="math inline">\(L:E_1\rightarrow E_2\)</span> is called a linear mapping if <span class="math display">\[
L(\alpha x+\beta y)=\alpha L(x)+\beta L(y)
\]</span> for all <span class="math inline">\(x,y\in E_1\)</span> and all scalars <span class="math inline">\(\alpha,\beta\)</span>.</p>
<p>In the remaining part of this section, we will assume that both spaces <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> are normed spaces.</p>
<p>==Definition 1.6.2== Continuous Mappings</p>
<p>Let <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> be normed spaces. A mapping <span class="math inline">\(F\)</span> from <span class="math inline">\(E_1\)</span> to <span class="math inline">\(E_2\)</span> is called continuous at <span class="math inline">\(x_0\in E_1\)</span> if for any sequence <span class="math inline">\((x_n)\)</span> of elements of <span class="math inline">\(E_1\)</span> convergent to <span class="math inline">\(x_0\)</span>, the sequence <span class="math inline">\((F(x_n))\)</span> converges to <span class="math inline">\(F(x_0)\)</span>. i.e. F is continuous at <span class="math inline">\(x_0\)</span> if <span class="math inline">\(||x_n-x_0||\rightarrow 0\)</span> implies <span class="math inline">\(||F(x_n)-F(x_0)||\rightarrow 0\)</span>. If F is continuous at every <span class="math inline">\(x\in E_1\)</span>, then we simply say that <span class="math inline">\(F\)</span> is continuous.</p>
<p>==Example 1.6.1==</p>
<p>The norm in a normed space <span class="math inline">\(E\)</span> is continuous mapping from <span class="math inline">\(E\)</span> into <span class="math inline">\(R\)</span>. <span class="math display">\[
||x_n-x||\rightarrow 0\implies |\space ||x_n||-||x||\space |\le||x_n-x||\rightarrow 0
\]</span> ==Theorem 1.6.1==</p>
<p>Let <span class="math inline">\(F:E_1\rightarrow E_2\)</span>. The following conditions are equivalent:</p>
<ol type="a">
<li><p><span class="math inline">\(F\)</span> is continuous</p></li>
<li><p>The inverse image <span class="math inline">\(F^{-1}(U)\)</span> of any open set U of <span class="math inline">\(E_2\)</span> is open in <span class="math inline">\(E_1\)</span></p></li>
<li><p>The inverse image <span class="math inline">\(F^{-1}(S)\)</span> of any closed set S of <span class="math inline">\(E_2\)</span> is closed in <span class="math inline">\(E_1\)</span></p></li>
</ol>
<p>Proof.</p>
<p><span class="math inline">\((a)\rightarrow (b)\)</span></p>
<p>If there exists an open set U of <span class="math inline">\(E_2\)</span>, the inverse image <span class="math inline">\(F^{-1}(U)\)</span> is not open in <span class="math inline">\(E_1\)</span></p>
<p>there exists <span class="math inline">\(x\in F^{-1}(U)\)</span>, for all <span class="math inline">\(\varepsilon&gt;0\)</span> such that <span class="math inline">\(B(x,\varepsilon)\)</span> contains elements in <span class="math inline">\(E_1\backslash F^{-1}(U)\)</span></p>
<p>Then we can choose the sequence <span class="math inline">\(x_1,x_2,\dotsm\in E_1\backslash F^{-1}(U)\)</span> satisfying <span class="math inline">\(x_n\in B(x,\frac{1}{n})\)</span> that means <span class="math inline">\(||x_n-x||\rightarrow 0\)</span></p>
<p>However, $y=F(x)U $ that means there exists <span class="math inline">\(\epsilon&gt;0\)</span> such that <span class="math inline">\(B(y,\epsilon) \subseteq U\)</span> . <span class="math inline">\(x_n\notin F^{-1}(U)\)</span> means <span class="math inline">\(F(x_n)\notin U\)</span>.</p>
<p><span class="math inline">\(F(x_n)\)</span> cannot converge to <span class="math inline">\(F(x)\)</span>. Proved.</p>
<p><span class="math inline">\((b)\rightarrow (a)\)</span></p>
<p>If for any open set U of <span class="math inline">\(E_2\)</span>, the inverse image <span class="math inline">\(F^{-1}(U)\)</span> is open in <span class="math inline">\(E_1\)</span>, then F is continuous.</p>
<p>For all <span class="math inline">\(a\in F^{-1}(U)\)</span>, <span class="math inline">\(F(a)\in U\)</span>, <span class="math inline">\(U\)</span> is open and therefore for every <span class="math inline">\(a\)</span>, there exists <span class="math inline">\(\epsilon_1\)</span> such that <span class="math inline">\(B(F(a),\epsilon_1)\subseteq U\)</span> and <span class="math inline">\(a\in F^{-1}(B(F(a),\epsilon_1))\)</span>. There exists <span class="math inline">\(\epsilon_2\)</span> such that <span class="math inline">\(B(a,\epsilon_2) \subset F^{-1}(B(F(a),\epsilon_1))\)</span></p>
<p>That mean <span class="math inline">\(||x-a||&lt;\epsilon_2\)</span> implies <span class="math inline">\(||F(x)-F(a)||&lt;\epsilon_1\)</span>. Q.E.D.</p>
<p>==Theorem 1.6.2==</p>
<p>A linear mapping <span class="math inline">\(L:E_1\rightarrow E_2\)</span> is continuous if and only if it is continuous at a point.</p>
<p>Proof.</p>
<p><span class="math inline">\(\implies\)</span> obviously</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>Assume L is continuous at <span class="math inline">\(x_0 \in E\)</span>. We define sequence <span class="math inline">\(x_1,x_2,\dotsm \in E\)</span> and for any point <span class="math inline">\(x\in E\)</span> such that <span class="math inline">\(||x_n-x ||\rightarrow 0\)</span>.</p>
<p>Since L is linear mapping, we have <span class="math display">\[
||L(x_n-x+x_0-x_0)||=||L(x_n-x+x_0)-L(x_0)||
\]</span> Since L is continuous at <span class="math inline">\(x_0\in E\)</span>, for every sequence converging to <span class="math inline">\(x_0\)</span>, the <span class="math inline">\(L\)</span> converges <span class="math inline">\(L(x_0)\)</span>. We have <span class="math display">\[
||L(x_n-x+x_0)-L(x_0)||\rightarrow 0
\]</span> Q.E.D.</p>
<p>==Definition 1.6.3== Bounded linear mapping</p>
<p>A linear mapping <span class="math inline">\(L:E_1\rightarrow E_2\)</span> is called bounded if there exists a number K such that <span class="math display">\[
||L(x)||\le K||x||
\]</span> for all <span class="math inline">\(x\in E_1\)</span>.</p>
<p>==Theorem 1.6.3==</p>
<p>A linear mapping is continuous if and only if it is bounded.</p>
<p>Proof.</p>
<p><span class="math inline">\(\implies\)</span></p>
<p>If the linear mapping is not bounded, that means for every number K, there exists some x such that <span class="math display">\[
||L(x)||&gt;K||x||
\]</span> then for every <span class="math inline">\(n-1\le|k_n|&lt; n\)</span>, there exists <span class="math inline">\(x_n \in E_1\)</span> such that <span class="math display">\[
||L(x_n)||&gt;k_n||x_n||
\]</span> We define the sequence <span class="math display">\[
y_n=\frac{x_n}{k_n||x_n||}
\]</span> when <span class="math inline">\(n\rightarrow \infty\)</span> <span class="math display">\[
||y_n||=\frac{1}{k_n}\rightarrow 0\\
||L(y_n)||&gt;k_n||y_n||=1
\]</span> Therefore, <span class="math inline">\(L\)</span> is not continuous.</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>If L is bounded, then there exists a number K, for all <span class="math inline">\(x\in E_1\)</span> such that <span class="math display">\[
||L(x)||\le K||x||
\]</span> Then we define a sequence <span class="math inline">\((x_n)\)</span> converging to zero. When <span class="math inline">\(n\rightarrow \infty\)</span>, <span class="math display">\[
||L(x_n)||\le K||x_n||\rightarrow 0
\]</span> By theorem 1.6.2, The linear mapping L is continuous at zero point, then it is continuous linear mapping.</p>
<p>Q.E.D.</p>
<p>The space of all linear mapping from a vector space <span class="math inline">\(E_1\)</span> into a vector space <span class="math inline">\(E_2\)</span> becomes a vector space if addition and multiplication by scalars are defined as follows <span class="math display">\[
(L_1+L_2)(x)=L_1(x)+L_2(x)\\
(\lambda L)(x)=\lambda(L(x))
\]</span> If <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> are normed spaces, then the set of all bounded linear mappings from <span class="math inline">\(E_1\)</span> into <span class="math inline">\(E_2\)</span>, denoted by <span class="math inline">\(\mathscr B(E_1,E_2)\)</span>, is a vector subspace of the space defined above.</p>
<p>==Theorem 1.6.4==</p>
<p>If <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> are normed space, then <span class="math inline">\(\mathscr B(E_1,E_2)\)</span> is a normed space with norm defined by <span class="math display">\[
||L||=\sup_{||x||=1}||L(x)||\quad \forall L\in \mathscr B(E_1,E_2)
\]</span> Proof.</p>
<p>Only show Triangle Inequality</p>
<p><span class="math inline">\(\forall L_1,L_2\in \mathscr B(E_1,E_2)\)</span> <span class="math display">\[
||L_1+L_2||=\sup_{||x||=1}||L_1(x)+L_2(x)||\le \sup_{||x||=1}||L_1(x)||+\sup_{||x||=1}||L_2(x)||=||L_1||+||L_2||
\]</span> Q.E.D.</p>
<p>It follows Theorem 1.6.4 that <span class="math inline">\(||L(x)||\le ||L||||x||\)</span> for all <span class="math inline">\(x\in E_1\)</span>. In fact, <span class="math inline">\(||L||\)</span> is the least number K such that <span class="math inline">\(||L(x)||\le K||x||\)</span> for all <span class="math inline">\(x\in E_1\)</span>.</p>
<p>The norm defined above is the standard norm in <span class="math inline">\(\mathscr B(E_1,E_2)\)</span>. It is usually called the ==operator norm==. Convergence with respect to this norm is called the uniform convergence of operator.</p>
<p>==Theorem 1.6.5==</p>
<p>If <span class="math inline">\(E_1\)</span> is normed space and <span class="math inline">\(E_2\)</span> is a Banach space, then <span class="math inline">\(\mathscr B(E_1,E_2)\)</span> is a Banach space.</p>
<p>Proof.</p>
<p>We only need to show that <span class="math inline">\(\mathscr B(E_1,E_2)\)</span> is complete.</p>
<p>A normed space <span class="math inline">\(\mathscr B(E_1,E_2)\)</span> is called complete if every Cauchy sequence in <span class="math inline">\(\mathscr B(E_1,E_2)\)</span> converges to an element of <span class="math inline">\(\mathscr B(E_1,E_2)\)</span>.</p>
<p>Let <span class="math inline">\((L_n)\)</span> be Cauchy sequence of <span class="math inline">\(\mathscr B(E_1,E_2)\)</span> and Let <span class="math inline">\(\forall x\in E_1\)</span></p>
<p>We have <span class="math display">\[
||L_m(x)-L_n(x)||\le ||L_m-L_n||||x||\rightarrow 0 \quad as\space n,m \rightarrow \infty
\]</span> That means <span class="math inline">\((L_n(x))\)</span> is Cauchy sequence in <span class="math inline">\(E_2\)</span>. By completeness of <span class="math inline">\(E_2\)</span>, we have <span class="math inline">\(L(x)\in E_2\)</span> such that <span class="math display">\[
L(x)=\lim_{n\rightarrow \infty} L_n(x)
\]</span> Then we only need to prove <span class="math inline">\(||L_n-L||\rightarrow 0\)</span> and <span class="math inline">\(L\in \mathscr B(E_1,E_2)\)</span></p>
<p>First we prove <span class="math inline">\(L\in \mathscr B(E_1,E_2)\)</span>, we only need to prove <span class="math inline">\(\exists K,||L(x)||\le K||x||\)</span> <span class="math display">\[
||L(x)||=||\lim_{n\rightarrow \infty }L_n(x)||=\lim_{n\rightarrow \infty}||L_n(x)||
\]</span> Since <span class="math inline">\(E_2\)</span> is complete and Cauchy sequence <span class="math inline">\((L_n)\)</span> is bounded, that means <span class="math inline">\(||L_n||\le M\)</span> <span class="math display">\[
\lim_{n\rightarrow \infty}||L_n(x)||\le M||x||\\
||L(x)||\le M||x||
\]</span> Then we need to prove <span class="math inline">\(||L_n-L||\rightarrow 0\)</span> <span class="math display">\[
||L_m-L_n||\rightarrow 0\\
m\rightarrow \infty\\
||L_n-L||\rightarrow 0
\]</span> Q.E.D.</p>
<p>==Theorem 1.6.6==</p>
<p>Let L be a continuous linear mapping from a subspace of a normed space <span class="math inline">\(E_1\)</span> into a Banach space <span class="math inline">\(E_2\)</span>. Then L has a unique extension continuous linear mapping defined on the closure of the domain <span class="math inline">\(\mathscr D(L)\)</span>. In particular, if <span class="math inline">\(\mathscr D(L)\)</span> is dense in <span class="math inline">\(E_1\)</span>, then L has a unique extension to a continuous linear mapping defined on the whole space <span class="math inline">\(E_1\)</span>.</p>
<p>==Theorem 1.6.7==</p>
<p>If <span class="math inline">\(L:E_1\rightarrow E_2\)</span> is a continuous linear mapping, then the null space <span class="math inline">\(\mathscr N(L)\)</span> is a closed subspace of <span class="math inline">\(E\)</span>. Moreover, if the domain <span class="math inline">\(\mathscr D(L)\)</span> is closed, then the graph <span class="math inline">\(\mathscr G(L)\)</span> is a closed subspace of <span class="math inline">\(E_1\times E_2\)</span>.</p>
<p>==Theorem 1.6.9==</p>
<p>Let <span class="math inline">\(\mathscr T\)</span> be a family of bounded linear mapping from a Banach space <span class="math inline">\(X\)</span> into a normed space <span class="math inline">\(Y\)</span>. If for every <span class="math inline">\(x\in X\)</span> there exists a constant <span class="math inline">\(M_x\)</span> such that <span class="math inline">\(||T(x)||\le M_x\)</span> for all <span class="math inline">\(T\in \mathscr T\)</span>, then there exists a constant <span class="math inline">\(M&gt;0\)</span> such that <span class="math inline">\(||T||\le M\)</span> for all <span class="math inline">\(T\in \mathscr T\)</span>.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/21/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/21/">21</a><span class="page-number current">22</span><a class="page-number" href="/page/23/">23</a><span class="space">&hellip;</span><a class="page-number" href="/page/43/">43</a><a class="extend next" rel="next" href="/page/23/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cheng-Zilong</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cheng-Zilong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
