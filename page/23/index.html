<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="keywords" content="Optimization, Machine Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Cheng-Zilong">
<meta property="og:url" content="http://yoursite.com/page/23/index.html">
<meta property="og:site_name" content="Cheng-Zilong">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cheng-Zilong">
  <link rel="canonical" href="http://yoursite.com/page/23/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Cheng-Zilong</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cheng-Zilong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Learning Notes</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/17/EE5907 Pattern Recognition/6. Bayesian Statistics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/04/17/EE5907 Pattern Recognition/6. Bayesian Statistics/" class="post-title-link" itemprop="url">Chapter 6. Bayesian Statistics</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-04-17 13:46:13 / Modified: 14:52:27" itemprop="dateCreated datePublished" datetime="2019-04-17T13:46:13+08:00">2019-04-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5907-Pattern-Recognition/" itemprop="url" rel="index"><span itemprop="name">EE5907 Pattern Recognition</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="bayes-model-selection">1. Bayes Model Selection</h2>
<p>Bayesian Model Selection tradeoffs complexity with training error</p>
<p>In practice, cross-validation works better than Bayesian model selection, but this is an important conceptual idea.</p>
<p>Bayesian model selection <span class="math display">\[
\hat m = \underset{m}{\operatorname{argmax}} p(m|D)\\
= \underset{m}{\operatorname{argmax}} p(D|m)p(m)\\
\]</span></p>
<p>assume all model chosen prior are the same, then we have <span class="math inline">\(p(m)\propto 1\)</span></p>
<p>then <span class="math display">\[
\hat m = \underset{m}{\operatorname{argmax}} p(D|m)
\]</span> Recall we have MAP estimation <span class="math display">\[
\theta_{MAP}=\underset{\theta}{\operatorname{argmax}} p(\theta|D)\\
=\underset{\theta}{\operatorname{argmax}} \frac{p(D|\theta)|p(\theta)}{p(D)}\\
\]</span> The above is based on the model assumption <span class="math display">\[
\theta_{MAP}=\underset{\theta}{\operatorname{argmax}} p(\theta|D)\\
=\underset{\theta}{\operatorname{argmax}} \frac{p(D|\theta)|p(\theta)}{p(D)}\\
=\underset{\theta}{\operatorname{argmax}} \frac{p(D|\theta,m)|p(\theta|m)}{p(D|m)}\\
\]</span> We ignore the denominator because only consider one model. From the above equation, we can see that the evidence is actually the model likelihood <span class="math inline">\(p(D|m)\)</span>.</p>
<p>But why it works?</p>
<p>Complex models can explain many things <span class="math inline">\(p(D&#39;|m)\)</span> is non-zero for many different and complex <span class="math inline">\(D&#39;\)</span></p>
<p>However, <span class="math inline">\(\sum_{D&#39;}p(D&#39;|m)=1\)</span> and many <span class="math inline">\(p(D&#39;|m)\)</span> are non-zero, which means <span class="math inline">\(p(D&#39;|m)\)</span> cannot be very big either.</p>
<p>Example, given dots coming from quadratic curve</p>
<ol type="1">
<li>M1 is linear curves, it cannot fit dots well, so <span class="math inline">\(p(dots|M_1)\)</span> is very small.</li>
<li>M3 is linear, quadratic and cubic curves. It can fit dots well but waste none-zero probability on cubic curves. We can think that cubic curves can also fit many other dots very well so <span class="math inline">\(p(dots|M_3)\)</span> is also big for many other data.</li>
<li>M2 is linear and quadratic curves. It can fit dots well and no waste. Therefore, <span class="math inline">\(p(dots|M_2)\)</span> is the highest.</li>
</ol>
<h2 id="computing-marginal-likelihood-evidence">2. Computing Marginal Likelihood (Evidence)</h2>
<p>Then we try to computing the marginal likelihood <span class="math display">\[
P(D)=\frac{p(D|\theta)p(\theta)}{p(\theta|D)}
\]</span> Example <span class="math display">\[
P(D|\theta)=Bin(N_0,N_1|\theta)\\
P(\theta)=Beta(a,b)\\
P(\theta|D)=Beta(a+N_1,b+N_0)\\
\]</span> then <span class="math display">\[
P(D)=\frac{p(D|\theta)p(\theta)}{p(\theta|D)}=\left(
\begin{array}{lll}
N\\N_1
\end{array}\right)
\frac{B(a+N_1,b+N_0)}{B(a,b)}
\]</span> which is very difficult to compute</p>
<p>we use Bayesian Information Criteria (BIC) to approximate <span class="math display">\[
\log p(D)\approx \log p(D|\theta_{ML})-(dof(\theta)/2)\log N
\]</span> So we see that a more complicated model will generally result in a better log likelihood. On the other hand, a more complex model has higher degree of freedoms and therefore gets penalized in the second term.</p>
<p>The log N might seem weird. So the more the data, the more penalty there is. If there is more data shouldn’t we be able to fit more complex models?</p>
<p>Well actually there is no contradiction because the first term grows linearly with N, so as we accumulate more data the first term should grow linearly with N, while the second term grows logarithmically with N.</p>
<p>Some literature suggests that using MAP estimate works better.</p>
<h2 id="bayesian-decision-theory">3. Bayesian Decision Theory</h2>
<p>We have estimated the posterior probability of some parameters, but in real world, we need to translate this posterior probability into action. When we choose an action a, we can define the loose function <span class="math inline">\(L(y,a)\)</span> . For example, <span class="math inline">\(L(y,a)=(y-a)^2\)</span></p>
<p>Then we can pick an action by minimizing posterior expected loss <span class="math display">\[
\delta(x)=\arg\min_{a\in A} E(L(y,a))=\arg\min_{a\in A} \sum_y L(y,a)p(y|x) 
\]</span> MAP Estimate Minimizes 0-1 Loss <span class="math display">\[
\rho(a|x)=\sum_y L(y,a)p(y|x)=p(y\neq a|x)=1-p(y=a|x)
\]</span> minimizing <span class="math inline">\(1-p(y=a|x)\)</span> equivalent to maximize <span class="math inline">\(p(y=a|x)\)</span></p>
<p>Posterior Mean Minimizes <span class="math inline">\(l_2\)</span> Loss <span class="math display">\[
\rho(a|x)=\sum_y L(y,a)p(y|x)=E [(y-a)^2|x]=E(y^2|x)-2aE(y|x)+a^2\\
\frac{\partial }{\partial a}\rho(a|x)=0\\
a=E(y|x)
\]</span> Posterior Mean Minimizes <span class="math inline">\(l_1\)</span> Loss</p>
<p><span class="math display">\[
\begin{array}{rcl}
\rho(a|x)&amp;=&amp;\sum_y (|y-a|)p(y|x)\\
&amp;=&amp; \sum_{y\ge a}(y-a)p(y|x)+ \sum_{y&lt; a}(a-y)p(y|x)\\
\frac{\partial \rho}{\partial a}&amp;=&amp; \sum_{y\ge a}-p(y|x)+ \sum_{y&lt; a}p(y|x)=0\\
&amp;\implies&amp; p(y&lt;a|x)=p(y\ge a|x)=0.5
\end{array}
\]</span> Therefore, a is the median.</p>
<p>False Positive (FP) - False Negative (FN) Tradeoff <span class="math display">\[
L(\hat y=0|x)=L_{FN}p(y=1|x)\\
L(\hat y=1|x)=L_{FP}p(y=0|x)\\
\]</span> Then we choose <span class="math inline">\(\hat y=1\)</span> when <span class="math display">\[
L(\hat y=0|x)&gt;L(\hat y=1|x)\\
L_{FN}p(y=1|x)&gt;L_{FP}p(y=0|x)\\
\frac{p(y=1|x)}{p(y=0|x)}&gt;\frac{L_{FP}}{L_{FN}}
\]</span></p>
<p>From the above, we define a threshold <span class="math inline">\(f(x)=\frac{p(y=1|x)}{p(y=0|x)}\)</span> at <span class="math inline">\(\tau=\frac{L_{FP}}{L_{FN}}\)</span> to make classification decision.</p>
<p>If we define true positive rate (TPR) as <span class="math inline">\(\frac{N_{TP}}{N_1}\)</span> where <span class="math inline">\(N_1=N_{FN}+N_{TP}\)</span> and false negative rate (FPR) as <span class="math inline">\(\frac{N_{FP}}{N_{0}}\)</span> where <span class="math inline">\(N_0=N_{TN}+N_{FP}\)</span></p>
<p>Then we can plot with different <span class="math inline">\(\tau\)</span> with x axis as TPR and y axis as <span class="math inline">\(FPR\)</span></p>
<p>The plot is called receiver operating characteristic (ROC)</p>
<h2 id="mixture-of-conjugate-priors">3. Mixture of Conjugate Priors</h2>
<p>Conjugate priors simplify computation but not flexible enough.</p>
<p>We define the conjugate priors as mixture <span class="math display">\[
p(\theta)=\sum_kp(z=k)p(\theta|z=k)
\]</span> Then we have the posterior <span class="math display">\[
p(\theta|D)=\sum_kp(z=k|D)p(\theta|z=k,D)\\
\]</span> where <span class="math display">\[
p(z=k|D)=\frac{p(z=k)p(D|z=k)}{\sum_{k&#39;}p(z=k&#39;)p(D|z=k&#39;)}
\]</span> Example</p>
<p>Suppose <span class="math inline">\(p(\theta)=0.5Beta(20,20)+0.5Beta(30,10)\)</span> Suppose we observe 20 heads and 10 tails, then the posterior <span class="math display">\[
p(D|z=1)=\left(
\begin{array}{lll}
N\\N_1
\end{array}\right)
\frac{B(a+N_1,b+N_0)}{B(a,b)}=\\
\left(
\begin{array}{lll}
30\\20
\end{array}\right)
\frac{B(40,30)}{B(20,20)}=0.0437\\
p(D|z=2)=\left(
\begin{array}{lll}
N\\N_1
\end{array}\right)
\frac{B(a+N_1,b+N_0)}{B(a,b)}=\\
\left(
\begin{array}{lll}
30\\20
\end{array}\right)
\frac{B(50,20)}{B(30,10)}=0.0826
\]</span> Then we have <span class="math display">\[
p(z=1|D)=\frac{0.0437}{0.0826+0.0437}=0.346\\
p(z=2|D)=\frac{0.0826}{0.0826+0.0437}=0.634\\
\]</span></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/11/EE5904 Neural Network/1.MLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/04/11/EE5904 Neural Network/1.MLP/" class="post-title-link" itemprop="url">Chapter 1. Multi-Layer Perceptron</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-04-11 13:45:47 / Modified: 14:40:28" itemprop="dateCreated datePublished" datetime="2019-04-11T13:45:47+08:00">2019-04-11</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5904-Neural-Network/" itemprop="url" rel="index"><span itemprop="name">EE5904 Neural Network</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="what-is-learning-in-nn">1. What is Learning in NN</h2>
<p>Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded.</p>
<p>Process of learning</p>
<ol type="1">
<li>The NN is stimulated by the environment</li>
<li>NN undergoes changes in its free parameters</li>
<li>NN responds in a new way to the environment</li>
</ol>
<p>Supervised Learning:</p>
<ol type="1">
<li>The NN is fed with input and produce an output</li>
<li>The teacher will tell what the desired output should be</li>
<li>The weights are adjusted by the error signals</li>
</ol>
<p>Unsupervised Learning:</p>
<ol type="1">
<li>NN is interacting with the environment by taking various actions</li>
<li>The learning system will be rewarded or penalized by its actions</li>
<li>The weights are adjusted by the reinforcement signal</li>
</ol>
<h2 id="perceptron">2. Perceptron</h2>
<p><span class="math display">\[
v = \sum_{i=1}^mw_ix_i+b\\
\]</span></p>
<p>For simplicity <span class="math display">\[
x(n)=[1,x_1(n),x_2(n),\dotsm,x_m(n)]^T\\
w(n)=[b(n),w_1(n),w_2(n),\dotsm,w_m(n)]^T\\
v(n)=w^T(n)x(n)
\]</span> where n denotes iteration step.</p>
<p>How to tune the parameters</p>
<p>First, consider the case, <span class="math display">\[
v=w^Tx&lt;0\quad \varphi(v)=0
\]</span> if the desired output is <span class="math inline">\(d=0\)</span>, then nothing needs to be done</p>
<p>if the desired output is <span class="math inline">\(d=1\)</span>, then we hope <span class="math inline">\(v&#39;\)</span> to be greater</p>
<p>then we have <span class="math display">\[
v&#39;-v = (w&#39;^T-w^T)x=\Delta w x&gt;0
\]</span> the most simple choice is <span class="math inline">\(\Delta w=x^T\)</span></p>
<p>then we have <span class="math display">\[
w&#39; = w+\eta x
\]</span> For the other case <span class="math display">\[
v=w^Tx&gt;0\quad \varphi(v)=1
\]</span> if the desired output is <span class="math inline">\(d=0\)</span>, then we hope <span class="math inline">\(v&#39;\)</span> to be smaller <span class="math display">\[
v&#39;-v = (w&#39;^T-w^T)x=\Delta w x&lt;0
\]</span> the most simple choice is <span class="math inline">\(\Delta w=-x^T\)</span> <span class="math display">\[
w&#39; = w-\eta x
\]</span> then we have <span class="math display">\[
w&#39;=w+\eta e x
\]</span></p>
<p>To summarize,</p>
<p>Perceptron Learning Algorithm</p>
<p>Start with a randomly chosen weight vector <span class="math inline">\(w(1)\)</span></p>
<p>while there exist input vectors that are misclassified by <span class="math inline">\(w(n)\)</span></p>
<p>Do Let <span class="math inline">\(x(n)\)</span> be a misclassified input vector</p>
<p>​ update the weight vector to</p>
<p>​ <span class="math inline">\(w(n+1)=w(n)+\eta e(n)x(n)\)</span></p>
<p>​ <span class="math inline">\(e(n)=d(n)-y(n)\)</span></p>
<p>​ where <span class="math inline">\(\eta&gt;0\)</span> and d=<span class="math inline">\(\begin{cases} 1\text{ if x belongs to class 1} \\ 0\text{ if x belongs to class 2}\end{cases}\)</span></p>
<p>​ n=n+1</p>
<p>end while</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">xc0 = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.5</span>], [<span class="number">1.0</span>, <span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.2</span>, <span class="number">1.1</span>]])</span><br><span class="line">xc1 = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2.5</span>, <span class="number">2.6</span>, <span class="number">2.4</span>, <span class="number">2.6</span>, <span class="number">2.5</span>], [<span class="number">3.0</span>, <span class="number">3.1</span>, <span class="number">3.2</span>, <span class="number">3.2</span>, <span class="number">3.0</span>]])</span><br><span class="line">w = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">fig1 = plt.plot(xc0[<span class="number">1</span>, :], xc0[<span class="number">2</span>, :], <span class="string">'bo'</span>, xc1[<span class="number">1</span>, :], xc1[<span class="number">2</span>, :], <span class="string">'ro'</span>)</span><br><span class="line">yplot = -w[<span class="number">1</span>]/w[<span class="number">2</span>]*np.array([<span class="number">0</span>, <span class="number">3</span>]) - w[<span class="number">0</span>]/w[<span class="number">2</span>]</span><br><span class="line">plt.plot(np.array([<span class="number">0</span>, <span class="number">3</span>]), yplot, <span class="string">'r'</span>)</span><br><span class="line">eta = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># This is for c0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">4</span>):</span><br><span class="line">        v = w.transpose().dot(xc0[:, j])</span><br><span class="line">        <span class="keyword">if</span> v &gt; <span class="number">0</span>:</span><br><span class="line">            y = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = <span class="number">0</span></span><br><span class="line">        e = <span class="number">0</span> - y</span><br><span class="line">        w = w + eta*e*xc0[:, j]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is for c1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">4</span>):</span><br><span class="line">        v = w.transpose().dot(xc1[:, j])</span><br><span class="line">        <span class="keyword">if</span> v &gt; <span class="number">0</span>:</span><br><span class="line">            y = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = <span class="number">0</span></span><br><span class="line">        e = <span class="number">1</span> - y</span><br><span class="line">        w = w + eta*e*xc1[:, j]</span><br><span class="line">    yplot = -w[<span class="number">1</span>] / w[<span class="number">2</span>] * np.array([<span class="number">0</span>, <span class="number">3</span>]) - w[<span class="number">0</span>] / w[<span class="number">2</span>]</span><br><span class="line">    plt.plot(np.array([<span class="number">0</span>, <span class="number">3</span>]), yplot, color=(<span class="number">0.5</span>, <span class="number">0.5</span>, i/<span class="number">10</span>))</span><br><span class="line">    plt.pause(<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Proof of Convergence</p>
<p>There exists <span class="math inline">\(w_0\)</span> such that</p>
<p>For samples in class <span class="math inline">\(C_1\)</span> <span class="math inline">\(w_0^Tx(k)&gt;0\)</span></p>
<p>For samples in class <span class="math inline">\(C_2\)</span> <span class="math inline">\(w_0^Tx(k)&lt;0\)</span> <span class="math display">\[
w(n+1)=w(n)+e(n)x(n)=e(1)x(1)+e(2)x(2)+\dotsm+e(n)x(n)\\
w_0^Tw(n+1)=e(1)w_0^Tx(1)+e(2)w_0^Tx(2)+\dotsm+e(n)w_0^Tx(n)\\
e(k)w_0^Tx(k)=|w_0^Tx(k)|&gt;0\\
\alpha=\min\{|w_0^Tx(i)|\}
\]</span> Then we have <span class="math display">\[
w_0^Tw(n+1)\ge n\alpha\\
||w_0||\cdot||w(n+1)||\ge||w_0^Tw(n+1)||\ge n\alpha\\
\]</span> we have <span class="math display">\[
||w(n+1)||\ge\frac{n\alpha}{||w_0||}
\]</span> Let <span class="math inline">\(\beta = \max \{||x(i)||^2\}\)</span></p>
<p>And we have <span class="math display">\[
||w(n+1)||^2\le n \beta
\]</span> Overall, we have <span class="math display">\[
\frac{n^2\alpha^2}{||w_o||^2}\le ||w(n+1)||^2\le n\beta
\]</span> Therefore, n must be finite. Otherwise, the inequality will not be satisfied.</p>
<h2 id="linear-regression-problem">3. Linear Regression Problem</h2>
<p><span class="math display">\[
E(w)=\frac{1}{2}e(w)^2\\
e(w)=d-x^Tw\\
g(w)=\frac{\partial E}{\partial w}=\frac{\partial E}{\partial e}\frac{\partial e}{\partial w}=-e(w)x^T\\
\]</span></p>
<p>Then we have <span class="math display">\[
w(n+1)=w(n)+\eta e(n)x(n)
\]</span></p>
<p>Example</p>
<p>Training samples {(1, 3.5), (1, 3.5), (1.5, 2.5), (3, 2), (3.5, 1), (4, 1)}</p>
<p>Initial weight <span class="math inline">\(w(1)=[2, 0]^T\)</span></p>
<p>Learning rate <span class="math inline">\(\eta\)</span> is 0.1.</p>
<p>==Iteration 1==</p>
<p>Forward Propagation <span class="math display">\[
e(1)=d^{(1)}-w(1)^Tx^{(1)}=3.5-2=1.5
\]</span> Backward Propagation <span class="math display">\[
w(2)=w(1)+\eta x^{(1)}e(1)=[2.15\quad 0.15]^T
\]</span> ==Iteration 2==</p>
<p>Forward Propagation <span class="math display">\[
e(2)=d^{(2)}-w(2)^Tx^{(2)}=0.125
\]</span> Backward Propagation <span class="math display">\[
w(3)=w(2)+\eta x^{(2)}e(2)=[2.1625\quad 0.1688]^T
\]</span></p>
<h2 id="multilayer-perceptrons">4. Multilayer Perceptrons</h2>
<p>MLP generally adopts a smooth nonlinear activation function, such as the following logistic function</p>
<p>Sigmoid Function <span class="math display">\[
\varphi(v)=\frac{1}{1+e^{-av}}
\]</span></p>
<p>MLP is fed with an input vector <span class="math inline">\(x(n)\)</span>, and produces an output vector <span class="math inline">\(y(n)\)</span>, where <span class="math inline">\(n\)</span> is the iteration number.</p>
<p>If we have a three layers NN, for the third layer</p>
Let <span class="math inline">\(d(n)\)</span> denote the desired network output, and then the error is <span class="math display">\[
e(n)=d(n)-y(n)=d(n)-x^{(3)}_{out}(n)
\]</span> Then the cost function is <span class="math display">\[
E(n)=\frac{1}{2}\sum_{j=1}^{n_3}(d_j(n)-x_{out,j}^{(3)}(n))^2
\]</span> where <span class="math inline">\(n_3\)</span> means the number of neurons in the third layer <span class="math display">\[
x^{(3)}_{out,j}=\varphi(v^{(3)}_{j})\\
v^{(3)}_{j} = \sum_i w_{ji}^{(3)}x^{(2)}_i
\]</span> Then we have $$
<span class="math display">\[\begin{array}{rcl}
\Delta w^{(3)}_{ji}(n)&amp;=&amp;-\eta\frac{\partial E(n)}{\partial w_{ji}^{(3)}(n)}\\
&amp;=&amp;\eta(d_j(n)-x^{(3)}_{out,j})\frac{\partial x^{(3)}_{out,j}}{\partial w_{ji}^{(3)}(n)}\\
&amp;=&amp;\eta(d_j(n)-x^{(3)}_{out,j})\frac{\partial x^{(3)}_{out,j}}{\partial v_j^{(3)}}
\frac{\partial v_j^{(3)}}{\partial w_{ji}^{(3)}(n)}\\

&amp;=&amp; \eta \delta_j^{(3)}(n)x_{out,i}^{(2)}(n)
\end{array}\]</span>
<p>$$</p>
<p>where <span class="math display">\[
\delta_j^{(3)}(n)=e_j(n)\dot \varphi^{(3)}(v_j^{(3)}(n))
\]</span> For the second layer,</p>
<p>Let d(n) denote the desired network output, and the error is then <span class="math display">\[
e(n)=d(n)-y(n)=d(n)-x^{(3)}_{out}(n)
\]</span> Then the cost function is <span class="math display">\[
E(n)=\frac{1}{2}\sum_{k=1}^{n_3}(d_k(n)-x_{out,k}^{(3)}(n))^2
\]</span> where <span class="math inline">\(n_3\)</span> means the number of neurons in the third layer <span class="math display">\[
x^{(3)}_{out,k}=\varphi(v^{(3)}_{k})\\
v^{(3)}_{k} = \sum_j w_{kj}^{(3)}x^{(2)}_j\\
x^{(2)}_{out,j}=\varphi(v^{(2)}_{j})\\
v^{(2)}_{j} = \sum_i w_{ji}^{(2)}x^{(1)}_i
\]</span> Then we have <span class="math display">\[
\Delta w^{(2)}_{ji}(n)=-\eta\frac{\partial E(n)}{\partial w_{ji}^{(2)}(n)}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})\frac{\partial x^{(3)}_{out,k}}{\partial x^{(2)}_{out,j}}
\frac{\partial x^{(2)}_{out,j}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})\frac{\partial x^{(3)}_{out,k}}{\partial v^{(3)}_k}
\frac{\partial v^{(3)}_k}{\partial x^{(2)}_{out,j}}
\frac{\partial x^{(2)}_{out,j}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})
\dot \varphi^{(3)}(v_k^{(3)})
w_{kj}^{(3)}
\frac{\partial x^{(2)}_{out,j}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})
\dot \varphi^{(3)}(v_k^{(3)}(n))
w_{kj}^{(3)}
\frac{\partial x^{(2)}_{out,j}}{\partial v_{j}^{(2)}}
\frac{\partial v_{j}^{(2)}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})
\dot \varphi^{(3)}(v_k^{(3)}(n))
w_{kj}^{(3)}
\dot \varphi^{(2)}(v_j^{(2)}(n))
x_{out,i}^{(1)}\\
=\eta\sum_{k=1}^{n_3}\delta_k^{(3)}
w_{kj}^{(3)}
\dot \varphi^{(2)}(v_j^{(2)}(n))
x_{out,i}^{(1)}\\
= \eta \delta_j^{(2)}(n)x_{out,i}^{(1)}(n)
\]</span> where <span class="math display">\[
\delta_j^{(2)}(n)=
\left(\sum_{k=1}^{n_3}\delta_k^{(3)}
w_{kj}^{(3)}\right)
\dot \varphi^{(2)}(v_j^{(2)}(n))\\
\]</span> Then we can summary <span class="math display">\[
w_{ji}^{(s)}(n+1)=w_{ji}^{(s)}(n)+\eta \delta_j^{(s)}(n)x_{out,i}^{(s-1)}(n)
\]</span> where <span class="math display">\[
\delta_j^{(s)}(n)=e_j(n)\dot \varphi^{(s)}\left(v_j^{(s)}(n)\right)\quad\text{Output Layer}\\
\delta_j^{(s)}(n)=
\left(\sum_{k=1}^{n_{s+1}}\delta_k^{(s+1)}(n)
w_{kj}^{(s+1)}(n)\right)
\dot \varphi^{(s)}\left(v_j^{(s)}(n)\right) \quad \text{Hidden Layer}\\
\]</span> To increase learning speed, modify the delta rule <span class="math display">\[
\Delta w_{ji}^{(s)}(k)=\alpha \Delta w_{ji}^{(s)}(k-1) + \eta \delta_j^{(s)}(k)x_{out,i}^{(s-1)}(k)\\
w_{ji}^{(s)}(n+1)=w_{ji}^{(s)}(n)+\Delta w_{ji}^{(s)}(k)
\]</span></p>
<h2 id="mlp-design-and-training-issues">5. MLP Design and Training Issues</h2>
<p>Many design and training issues to consider</p>
<h3 id="sequential-mode-or-batch-mode">1. Sequential mode or batch mode</h3>
<p>For sequential mode, weight updating is performed after each presentation of the training example. N times adjustments in one epoch.</p>
<p>For batch mode, update weight after the presentation of all the training examples. One time adjustment in one epoch.</p>
<p>Sequential learning is generally the preferred one since sequential learning often results in better solutions. Nonlinear networks often have multiple local minima. Batch learning can easily get trapped into the local minimum.</p>
<p>But sequential mode always causes fluctuation because of the noise.</p>
<p>One way to reduce fluctuation is to use mini-batches.</p>
<p>That is to start with a small batch size and increase the size as the training proceeds.</p>
<p>Second Order Method <span class="math display">\[
E(w(n+1))=E(w(n)+\Delta w(n))=E(w(n))+g^T(n)\Delta w(n)+\frac{1}{2}\Delta w^T(n)H(n)\Delta w(n)
\]</span> If we hope to get choose <span class="math inline">\(\Delta w(n)\)</span> to let <span class="math inline">\(E(w(n+1))\)</span> be minimized <span class="math display">\[
\frac{\partial E(w(n+1))}{\partial \Delta w(n)}=g^T(n)+\Delta w^T(n)H(n)=0\\
\Delta w(n) = -H^{-1}(n)g(n)
\]</span> This is so called Newton's method.</p>
<h3 id="normalize-the-input-variables">2. Normalize the input variables</h3>
<p>In general, any shift of the average input away from zero will bias the updates in a partialicular direction and this slow down learning.</p>
<p>Normalize <span class="math display">\[
\bar x_i = \frac{\sum_{n=1}^Nx_i(n)}{N}\quad \sigma=\sqrt \frac{\sum_{n=1}^N (x_i(n)-\bar x_i)}{N}\quad x_i&#39;(n) = \frac{x_i(n)-\bar x_i}{\sigma}
\]</span> Let the range are close to [-1, 1]</p>
<h3 id="how-to-choose-activation-functions-in-hidden-layers">3. How to choose activation functions in hidden layers</h3>
<p>For the hidden layers, <span class="math display">\[
\tan sig(x)=\frac{2}{1+e^{-2x}}-1
\]</span> is better than <span class="math display">\[
\log sig(x)=\frac{1}{1+e^{-x}}
\]</span> Every neuron output is closed to [-1, 1]</p>
<p>However, for the output layer, if the problem is pattern recognition problem <span class="math display">\[
\log sig(x)=\frac{1}{1+e^{-x}}
\]</span> is better because the output is 0 or 1</p>
<p>For regression problem, using linear neuron at the output layer would be more flexible.</p>
<h3 id="how-to-choose-target-values-for-pattern-recognition">4. How to choose target values for pattern recognition</h3>
<p>For the sigmoid function, the training process will try to derive the output as close as possible to the target value, that can cause the input to the output layer to be bigger and bigger in the training process. Therefore, it is better to choose [-0.6, 0.6] for tansig and [0.2, 0.8] for logsig.</p>
<h3 id="how-many-hidden-layers-and-how-many-hidden-neurons-in-each-layer">5. How many hidden layers AND how many hidden neurons in each layer</h3>
<p>Adding more layers can reduce the parameters but may make the MLP more prone to local minima traps because of its more complicated structure.</p>
<h3 id="how-to-avoid-over-fitting">6. How to avoid over-fitting</h3>
<p>One way is to limit the number of hidden units. It is important to seek the minimal structure of the MLP</p>
<p>For valid generation, the size of the weights is more important than the size of the network</p>
<p>Use regularization techniques</p>
<h3 id="use-singular-value-decomposition-svd-to-decide-the-minimal-structure">7. Use Singular Value Decomposition (SVD) to decide the minimal structure</h3>
<p>Calculate the outputs of the hidden neurons for each training sample input vector <span class="math inline">\(x(k)\)</span> <span class="math display">\[
h_{ki} = f_i(\sum_{j=1}^M w_{ij}x_j(k)+b_i)\in H
\]</span> Use SVD to get the effective rank of H. The threshold is always chosen as 0.95 or 0.99.</p>
<p>Train the MLP on the N training samples (<span class="math inline">\(X_{N\times M}\)</span>) with sufficiently large number of <span class="math inline">\(n\)</span> hidden neurons until converge.</p>
<p>Calculate the effective rank.</p>
<p>Train the MLP with number of effective rank hidden neurons until converge.</p>
<p>Repeat the procedure until convergence.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/22/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><span class="page-number current">23</span><a class="page-number" href="/page/24/">24</a><span class="space">&hellip;</span><a class="page-number" href="/page/35/">35</a><a class="extend next" rel="next" href="/page/24/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cheng-Zilong</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cheng-Zilong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
