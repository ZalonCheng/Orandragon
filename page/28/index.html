<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="keywords" content="Optimization, Machine Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Cheng-Zilong">
<meta property="og:url" content="http://yoursite.com/page/28/index.html">
<meta property="og:site_name" content="Cheng-Zilong">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cheng-Zilong">
  <link rel="canonical" href="http://yoursite.com/page/28/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Cheng-Zilong</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cheng-Zilong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Learning Notes</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/06/EE5907 Pattern Recognition/2. Probabilistic Estimation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/03/06/EE5907 Pattern Recognition/2. Probabilistic Estimation/" class="post-title-link" itemprop="url">Chapter 2. Probabilistic Estimation</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-03-06 13:46:13 / Modified: 14:49:50" itemprop="dateCreated datePublished" datetime="2019-03-06T13:46:13+08:00">2019-03-06</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5907-Pattern-Recognition/" itemprop="url" rel="index"><span itemprop="name">EE5907 Pattern Recognition</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="map-estimation">1. MAP Estimation</h2>
<p>The key goal in machine learning is to do estimation. That means we give the estimated result when we are given observation. If we have no model of the estimation in the prior, we can give the estimated result only based on the result. So we choose maximum likelihood estimation. But if we have the prior knowledge, we can give the estimated result based on both the data and the prior knowledge, that is the maximum a posterior estimation.</p>
<p>Maximum a posterior estimation <span class="math display">\[
\begin{array}{rcl}
y_{MAP}&amp;=&amp;\underset{y}{\operatorname{argmax}}p(y|x)\\
&amp;=&amp;\underset{y}{\operatorname{argmax}}\frac{p(y)p(x|y)}{p(x)}\\
&amp;=&amp;\underset{y}{\operatorname{argmax}}p(y)p(x|y)\\
\end{array}
\]</span> In this case, we hope to find a maximum value of the probability when the data is given and we have the prior knowledge of the estimated data <span class="math inline">\(p(y)\)</span>.</p>
<h2 id="ml-estimation">2. ML Estimation</h2>
<p>Maximum likelihood estimation</p>
<p>When <span class="math display">\[
p(y)=1
\]</span> we have <span class="math display">\[
y_{MAP}=\underset{y}{\operatorname{argmax}}p(x|y)=y_{ML}
\]</span> In this case, we hope to get the max likelihood of the observation x given y. In general, ML estimation is computationally easier than MAP estimation, because we do not need to deal with the extra prior term. We often use ML estimation if the prior is unknown or if we do not feel comfortable assuming priors.</p>
<p>If sample goes to infinity, then <span class="math display">\[
\lim_{N\rightarrow\infty}y_{MAP}=y_{ML}
\]</span></p>
<h2 id="modeling-pyx">3. Modeling p(y|x)</h2>
<p><span class="math display">\[
p(y|x)=\frac{p(x|y)p(y)}{p(x)}
\]</span></p>
<p>Generative supervised learning: Modeling likelihood p(x|y) and prior p(y) directly, "Generative" because we can generate new data from p(x,y)=p(x|y)p(y)</p>
<p>Discriminative supervised learning: model p(y|x) directly</p>
<h2 id="probabilistic-estimation-of-model-parameters">4. Probabilistic Estimation of Model Parameters</h2>
<p>In general, parameter <span class="math inline">\(\theta\)</span> needs to be learnt from the training set for both generative models <span class="math inline">\(p(x,y|\theta)\)</span> and discriminative models <span class="math inline">\(p(y|x,\theta)\)</span>. Here we can use different strategies to learn the parameters from the data set.</p>
<p>Strategy 1: ML</p>
<p>First estimate <span class="math inline">\(\theta_{ML}=\underset{\theta}{\operatorname{argmax}}p(D|\theta)\\\)</span></p>
<p>Then plug in <span class="math inline">\(\theta_{ML}\)</span> into <span class="math inline">\(p(x,y|\theta_{ML})\)</span> and find ==MAP== estimate of y</p>
<p>Strategy 2: MAP</p>
<p>First estimate <span class="math inline">\(\theta_{MAP}=\underset{\theta}{\operatorname{argmax}}p(\theta|D)\\\)</span></p>
<p>Then plug in <span class="math inline">\(\theta_{MAP}\)</span> into <span class="math inline">\(p(x,y|\theta_{MAP})\)</span> and find ==MAP== estimate of y</p>
<p>Strategy 3: Posterior Predictive</p>
<p>Discuss in the future</p>
<h2 id="beta-binomial-generative-model">5. Beta-Binomial Generative Model</h2>
<p>Beta Distribution (Discussed in the last passage) <span class="math display">\[
p(\theta|a,b)=\frac{1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-1}
\]</span> Consider N coin tosses: data <span class="math inline">\(D=(N_0,N_1)\)</span>, where <span class="math inline">\(N_0=\#tails\)</span> and <span class="math inline">\(N_1=\#heads\)</span> , <span class="math inline">\(N=N_0+N_1\)</span> . <span class="math inline">\(\theta\)</span> is the probability of head</p>
<p>Assume beta distribution prior on <span class="math inline">\(\theta\)</span>: <span class="math inline">\(p(\theta|a,b)=Beta(\theta|a,b)\)</span></p>
<p>Coin tosses problem <span class="math display">\[
P(D|\theta)=\left(
\begin{array}{c}
N\\
N_0
\end{array}
\right)\theta^{N_1}(1-\theta)^{N_0}
\]</span></p>
<p>Posterior <span class="math display">\[
\begin{array}{lll}
p(\theta|D)&amp;=&amp;\frac{p(D|\theta)p(\theta)}{p(D)}\\
&amp;=&amp; \frac{\left(
\begin{array}{c}
N\\
N_0
\end{array}
\right)\theta^{N_1}(1-\theta)^{N_0} \quad \frac{1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-1}}{p(D)}\\
&amp;=&amp; \frac{\frac{B(N_1+a,N_0+b)}{B(a,b)}\left(
\begin{array}{c}
N\\
N_0
\end{array}
\right)
\frac{1}{B(N_1+a,N_0+b)}
\theta^{N_1+\alpha-1}(1-\theta)^{N_0+b-1} \quad }{p(D)}\\
&amp;=&amp; \frac{k(N_0,N_1,a,b)Beta(\theta|N_1+a, N_0+b)}{p(D)}\\
&amp;=&amp; Beta(\theta|N_1+a, N_0+b)
\end{array}
\]</span> Here we can see that the posterior is the same form as the prior, so we say that the beta is a conjugate prior of the binomial distribution.</p>
<p>Since ML is a special case of MAP, first we consider MAP here, and we know that the mode of Beta distribution is <span class="math inline">\(mode=\frac{c-1}{c+d-2}\)</span>, we have <span class="math display">\[
\begin{array}{rcl}
\theta_{MAP}&amp;=&amp;\underset{\theta}{\operatorname{argmax}}p(\theta|D)\\
&amp;=&amp;\frac{N_1+a-1}{N+a+b-2}
\end{array}
\]</span> when a=b=1 <span class="math display">\[
\theta_{ML}=\frac{N_1}{N}
\]</span> Predicting Future Coin Tosses</p>
<p>Strategy 1 ML <span class="math inline">\(p(\tilde x=1)=\theta_{ML}\)</span></p>
<p>Strategy 2 MAP <span class="math inline">\(p(\tilde x=1)=\theta_{MAP}\)</span></p>
<p>Strategy 3 Posterior Predictive Distribution</p>
<p>Actually our problem is <span class="math display">\[
p(\tilde x=1|D)
=\int_0^1 p(\tilde x=1,\theta|D)d\theta
=\int_0^1 p(\tilde x=1|\theta,D)p(\theta|D)d\theta 
= \int_0^1 \theta p(\theta|D)d\theta\\
= \int_0^1 \theta Beta(\theta|N_1+a, N_0+b)d\theta
\]</span> This is the mean of posterior distribution <span class="math display">\[
p(\tilde x=1|D)=\frac{N_0+b}{N+a+b}
\]</span></p>
<h2 id="dirichlet-multinomial-generative-model">6. Dirichlet-Multinomial Generative Model</h2>
<p>Multinomial Distribution <span class="math display">\[
\begin{array}{ccc}
P(X_1=x_1,X_2=x_2,X_3=x_3\dotsm)=\frac{n!}{x_1!x_2!\dotsm x_k!}p_1^{x_1}p_2^{x_2}\dotsm p_k^{x_k}\quad when \sum_{i=1}^kx_i=n\\
otherwise\quad P(X_1=x_1,X_2=x_2,X_3=x_3\dotsm)=0
\end{array}
\]</span> Dirichlet Distribution <span class="math display">\[
P(x|\alpha)=\frac{1}{B(\alpha)}\prod_{i=1}^Kx_i^{\alpha_i-1}
\]</span> Consider N rolls of K-sided dice <span class="math display">\[
p(D|\theta)=\frac{N!}{N_1!N_2!\dotsm N_k!}\theta_1^{N_1}\theta_2^{N_2}\dotsm \theta_K^{N_K}\quad
\propto\prod_{i=1}^K\theta_i^{N_i}\\
p(\theta|\alpha)=\frac{1}{B(\alpha)}\prod_{i=1}^Kx_i^{\alpha_i-1}
\]</span> Then <span class="math display">\[
p(\theta|D)\propto p(D|\theta)p(\theta)=Dir(\theta|N_1+\alpha_1,\dotsm,N_K+\alpha_K)
\]</span> Then MAP estimation of parameters <span class="math display">\[
\hat \theta_k^{MAP}=\frac{N_k+\alpha_k-1}{N+\sum_k\alpha_k-K}
\]</span> ML <span class="math display">\[
\hat \theta_k^{ML}=\frac{N_k}{N}
\]</span></p>
<p>Predicting Future Dice Rolls</p>
<p>Strategy 1 ML <span class="math inline">\(p(\tilde x=i)=\theta_{ML}^i\)</span></p>
<p>Strategy 2 MAP <span class="math inline">\(p(\tilde x=i)=\theta_{MAP}^i\)</span></p>
<p>Strategy 3 Posterior Predictive Distribution <span class="math inline">\(p(\tilde x=i|D)=\frac{N_i+\alpha_i}{N+\sum_k\alpha_k}\)</span></p>
<h2 id="univariate-gaussian">7. Univariate Gaussian</h2>
<p>Binomial and Multinomial are discrete random variables, here we look at continuous variables</p>
<p>Gaussian Distribution <span class="math display">\[
P(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</span> Given training data D, we hope to estimate parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></p>
<p>ML: <span class="math display">\[
(\hat \mu,\hat\sigma^2)=\underset{\hat \mu,\hat\sigma^2}{\operatorname{argmax}}p(D|\hat \mu,\hat\sigma^2)
\]</span> We have <span class="math display">\[
\hat \mu=\frac{1}{N}\sum_{n=1}^Nx_n\\
\hat \sigma^2=\frac{1}{N}\sum_{n=1}^N(x_n-\hat\mu)^2
\]</span> MAP:</p>
<p>Using Normal Inverse Gamma Distribution as prior <span class="math display">\[
p(\mu,\sigma^2)=NormInvGam(\mu,\sigma^2|\alpha,\beta,\gamma,\delta)
\]</span></p>
<p><span class="math display">\[
\hat \mu=\frac{\sum_{n=1}^Nx_n+\gamma\delta}{N+\gamma}\\
\hat \sigma^2=\frac{\sum_{n=1}^N(x_n-\hat \mu)^2+2\beta+\gamma(\delta-\hat \mu)^2}{N+3+2\alpha}
\]</span></p>
<p>Predicting Future Gaussian Samples</p>
<p>Strategy 1 ML <span class="math inline">\(p(x^*)=\mathcal N(\mu_{ML},\sigma^2_{ML})\)</span></p>
<p>Strategy 2 MAP <span class="math inline">\(p(x^*)=\mathcal N(\mu_{MAP},\sigma^2_{MAP})\)</span></p>
<p>Strategy 3 Posterior Predictive Distribution (skipped)</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/03/EE5907 Pattern Recognition/1. Probability/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/03/03/EE5907 Pattern Recognition/1. Probability/" class="post-title-link" itemprop="url">Chapter 1. Probability</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-03-03 13:46:13 / Modified: 14:44:47" itemprop="dateCreated datePublished" datetime="2019-03-03T13:46:13+08:00">2019-03-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5907-Pattern-Recognition/" itemprop="url" rel="index"><span itemprop="name">EE5907 Pattern Recognition</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="types-of-machine-learning">1. Types of Machine Learning</h2>
<p>Supervised Learning: Great if we know what we what to predict</p>
<p>Unsupervised learning: Great if we want to discover something new</p>
<h2 id="design-cycle">2. Design Cycle</h2>
<p>Data collection</p>
<p>Preprocessing: Image enhancement, remove background, segmentation</p>
<p>Divide data into training and test sets (Feature Extraction, Train classifier)</p>
<p>Re-visit previous steps if performance unsatisfactory</p>
<h2 id="probability-review">3. Probability Review</h2>
<p>A random variable x is a quantity that is uncertain.</p>
<p>If x is discrete, then the probability of x <span class="math inline">\(p(x)\)</span> is probability mass function (pmf)</p>
<p>if x is continuous, then the probability of x <span class="math inline">\(p(x)\)</span> is probability distribution function (pdf)</p>
<p><span class="math inline">\(p(x,y)\)</span> is joint probability distribution of x and y. we have <span class="math display">\[
p(y)=\int_x p(x,y)dx\quad or\quad p(y)=\sum_xp(x,y)
\]</span> This is called marginalization.</p>
<p>Suppose we observe y to be <span class="math inline">\(y_1\)</span> , then <span class="math inline">\(p(x|y=y_1)\)</span> is how likely x will take on various values given this observation, that read as conditional probability of x given y is equal to <span class="math inline">\(y_1\)</span> . We have <span class="math display">\[
p(x|y)=\frac{p(x,y)}{p(y)}=\frac{p(y|x)p(x)}{\int_xp(x,y)dx}=\frac{p(y|x)p(x)}{\int_xp(y|x)p(x)dx}
\]</span></p>
<h2 id="discrete-distributions">4. Discrete Distributions</h2>
<p>==Bernoulli Distribution== <span class="math display">\[
P(x)=\lambda^x(1-\lambda)^{1-x}
\]</span> where x=0, 1</p>
<p>Bernoulli Distribution describes situation where only two possible outcomes x=0, 1</p>
<p>==Categorical Distribution== <span class="math display">\[
P(x=k)=\lambda_k
\]</span> Categorical Distribution describes situation where K possible outcomes</p>
<p>==Binomial Distribution==</p>
<p>The binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own boolean-valued outcome: a random variable containing a single bit of information: success/yes/true/one (with probability p) or failure/no/false/zero (with probability q = 1 − p). <span class="math display">\[
P(x=k)=\left(
\begin{array}{c}
n\\
k
\end{array}
\right)p^{k}(1-p)^k
\]</span> ==Multinomial Distribution==</p>
<p>The multinomial distribution is a generalization of the binomial distribution. For example, it models the probability of counts for rolling a k-sided die n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories. <span class="math display">\[
\begin{array}{ccc}
P(X_1=x_1,X_2=x_2,X_3=x_3\dotsm)=\frac{n!}{x_1!x_2!\dotsm x_k!}p_1^{x_1}p_2^{x_2}\dotsm p_k^{x_k}\quad when \sum_{i=1}^kx_i=n\\
otherwise\quad P(X_1=x_1,X_2=x_2,X_3=x_3\dotsm)=0
\end{array}
\]</span> ==Poisson Distribution==</p>
<p>The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. <span class="math display">\[
P(x=k)=e^{-\lambda}\frac{\lambda^k}{k!}
\]</span></p>
<h2 id="continuous-distributions">5. Continuous Distributions</h2>
<p>==Gaussian Distribution (Normal Distribution)==</p>
<p>It states that averages of samples of observations of random variables independently drawn from independent distributions converge in distribution to the normal, that is, they become normally distributed when the number of observations is sufficiently large <span class="math display">\[
P(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</span> ==Exponential Distribution==</p>
<p>The exponential distribution is the probability distribution that describes the time between events in a Poisson point process. <span class="math display">\[
P(X=x)=\lambda e^{-\lambda x}
\]</span> ==Gamma Distribution==</p>
<p>The gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution. <span class="math display">\[
P(x|\alpha,\beta)=\frac{\beta^\alpha x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)}
\]</span> where <span class="math inline">\(\Gamma(\alpha)\)</span> is the Gamma Function.</p>
<p>==Beta Distribution==</p>
<p>The beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable and control the shape of the distribution. It is a special case of the Dirichlet distribution.</p>
<p>The beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines.</p>
<p>In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. For example, the beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions. <span class="math display">\[
P(x|\alpha,\beta)=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}
\]</span> where <span class="math inline">\(B(\alpha,\beta)\)</span> is Beta Function defined by <span class="math display">\[
B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
\]</span> ==Dirichlet Distribution==</p>
<p>the Dirichlet distribution often denoted as Dir(<span class="math inline">\(\alpha\)</span>) is a family of continuous multivariate probability distributions parameterized by a vector <span class="math inline">\(\alpha\)</span> of positive reals. It is a multivariate generalization of the beta distribution, hence its alternative name of Multivariate Beta distribution (MBD). <span class="math display">\[
P(x|\alpha)=\frac{1}{B(\alpha)}\prod_{i=1}^Kx_i^{\alpha_i-1}
\]</span></p>
<h2 id="bayes-rule">5. Bayes' Rule</h2>
<p>For continuous <span class="math display">\[
p(x,y)=p(x)p(y|x)=p(y)p(x|y) \implies \\
p(y|x)=\frac{p(y)p(x|y)}{p(x)}=\frac{p(y)p(x|y)}{\int p(x,y)dy}=\frac{p(y)p(x|y)}{\int p(y)p(x|y) dy}
\]</span> For discrete <span class="math display">\[
p(x,y)=p(x)p(y|x)=p(y)p(x|y) \implies \\
p(y|x)=\frac{p(y)p(x|y)}{p(x)}=\frac{p(y)p(x|y)}{\sum_y p(x,y)}=\frac{p(y)p(x|y)}{\sum_y p(y)p(x|y) }
\]</span> Denotion <span class="math display">\[
\underbrace{p(y|x)}_\text{Posterior}=\frac{\overbrace{p(y)}^\text{Prior}\quad\overbrace{p(x|y)}^\text{Liklihood}}{\underbrace{p(x)}_\text{Evidence}}
\]</span> Prior: What we know before seeing x</p>
<p>Posterior: What we know after seeing x</p>
<p>Likelihood: Propensity for observing a certain value of x given certain value of y</p>
<p>Evidence: Make sure the left hand is a valid distribution</p>
<h2 id="expectation">6. Expectation</h2>
<p>Definition of expectation <span class="math display">\[
E(f(x))=\sum_xf(x)p(x)\\
E(f(x))=\int_xf(x)p(x)dx\\
\]</span> Definition of mean <span class="math display">\[
E(x)=\int_xxp(x)dx=\mu_x
\]</span> Definition of variance <span class="math display">\[
E((x-\mu_x)^2)=\int_x(x-\mu_x)^2p(x)dx=\sigma_x^2
\]</span> then <span class="math inline">\(\sigma_x\)</span> is called standard deviation</p>
<p>Definition of covariance <span class="math display">\[
E((x-\mu_x)(y-\mu_y))=\int\int(x-\mu_x)(y-\mu_y)p(x,y)dxdy=Cov(x,y)
\]</span> Some important conclusions for expectation <span class="math display">\[
E(af(x)+bf(y))=aE(f(x))+bE(f(y))\\
Cov(x,y)=E(xy)-E(x)E(y)\\
Var(x)=Cov(x,x)=E(x^2)-E^2(x)
\]</span> Definition of Conditional Expectation <span class="math display">\[
E(f(x,y)|y)=E_{p(x|y)}[f(x,y)]=\sum_xf(x,y)p(x|y)
\]</span> Definition of Conditional Independece <span class="math display">\[
p(x_1,x_2|x_3)=p(x_1|x_3)p(x_2|x_3)
\]</span> Attention!</p>
<p><span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are independent does not imply <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are conditionally independent given <span class="math inline">\(x_3\)</span>.</p>
<p><span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are independent given <span class="math inline">\(x_3\)</span> does not imply <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are independent.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/27/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><span class="page-number current">28</span><a class="page-number" href="/page/29/">29</a><span class="space">&hellip;</span><a class="page-number" href="/page/38/">38</a><a class="extend next" rel="next" href="/page/29/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cheng-Zilong</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">75</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cheng-Zilong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
