<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="keywords" content="Optimization, Machine Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Cheng-Zilong">
<meta property="og:url" content="http://yoursite.com/page/31/index.html">
<meta property="og:site_name" content="Cheng-Zilong">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cheng-Zilong">
  <link rel="canonical" href="http://yoursite.com/page/31/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Cheng-Zilong</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cheng-Zilong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Learning Notes</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/11/Neural Network/1.MLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/04/11/Neural Network/1.MLP/" class="post-title-link" itemprop="url">Chapter 1. Multi-Layer Perceptron</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-04-11 13:45:47 / Modified: 14:40:28" itemprop="dateCreated datePublished" datetime="2019-04-11T13:45:47+08:00">2019-04-11</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Neural-Network/" itemprop="url" rel="index"><span itemprop="name">Neural Network</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="what-is-learning-in-nn">1. What is Learning in NN</h2>
<p>Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded.</p>
<p>Process of learning</p>
<ol type="1">
<li>The NN is stimulated by the environment</li>
<li>NN undergoes changes in its free parameters</li>
<li>NN responds in a new way to the environment</li>
</ol>
<p>Supervised Learning:</p>
<ol type="1">
<li>The NN is fed with input and produce an output</li>
<li>The teacher will tell what the desired output should be</li>
<li>The weights are adjusted by the error signals</li>
</ol>
<p>Unsupervised Learning:</p>
<ol type="1">
<li>NN is interacting with the environment by taking various actions</li>
<li>The learning system will be rewarded or penalized by its actions</li>
<li>The weights are adjusted by the reinforcement signal</li>
</ol>
<h2 id="perceptron">2. Perceptron</h2>
<p><span class="math display">\[
v = \sum_{i=1}^mw_ix_i+b\\
\]</span></p>
<p>For simplicity <span class="math display">\[
x(n)=[1,x_1(n),x_2(n),\dotsm,x_m(n)]^T\\
w(n)=[b(n),w_1(n),w_2(n),\dotsm,w_m(n)]^T\\
v(n)=w^T(n)x(n)
\]</span> where n denotes iteration step.</p>
<p>How to tune the parameters</p>
<p>First, consider the case, <span class="math display">\[
v=w^Tx&lt;0\quad \varphi(v)=0
\]</span> if the desired output is <span class="math inline">\(d=0\)</span>, then nothing needs to be done</p>
<p>if the desired output is <span class="math inline">\(d=1\)</span>, then we hope <span class="math inline">\(v&#39;\)</span> to be greater</p>
<p>then we have <span class="math display">\[
v&#39;-v = (w&#39;^T-w^T)x=\Delta w x&gt;0
\]</span> the most simple choice is <span class="math inline">\(\Delta w=x^T\)</span></p>
<p>then we have <span class="math display">\[
w&#39; = w+\eta x
\]</span> For the other case <span class="math display">\[
v=w^Tx&gt;0\quad \varphi(v)=1
\]</span> if the desired output is <span class="math inline">\(d=0\)</span>, then we hope <span class="math inline">\(v&#39;\)</span> to be smaller <span class="math display">\[
v&#39;-v = (w&#39;^T-w^T)x=\Delta w x&lt;0
\]</span> the most simple choice is <span class="math inline">\(\Delta w=-x^T\)</span> <span class="math display">\[
w&#39; = w-\eta x
\]</span> then we have <span class="math display">\[
w&#39;=w+\eta e x
\]</span></p>
<p>To summarize,</p>
<p>Perceptron Learning Algorithm</p>
<p>Start with a randomly chosen weight vector <span class="math inline">\(w(1)\)</span></p>
<p>while there exist input vectors that are misclassified by <span class="math inline">\(w(n)\)</span></p>
<p>Do Let <span class="math inline">\(x(n)\)</span> be a misclassified input vector</p>
<p>​ update the weight vector to</p>
<p>​ <span class="math inline">\(w(n+1)=w(n)+\eta e(n)x(n)\)</span></p>
<p>​ <span class="math inline">\(e(n)=d(n)-y(n)\)</span></p>
<p>​ where <span class="math inline">\(\eta&gt;0\)</span> and d=<span class="math inline">\(\begin{cases} 1\text{ if x belongs to class 1} \\ 0\text{ if x belongs to class 2}\end{cases}\)</span></p>
<p>​ n=n+1</p>
<p>end while</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">xc0 = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.5</span>], [<span class="number">1.0</span>, <span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.2</span>, <span class="number">1.1</span>]])</span><br><span class="line">xc1 = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2.5</span>, <span class="number">2.6</span>, <span class="number">2.4</span>, <span class="number">2.6</span>, <span class="number">2.5</span>], [<span class="number">3.0</span>, <span class="number">3.1</span>, <span class="number">3.2</span>, <span class="number">3.2</span>, <span class="number">3.0</span>]])</span><br><span class="line">w = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">fig1 = plt.plot(xc0[<span class="number">1</span>, :], xc0[<span class="number">2</span>, :], <span class="string">'bo'</span>, xc1[<span class="number">1</span>, :], xc1[<span class="number">2</span>, :], <span class="string">'ro'</span>)</span><br><span class="line">yplot = -w[<span class="number">1</span>]/w[<span class="number">2</span>]*np.array([<span class="number">0</span>, <span class="number">3</span>]) - w[<span class="number">0</span>]/w[<span class="number">2</span>]</span><br><span class="line">plt.plot(np.array([<span class="number">0</span>, <span class="number">3</span>]), yplot, <span class="string">'r'</span>)</span><br><span class="line">eta = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># This is for c0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">4</span>):</span><br><span class="line">        v = w.transpose().dot(xc0[:, j])</span><br><span class="line">        <span class="keyword">if</span> v &gt; <span class="number">0</span>:</span><br><span class="line">            y = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = <span class="number">0</span></span><br><span class="line">        e = <span class="number">0</span> - y</span><br><span class="line">        w = w + eta*e*xc0[:, j]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is for c1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">4</span>):</span><br><span class="line">        v = w.transpose().dot(xc1[:, j])</span><br><span class="line">        <span class="keyword">if</span> v &gt; <span class="number">0</span>:</span><br><span class="line">            y = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y = <span class="number">0</span></span><br><span class="line">        e = <span class="number">1</span> - y</span><br><span class="line">        w = w + eta*e*xc1[:, j]</span><br><span class="line">    yplot = -w[<span class="number">1</span>] / w[<span class="number">2</span>] * np.array([<span class="number">0</span>, <span class="number">3</span>]) - w[<span class="number">0</span>] / w[<span class="number">2</span>]</span><br><span class="line">    plt.plot(np.array([<span class="number">0</span>, <span class="number">3</span>]), yplot, color=(<span class="number">0.5</span>, <span class="number">0.5</span>, i/<span class="number">10</span>))</span><br><span class="line">    plt.pause(<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Proof of Convergence</p>
<p>There exists <span class="math inline">\(w_0\)</span> such that</p>
<p>For samples in class <span class="math inline">\(C_1\)</span> <span class="math inline">\(w_0^Tx(k)&gt;0\)</span></p>
<p>For samples in class <span class="math inline">\(C_2\)</span> <span class="math inline">\(w_0^Tx(k)&lt;0\)</span> <span class="math display">\[
w(n+1)=w(n)+e(n)x(n)=e(1)x(1)+e(2)x(2)+\dotsm+e(n)x(n)\\
w_0^Tw(n+1)=e(1)w_0^Tx(1)+e(2)w_0^Tx(2)+\dotsm+e(n)w_0^Tx(n)\\
e(k)w_0^Tx(k)=|w_0^Tx(k)|&gt;0\\
\alpha=\min\{|w_0^Tx(i)|\}
\]</span> Then we have <span class="math display">\[
w_0^Tw(n+1)\ge n\alpha\\
||w_0||\cdot||w(n+1)||\ge||w_0^Tw(n+1)||\ge n\alpha\\
\]</span> we have <span class="math display">\[
||w(n+1)||\ge\frac{n\alpha}{||w_0||}
\]</span> Let <span class="math inline">\(\beta = \max \{||x(i)||^2\}\)</span></p>
<p>And we have <span class="math display">\[
||w(n+1)||^2\le n \beta
\]</span> Overall, we have <span class="math display">\[
\frac{n^2\alpha^2}{||w_o||^2}\le ||w(n+1)||^2\le n\beta
\]</span> Therefore, n must be finite. Otherwise, the inequality will not be satisfied.</p>
<h2 id="linear-regression-problem">3. Linear Regression Problem</h2>
<p><span class="math display">\[
E(w)=\frac{1}{2}e(w)^2\\
e(w)=d-x^Tw\\
g(w)=\frac{\partial E}{\partial w}=\frac{\partial E}{\partial e}\frac{\partial e}{\partial w}=-e(w)x^T\\
\]</span></p>
<p>Then we have <span class="math display">\[
w(n+1)=w(n)+\eta e(n)x(n)
\]</span></p>
<p>Example</p>
<p>Training samples {(1, 3.5), (1, 3.5), (1.5, 2.5), (3, 2), (3.5, 1), (4, 1)}</p>
<p>Initial weight <span class="math inline">\(w(1)=[2, 0]^T\)</span></p>
<p>Learning rate <span class="math inline">\(\eta\)</span> is 0.1.</p>
<p>==Iteration 1==</p>
<p>Forward Propagation <span class="math display">\[
e(1)=d^{(1)}-w(1)^Tx^{(1)}=3.5-2=1.5
\]</span> Backward Propagation <span class="math display">\[
w(2)=w(1)+\eta x^{(1)}e(1)=[2.15\quad 0.15]^T
\]</span> ==Iteration 2==</p>
<p>Forward Propagation <span class="math display">\[
e(2)=d^{(2)}-w(2)^Tx^{(2)}=0.125
\]</span> Backward Propagation <span class="math display">\[
w(3)=w(2)+\eta x^{(2)}e(2)=[2.1625\quad 0.1688]^T
\]</span></p>
<h2 id="multilayer-perceptrons">4. Multilayer Perceptrons</h2>
<p>MLP generally adopts a smooth nonlinear activation function, such as the following logistic function</p>
<p>Sigmoid Function <span class="math display">\[
\varphi(v)=\frac{1}{1+e^{-av}}
\]</span></p>
<p>MLP is fed with an input vector <span class="math inline">\(x(n)\)</span>, and produces an output vector <span class="math inline">\(y(n)\)</span>, where <span class="math inline">\(n\)</span> is the iteration number.</p>
<p>If we have a three layers NN, for the third layer</p>
Let <span class="math inline">\(d(n)\)</span> denote the desired network output, and then the error is <span class="math display">\[
e(n)=d(n)-y(n)=d(n)-x^{(3)}_{out}(n)
\]</span> Then the cost function is <span class="math display">\[
E(n)=\frac{1}{2}\sum_{j=1}^{n_3}(d_j(n)-x_{out,j}^{(3)}(n))^2
\]</span> where <span class="math inline">\(n_3\)</span> means the number of neurons in the third layer <span class="math display">\[
x^{(3)}_{out,j}=\varphi(v^{(3)}_{j})\\
v^{(3)}_{j} = \sum_i w_{ji}^{(3)}x^{(2)}_i
\]</span> Then we have $$
<span class="math display">\[\begin{array}{rcl}
\Delta w^{(3)}_{ji}(n)&amp;=&amp;-\eta\frac{\partial E(n)}{\partial w_{ji}^{(3)}(n)}\\
&amp;=&amp;\eta(d_j(n)-x^{(3)}_{out,j})\frac{\partial x^{(3)}_{out,j}}{\partial w_{ji}^{(3)}(n)}\\
&amp;=&amp;\eta(d_j(n)-x^{(3)}_{out,j})\frac{\partial x^{(3)}_{out,j}}{\partial v_j^{(3)}}
\frac{\partial v_j^{(3)}}{\partial w_{ji}^{(3)}(n)}\\

&amp;=&amp; \eta \delta_j^{(3)}(n)x_{out,i}^{(2)}(n)
\end{array}\]</span>
<p>$$</p>
<p>where <span class="math display">\[
\delta_j^{(3)}(n)=e_j(n)\dot \varphi^{(3)}(v_j^{(3)}(n))
\]</span> For the second layer,</p>
<p>Let d(n) denote the desired network output, and the error is then <span class="math display">\[
e(n)=d(n)-y(n)=d(n)-x^{(3)}_{out}(n)
\]</span> Then the cost function is <span class="math display">\[
E(n)=\frac{1}{2}\sum_{k=1}^{n_3}(d_k(n)-x_{out,k}^{(3)}(n))^2
\]</span> where <span class="math inline">\(n_3\)</span> means the number of neurons in the third layer <span class="math display">\[
x^{(3)}_{out,k}=\varphi(v^{(3)}_{k})\\
v^{(3)}_{k} = \sum_j w_{kj}^{(3)}x^{(2)}_j\\
x^{(2)}_{out,j}=\varphi(v^{(2)}_{j})\\
v^{(2)}_{j} = \sum_i w_{ji}^{(2)}x^{(1)}_i
\]</span> Then we have <span class="math display">\[
\Delta w^{(2)}_{ji}(n)=-\eta\frac{\partial E(n)}{\partial w_{ji}^{(2)}(n)}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})\frac{\partial x^{(3)}_{out,k}}{\partial x^{(2)}_{out,j}}
\frac{\partial x^{(2)}_{out,j}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})\frac{\partial x^{(3)}_{out,k}}{\partial v^{(3)}_k}
\frac{\partial v^{(3)}_k}{\partial x^{(2)}_{out,j}}
\frac{\partial x^{(2)}_{out,j}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})
\dot \varphi^{(3)}(v_k^{(3)})
w_{kj}^{(3)}
\frac{\partial x^{(2)}_{out,j}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})
\dot \varphi^{(3)}(v_k^{(3)}(n))
w_{kj}^{(3)}
\frac{\partial x^{(2)}_{out,j}}{\partial v_{j}^{(2)}}
\frac{\partial v_{j}^{(2)}}{\partial w_{ji}^{(2)}}\\
=\eta\sum_{k=1}^{n_3}(d_k(n)-x^{(3)}_{out,k})
\dot \varphi^{(3)}(v_k^{(3)}(n))
w_{kj}^{(3)}
\dot \varphi^{(2)}(v_j^{(2)}(n))
x_{out,i}^{(1)}\\
=\eta\sum_{k=1}^{n_3}\delta_k^{(3)}
w_{kj}^{(3)}
\dot \varphi^{(2)}(v_j^{(2)}(n))
x_{out,i}^{(1)}\\
= \eta \delta_j^{(2)}(n)x_{out,i}^{(1)}(n)
\]</span> where <span class="math display">\[
\delta_j^{(2)}(n)=
\left(\sum_{k=1}^{n_3}\delta_k^{(3)}
w_{kj}^{(3)}\right)
\dot \varphi^{(2)}(v_j^{(2)}(n))\\
\]</span> Then we can summary <span class="math display">\[
w_{ji}^{(s)}(n+1)=w_{ji}^{(s)}(n)+\eta \delta_j^{(s)}(n)x_{out,i}^{(s-1)}(n)
\]</span> where <span class="math display">\[
\delta_j^{(s)}(n)=e_j(n)\dot \varphi^{(s)}\left(v_j^{(s)}(n)\right)\quad\text{Output Layer}\\
\delta_j^{(s)}(n)=
\left(\sum_{k=1}^{n_{s+1}}\delta_k^{(s+1)}(n)
w_{kj}^{(s+1)}(n)\right)
\dot \varphi^{(s)}\left(v_j^{(s)}(n)\right) \quad \text{Hidden Layer}\\
\]</span> To increase learning speed, modify the delta rule <span class="math display">\[
\Delta w_{ji}^{(s)}(k)=\alpha \Delta w_{ji}^{(s)}(k-1) + \eta \delta_j^{(s)}(k)x_{out,i}^{(s-1)}(k)\\
w_{ji}^{(s)}(n+1)=w_{ji}^{(s)}(n)+\Delta w_{ji}^{(s)}(k)
\]</span></p>
<h2 id="mlp-design-and-training-issues">5. MLP Design and Training Issues</h2>
<p>Many design and training issues to consider</p>
<h3 id="sequential-mode-or-batch-mode">1. Sequential mode or batch mode</h3>
<p>For sequential mode, weight updating is performed after each presentation of the training example. N times adjustments in one epoch.</p>
<p>For batch mode, update weight after the presentation of all the training examples. One time adjustment in one epoch.</p>
<p>Sequential learning is generally the preferred one since sequential learning often results in better solutions. Nonlinear networks often have multiple local minima. Batch learning can easily get trapped into the local minimum.</p>
<p>But sequential mode always causes fluctuation because of the noise.</p>
<p>One way to reduce fluctuation is to use mini-batches.</p>
<p>That is to start with a small batch size and increase the size as the training proceeds.</p>
<p>Second Order Method <span class="math display">\[
E(w(n+1))=E(w(n)+\Delta w(n))=E(w(n))+g^T(n)\Delta w(n)+\frac{1}{2}\Delta w^T(n)H(n)\Delta w(n)
\]</span> If we hope to get choose <span class="math inline">\(\Delta w(n)\)</span> to let <span class="math inline">\(E(w(n+1))\)</span> be minimized <span class="math display">\[
\frac{\partial E(w(n+1))}{\partial \Delta w(n)}=g^T(n)+\Delta w^T(n)H(n)=0\\
\Delta w(n) = -H^{-1}(n)g(n)
\]</span> This is so called Newton's method.</p>
<h3 id="normalize-the-input-variables">2. Normalize the input variables</h3>
<p>In general, any shift of the average input away from zero will bias the updates in a partialicular direction and this slow down learning.</p>
<p>Normalize <span class="math display">\[
\bar x_i = \frac{\sum_{n=1}^Nx_i(n)}{N}\quad \sigma=\sqrt \frac{\sum_{n=1}^N (x_i(n)-\bar x_i)}{N}\quad x_i&#39;(n) = \frac{x_i(n)-\bar x_i}{\sigma}
\]</span> Let the range are close to [-1, 1]</p>
<h3 id="how-to-choose-activation-functions-in-hidden-layers">3. How to choose activation functions in hidden layers</h3>
<p>For the hidden layers, <span class="math display">\[
\tan sig(x)=\frac{2}{1+e^{-2x}}-1
\]</span> is better than <span class="math display">\[
\log sig(x)=\frac{1}{1+e^{-x}}
\]</span> Every neuron output is closed to [-1, 1]</p>
<p>However, for the output layer, if the problem is pattern recognition problem <span class="math display">\[
\log sig(x)=\frac{1}{1+e^{-x}}
\]</span> is better because the output is 0 or 1</p>
<p>For regression problem, using linear neuron at the output layer would be more flexible.</p>
<h3 id="how-to-choose-target-values-for-pattern-recognition">4. How to choose target values for pattern recognition</h3>
<p>For the sigmoid function, the training process will try to derive the output as close as possible to the target value, that can cause the input to the output layer to be bigger and bigger in the training process. Therefore, it is better to choose [-0.6, 0.6] for tansig and [0.2, 0.8] for logsig.</p>
<h3 id="how-many-hidden-layers-and-how-many-hidden-neurons-in-each-layer">5. How many hidden layers AND how many hidden neurons in each layer</h3>
<p>Adding more layers can reduce the parameters but may make the MLP more prone to local minima traps because of its more complicated structure.</p>
<h3 id="how-to-avoid-over-fitting">6. How to avoid over-fitting</h3>
<p>One way is to limit the number of hidden units. It is important to seek the minimal structure of the MLP</p>
<p>For valid generation, the size of the weights is more important than the size of the network</p>
<p>Use regularization techniques</p>
<h3 id="use-singular-value-decomposition-svd-to-decide-the-minimal-structure">7. Use Singular Value Decomposition (SVD) to decide the minimal structure</h3>
<p>Calculate the outputs of the hidden neurons for each training sample input vector <span class="math inline">\(x(k)\)</span> <span class="math display">\[
h_{ki} = f_i(\sum_{j=1}^M w_{ij}x_j(k)+b_i)\in H
\]</span> Use SVD to get the effective rank of H. The threshold is always chosen as 0.95 or 0.99.</p>
<p>Train the MLP on the N training samples (<span class="math inline">\(X_{N\times M}\)</span>) with sufficiently large number of <span class="math inline">\(n\)</span> hidden neurons until converge.</p>
<p>Calculate the effective rank.</p>
<p>Train the MLP with number of effective rank hidden neurons until converge.</p>
<p>Repeat the procedure until convergence.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/17/Pattern Recognition/5. Non-Parametric Tech/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/03/17/Pattern Recognition/5. Non-Parametric Tech/" class="post-title-link" itemprop="url">Chapter 5. Non-Parametric Techniques</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-03-17 13:46:13 / Modified: 14:46:13" itemprop="dateCreated datePublished" datetime="2019-03-17T13:46:13+08:00">2019-03-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pattern-Recognition/" itemprop="url" rel="index"><span itemprop="name">Pattern Recognition</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="non-parametric-techniques">Non-Parametric Techniques</h1>
<h2 id="non-parametric-tech">1. Non-Parametric Tech</h2>
<p><span class="math display">\[
p(x)=\frac{k(x)/N}{V}
\]</span></p>
<p>For particular x, count number of samples k falling in the window of volume V centered at x</p>
<p>Two approaches: Parzen's Window fix <span class="math inline">\(V\)</span>, estimate <span class="math inline">\(k\)</span>; KNN: fix <span class="math inline">\(x\)</span>, determine <span class="math inline">\(V\)</span>.</p>
<h2 id="parzens-window">2. Parzen's Window</h2>
<p>Square cube function <span class="math display">\[
\phi(u)=
\begin{cases}
1\quad|u_j|\le0.5,j=1,\dotsm,d\\
0\quad otherwise
\end{cases}
\]</span> d is the dimensionality of data</p>
<p>Then <span class="math display">\[
\phi(\frac{x-x_i}{h})=
\begin{cases}
1\quad x_i\text{ inside the hypercube of width h centered at x}\\
0\quad otherwise
\end{cases}
\]</span></p>
<p><span class="math display">\[
k(x)=\sum_i\phi(\frac{x-x_i}{h})\\
p(x)=\frac{1}{N}\sum_{i=1}^N\frac{1}{h^d}\phi(\frac{x-x_i}{h})
\]</span> d is the dimension of data</p>
<p>h is the size of the window</p>
<p>N is the number of data</p>
<p>Example</p>
<p>We have 6 examples at 2,3,4,8,10,11</p>
<p>We choose d=1, h=3, then <span class="math display">\[
p(1)=\frac{1}{N}\sum_{i=1}^N\frac{1}{h^d}\phi(\frac{x-x_i}{h})\\
=\frac{1}{6}\sum_{i=1}^6\frac{1}{3}\phi(\frac{1-x_i}{3})=\frac{1}{18}\\
\]</span> Smooth window</p>
<p>Gaussians window</p>
<p>Convergence Conditions</p>
<p>The idea is that we might want to change the volume V depending on how many samples we have. So V now depends on N. As <span class="math inline">\(N\rightarrow \infty\)</span> we want <span class="math inline">\(p_N(x)\rightarrow true \space p(x)\)</span>. 4 sufficient conditions</p>
<ol type="1">
<li><span class="math inline">\(\sup_u \phi(u)\le \infty\)</span></li>
<li><span class="math inline">\(\lim_{||u||\rightarrow\infty} \phi(u)\prod_{i=1}^du_i=0\)</span> (This condition means we hope <span class="math inline">\(\phi\)</span> fall rapidly when <span class="math inline">\(u\)</span> goes to infinity)</li>
<li><span class="math inline">\(\lim_{N\rightarrow\infty}V_N=0\)</span></li>
<li><span class="math inline">\(\lim_{N\rightarrow\infty}NV_N=\infty\)</span></li>
</ol>
<p>Then we can choose <span class="math inline">\(h_N=h_1/\sqrt N\)</span></p>
<p>Setting window size h: Cross-validation</p>
<h2 id="knn-density-estimation">3. KNN Density Estimation</h2>
<p><span class="math display">\[
P_N(x)=\frac{k_N(x)/N}{V_N}
\]</span></p>
<p>Rather than fix windows size, grow volume around x until <span class="math inline">\(K_N(x)\)</span> samples</p>
<p>Convergence condition <span class="math display">\[
\lim_{N\rightarrow\infty}k_N=\infty\\
\lim_{N\rightarrow\infty}k_N/N=0
\]</span> Example</p>
<p>We have 6 examples at 2,3,4,8,10,11</p>
<p>We choose k=3 <span class="math display">\[
P_N(0)=\frac{3/6}{8}=\frac{1}{16}\\
P_N(3)=\frac{3/6}{2}=\frac{1}{4}
\]</span></p>
<h2 id="knn-classification">4. KNN Classification</h2>
<p>Given feature x, we want to estimate y <span class="math display">\[
p(x,y=c)=\frac{k_c/N}{V}
\]</span> Then we have the posterior <span class="math display">\[
p(y=c|x)=\frac{p(x,y=c)}{\sum_{c&#39;=1}^Cp(x,y=c&#39;)}=\frac{k_c}{K}
\]</span></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/30/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/30/">30</a><span class="page-number current">31</span><a class="page-number" href="/page/32/">32</a><span class="space">&hellip;</span><a class="page-number" href="/page/43/">43</a><a class="extend next" rel="next" href="/page/32/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cheng-Zilong</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cheng-Zilong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
