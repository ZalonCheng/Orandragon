<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="keywords" content="Optimization, Machine Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Cheng-Zilong">
<meta property="og:url" content="http://yoursite.com/page/20/index.html">
<meta property="og:site_name" content="Cheng-Zilong">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cheng-Zilong">
  <link rel="canonical" href="http://yoursite.com/page/20/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Cheng-Zilong</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cheng-Zilong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Learning Notes</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/13/EE5904 Neural Network/4.SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/05/13/EE5904 Neural Network/4.SVM/" class="post-title-link" itemprop="url">Chapter 4. Support Vector Machine</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-05-13 13:45:47 / Modified: 14:36:35" itemprop="dateCreated datePublished" datetime="2019-05-13T13:45:47+08:00">2019-05-13</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5904-Neural-Network/" itemprop="url" rel="index"><span itemprop="name">EE5904 Neural Network</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="support-vector-machine-svm">Support Vector Machine (SVM)</h1>
<h2 id="introduction">1. Introduction</h2>
<p>There are many planes to separate data. SVM is one of the best planes.</p>
<h2 id="data">2. Data</h2>
<p>Data set <span class="math inline">\(\sum\)</span> containing <span class="math inline">\(N+\bar N\)</span> examples is partitioned into a training set <span class="math display">\[
S=\{(x_1,d_1),\dotsm,(x_N,d_N)\}
\]</span> and a test set <span class="math display">\[
\bar S=\{(\bar x_1,\bar d_1),\dotsm,(\bar x_\bar N,\bar d_\bar N)\}
\]</span></p>
<h2 id="classification">3. Classification</h2>
<p>A hyperplane denoted by <span class="math inline">\((w,b)\)</span> can be expressed by <span class="math display">\[
g(x)=w^Tx+b=0
\]</span> Hyperplane classifies a given <span class="math inline">\(x_i\)</span> with <span class="math display">\[
sgn[g(x_i)]=
\begin{cases}
+1\quad if\quad g(x_i)&gt;0\\
-1\quad if\quad g(x_i)&lt;0
\end{cases}
\]</span> Hyperplane classifies an example <span class="math inline">\((x_i,d_i)\)</span> correctly if <span class="math display">\[
sgn[d_i g(x_i)]=1
\]</span> Two classes of data are linearly separable if and only if there exists a hyperplane that separates the two classes.</p>
<h2 id="margins">4. Margins</h2>
<p>The functional margin of an example <span class="math inline">\((x_i,d_i)\)</span> with respect to a hyperplane is defined as <span class="math display">\[
\gamma_i^f=d_i(w^Tx_i+b)
\]</span> The Geometric margin of an example <span class="math inline">\((x_i,d_i)\)</span> with respect to a hyperplane is defined as <span class="math display">\[
\gamma_i^g=d_i(\frac{1}{||w||}w^Tx_i+\frac{1}{||w||}b)
\]</span> that is the Euclidean distance from the point to the hyperplane.</p>
<p>It is easy to see that <span class="math inline">\(\frac{1}{||w||}b\)</span> is the distance from origin to the hyperplane. <span class="math inline">\(\frac{1}{||w||}w^T x_i\)</span> is the projection distance (vector projection distance) from the point <span class="math inline">\(x_i\)</span> to the subspace <span class="math inline">\(w\)</span> , therefore <span class="math inline">\(\gamma_i^g\)</span> is the Euclidean distance from the point to the hyperplane.</p>
<p>The hyperplane does not change when scaled by a real constant. <span class="math display">\[
w^Tx+b=0\\
cw^Tx+cb=0
\]</span> <span class="math inline">\(\gamma_i^g\)</span> does not change but <span class="math inline">\(\gamma_i^f\)</span> changes (This property is important).</p>
<p>We have the following equation <span class="math display">\[
\gamma_i^g=\frac{\gamma_i^f}{||w||}
\]</span> The functional margin of the training set is defined as <span class="math display">\[
\gamma^f=\min_{1\le i\le N}\{\gamma_i^f\}
\]</span> The geometric margin of a training set is defined as <span class="math display">\[
\gamma^g=\min_{1\le i \le N}\{\gamma_i^g\}
\]</span> The training set S can have different geometric margins depending on how hyperplane is defined.</p>
<p>The optimal hyperplane for a given S is one that gives maximum <span class="math inline">\(\gamma^g\)</span> over all possible hyperplanes. <span class="math display">\[
\gamma =\max\{\gamma^g_{(w_1,b_1)},\gamma^g_{(w_2,b_2)},\dotsm\}
\]</span></p>
<h2 id="primal">5. Primal</h2>
<p>How to find the margin of S?</p>
<p>For a given w, We have <span class="math display">\[
\gamma_i^g=\frac{\gamma_i^f}{||w||}
\]</span> The sample k that yields <span class="math display">\[
\gamma^f=\gamma^f_k
\]</span> also yields <span class="math display">\[
\gamma^g =\gamma_k^g
\]</span> So we have <span class="math display">\[
\gamma^g=\frac{\gamma^f}{||w||}
\]</span> therefore, we can maximize <span class="math inline">\(\gamma^g\)</span> by fixing <span class="math inline">\(\gamma^f\)</span> and then minimizing <span class="math inline">\(||w||\)</span> <span class="math display">\[
\gamma^f = d_k(w^Tx_k+b)=1
\]</span> That means we choose different to satisfy <span class="math inline">\(\gamma^f=1\)</span> . Without this constraint, we have infinite solutions <span class="math inline">\(w\)</span> since <span class="math inline">\(w&#39;=cw\)</span> and <span class="math inline">\(b&#39;=cb\)</span> where <span class="math inline">\(c\)</span> can any number.</p>
<p>With <span class="math inline">\(\gamma^f\)</span> fixed at 1, for any examples in <span class="math inline">\(x_i\)</span> , we have <span class="math display">\[
d_i(w^Tx_i+b)\ge1
\]</span></p>
<p>Therefore we get the constraint optimization problem, <span class="math display">\[
\begin{array}{rl}
\text{Minimize:} &amp; f(w)=\frac{1}{2}||w||^2=\frac{1}{2}w^Tw\\
\text{Subject to:} &amp; d_i(w^Tx_i+b)\ge 1
\end{array}
\]</span> Solving this problem yields the optimal hyperplane <span class="math inline">\((w_0,b_0)\)</span></p>
<p>Then the discriminant function is <span class="math display">\[
g(x)=w_0^Tx+b_0
\]</span></p>
<h2 id="lagrange-multipliers-and-kkt-conditions">6. Lagrange Multipliers and KKT Conditions</h2>
<p>For the constraint optimization problem <span class="math display">\[
\begin{array}{rl}
\text{Minimize:} &amp; f(w)\\
\text{Subject to:} &amp; h(w)=0\\
\end{array}
\]</span> We can use Lagrange Theorem to solve. The necessary for the existence of an optimal solution <span class="math inline">\(w_0\)</span> corresponding to a minimum of <span class="math inline">\(f(w)\)</span> subject to <span class="math inline">\(h_i(w)=0\)</span> are <span class="math display">\[
\frac{\partial L(w_0,\beta_0)}{\partial w}=0\\
\frac{\partial L(w_0,\beta_0)}{\partial \beta_i}=0\\
\]</span></p>
<p>where we define the Lagrangian Function <span class="math display">\[
L(w,\beta)=f(w)+\sum_{i=1}^m \beta_i h_i(w)
\]</span> Lagrange Theorem can be generalized to deal with problems having both equality and inequality constraints.</p>
<p>For the primal problem, <span class="math display">\[
\begin{array}{rl}
\text{Minimize:} &amp; f(w)\\
\text{Subject to:} &amp; q_i(w)\le 0\\
&amp; h_i(w)=0
\end{array}
\]</span> We have the generalized Lagrangian function <span class="math display">\[
L(w,\alpha,\beta)=f(w)+\sum_{i=1}^k \alpha_iq_i(w)+\sum_{i=1}^m\beta_ih_i(w)
\]</span> where <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\beta_i\)</span> are the Lagrange Multipliers.</p>
<p>In order to solve the primal problem, we have Kuhn-Tucker Theorem</p>
<p>The sufficient and necessary condition for a point <span class="math inline">\(w_o\)</span> to be an optimal solution is the existence of <span class="math inline">\(\alpha_o\)</span> and <span class="math inline">\(\beta_o\)</span> such that <span class="math display">\[
\begin{array}{rcl}
\frac{\partial L(w_o,\alpha_o\beta_o)}{\partial w}&amp;=&amp;0\\
\frac{\partial L(w_o,\alpha_o\beta_o)}{\partial \beta_i}&amp;=&amp;0\\
\alpha_{o,i}q_i(w_o)&amp;=&amp;0\\
q_i(w_o)&amp;\le&amp;0\\
\alpha_{o,i}&amp;\ge&amp;0
\end{array}
\]</span></p>
<h2 id="lagrange-dual-problem">7. Lagrange Dual Problem</h2>
<p>In mathematical optimization theory, duality or the duality principle is the principle that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem. The solution to the dual problem provides a lower bound to the solution of the primal (minimization) problem. However in general the optimal values of the primal and dual problems need not be equal. Their difference is called the ==duality gap==. For convex optimization problems, the duality gap is zero under a constraint qualification condition (Regularity conditions). (From Wikipedia Duality (Optimization))</p>
<p>The Lagrangian dual problem is obtained by forming the Lagrangian of a minimization problem by using nonnegative Lagrange multipliers to add the constraints to the objective function, and then solving for the primal variable values that minimize the original objective function. This solution gives the primal variables as functions of the Lagrange multipliers, which are called dual variables, so that the new problem is to maximize the objective function with respect to the dual variables under the derived constraints on the dual variables (including at least the nonnegativity constraints).</p>
For a convex minimization problem with inequality constraints (primal problem), <span class="math display">\[
\begin{array}{ll}{\underset{x}{\operatorname{minimize}}} &amp; {f(x)} \\ {\text { subject to }} &amp; {g_{i}(x) \leq 0, \quad i=1, \ldots, m}\end{array}
\]</span> The Lagrangian Dual Problem is <span class="math display">\[
\begin{array}{cl}{\underset{u}{\operatorname{maximize}}} &amp; {\inf _{x}\left(f(x)+\sum_{j=1}^{m} u_{j} g_{j}(x)\right)} \\ {\text { subject to }} &amp; {u_{i} \geq 0, \quad i=1, \ldots, m}\end{array}
\]</span> The infimum occurs where the gradient is equal to zero. Then problem is denoted as <span class="math display">\[
\begin{array}{cl}
\underset{x, u}{\operatorname{maximize}} &amp; f(x)+\sum_{j=1}^{m} u_{j} g_{j}(x) \\ 
\text { subject to } &amp; \nabla f(x)+\sum_{j=1}^{m} u_{j} \nabla g_{j}(x)=0 \\ 
&amp; {u_{i} \geq 0, \quad i=1, \ldots, m}
\end{array}
\]</span> Therefore, for the primal problem of SVM, <span class="math display">\[
\begin{array}{rl}
\text{Minimize:} &amp; f(w)=\frac{1}{2}||w||^2=\frac{1}{2}w^Tw\\
\text{Subject to:} &amp; d_i(w^Tx_i+b)\ge 1
\end{array}
\]</span> we have the dual problem <span class="math display">\[
\begin{array}{rl}
\text{Maximize:} &amp; L(w,\alpha,\beta)\\
\text{Subject to:} &amp; \frac{\partial L(w_o,\alpha_o,\beta_o)}{\partial w}=0\\
&amp;\alpha\ge0 
\end{array}
\]</span> Then we hope to reduce the variables in the dual problem <span class="math display">\[
\begin{array}{rcl}
q_i &amp;=&amp; -d_i(w^Tx_i+b)+1\le0\\
L(w,b,\alpha)&amp;=&amp;\frac{1}{2}w^Tw-\sum_{i=1}^N\alpha_i(d_i(w^Tx_i+b)-1)\\
&amp;=&amp;\frac{1}{2}w^Tw-\sum_{i=1}^N\alpha_id_iw^Tx_i-b\sum_{i=1}^N\alpha_id_i+\sum_{i=1}^N\alpha_i
\end{array}
\]</span> Then we have <span class="math display">\[
\begin{array}{rcl}
\frac{\partial L(w_o,\alpha_o,\beta_o)}{\partial w}&amp;=&amp; \frac{1}{2}\frac{\partial w^Tw}{\partial w}-\sum_{i=1}^N\alpha_id_i(\frac{\partial w^Tx_i}{\partial w})\\
&amp;=&amp;w-\sum_{i=1}^N\alpha_id_ix_i=0\\
w&amp;=&amp;\sum_{i=1}^N\alpha_id_ix_i\\
\frac{\partial L(w_o,\alpha_o,\beta_o)}{\partial b}&amp;=&amp;\sum_{i=1}^Na_id_i=0
\end{array}
\]</span> Then we have the simplified dual problem $$
<span class="math display">\[\begin{array}{rcl}
L(w,b,\alpha)&amp;=&amp;\frac{1}{2}w^Tw-\sum_{i=1}^N\alpha_id_iw^Tx_i-b\sum_{i=1}^N\alpha_id_i+\sum_{i=1}^N\alpha_i\\

&amp;=&amp;\frac{1}{2}\left[\sum_{i=1}^N\alpha_id_ix_i^T\right]\left[\sum_{i=1}^N\alpha_id_ix_i\right]\\
&amp;&amp;-\sum_{i=1}^N\alpha_id_i\left[\sum_{i=1}^N\alpha_id_ix_i^T\right]x_i\\
&amp;&amp;-b\sum_{i=1}^N\alpha_id_i+\sum_{i=1}^N\alpha_i\\

&amp;=&amp; \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_jx_i^Tx_i\\
&amp;&amp;-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_jx_i^Tx_i\\
&amp;&amp;-0+\sum_{i=1}^N\alpha_i
\end{array}\]</span>
<span class="math display">\[
Then we have the dual problem
\]</span>
<span class="math display">\[\begin{array}{rl}
\text{Maximize:} &amp; Q(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}
{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_jx_i^Tx_i\\

\text{Subject to:} &amp; \alpha_i\ge0 \\
&amp;\sum_{i=1}^Na_id_i=0
\end{array}\]</span>
<p><span class="math display">\[
From the KKT condition, we have
\]</span> _{o,i}(d_i(w_o^Tx_i+b_o)-1)=0 $$ For the support vectors, we have <span class="math inline">\(d_i(w_o^Tx_i+b_o)-1=0\)</span> Then <span class="math inline">\(\alpha_{o,i}\neq 0\)</span></p>
<p>Furthermore, fore a support vector <span class="math inline">\(x_i\)</span>, <span class="math inline">\(d_i(w_o^Tx_i+b_o)-1=0\)</span> then <span class="math inline">\(b_o=\frac{1}{d_i}-w_o^Tx_i\)</span> .</p>
<p>As <span class="math inline">\(\alpha_{o,i}\)</span> is obtained, we can calculate <span class="math inline">\(w_o\)</span> and <span class="math inline">\(b_o\)</span> as follows <span class="math display">\[
w_o=\sum_{i=1}^N \alpha_{o,i}d_ix_i\\
b_o=\frac{1}{d^{(s)}}-w_o^Tx^{(s)}\\
\]</span> <span class="math inline">\(\alpha_{o,i}\)</span> can be solved by many methods such as quadratic programming.</p>
<h2 id="soft-margin">8. Soft Margin</h2>
<p>We add nonnegative slack variables <span class="math inline">\(\xi_i\)</span></p>
<p><span class="math inline">\(\xi_i =0\)</span> when data point is not in region of separation</p>
<p><span class="math inline">\(0\le \xi_i\le 1\)</span> when data point is in the region of separation and on correct side of hyperplane</p>
<p><span class="math inline">\(\xi_i &gt; 0\)</span> when data point is in the region of separation but on wrong side of hyperplane</p>
<p>We hope to minimize the cost function <span class="math display">\[
f(w,\xi)=\frac{1}{2}w^Tw+C\sum_{i=1}^N \xi_i
\]</span> A large C generally leads to smaller margin but also fewer misclassification</p>
<p>A small C generally leads to larger margin but also more misclassification</p>
Then for an example we have <span class="math display">\[
d_i(w^Tx_i+b)\ge 1-\xi_i 
\]</span> Finally we have the dual problem $$
<span class="math display">\[\begin{array}{rl}
\text{Maximize:} &amp; Q(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}
{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_jx_i^Tx_i\\

\text{Subject to:} &amp; 0\le\alpha_i\le C \\
&amp;\sum_{i=1}^Na_id_i=0
\end{array}\]</span>
<p>$$ Then we try to find the support vector</p>
<ol type="1">
<li>For the support vector with <span class="math inline">\(\alpha_i&lt;C\)</span> . Support is located on the boarder of margin of separation.</li>
<li>For the support vector with <span class="math inline">\(\alpha_i=C\)</span>. Support is located inside the margin of separation.</li>
</ol>
<p>Then for each sample <span class="math inline">\(x_i\)</span> with <span class="math inline">\(0\le \alpha_i \le C\)</span> <span class="math display">\[
w_o=\sum_{i=1}^N\alpha_{o,i}d_ix_i\\
b_{o,i}=\frac{1}{d_i}-w_o^Tx_i\\
b_o=\frac{\sum_{i=1}^mb_{o,i}}{m}
\]</span></p>
<h2 id="kernel">9. Kernel</h2>
<p>Cover's Theorem</p>
<p>Probability that classes are linearly separable increases when data points in input space are nonlinearly mapped to a higher dimensional feature space.</p>
<p>Then if we map <span class="math inline">\(x_i\)</span> to <span class="math inline">\(\varphi(x_i)\)</span></p>
The discriminant function becomes <span class="math display">\[
g(x)=w_o^T\varphi(x)+b_o
\]</span> The dual problem becomes $$
<span class="math display">\[\begin{array}{rl}
\text{Maximize:} &amp; Q(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}
{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_j\varphi(x_i)^T\varphi(x_j)\\

\text{Subject to:} &amp; 0\le\alpha_i\le C \\
&amp;\sum_{i=1}^Na_id_i=0
\end{array}\]</span>
<span class="math display">\[
Given the optimal value $\alpha_{o,i}$ 
\]</span> w_o=<em>{i=1}^N</em>{o,i}d_i(x_i)\ b_o=-w_o<sup>T(x</sup>{(s)})\ <span class="math display">\[
Solution requires $\varphi$ but finding explicit form of $\varphi$ is not easy, then we define
\]</span> K(x,y)=^T(x)(y) <span class="math display">\[
The dual problem becomes
\]</span>
<span class="math display">\[\begin{array}{rl}
\text{Maximize:} &amp; Q(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}
{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_jK(x_i,x_j)\\

\text{Subject to:} &amp; 0\le\alpha_i\le C \\
&amp;\sum_{i=1}^Na_id_i=0
\end{array}\]</span>
<span class="math display">\[
Given the optimal value $\alpha_{o,i}$, we have
\]</span> b_o=-<em>{i=1}<sup>N<em>{o,i}d_iK(x_i,x^{(s)}) <span class="math display">\[
The discriminant function is 
\]</span> g(x)=</em>{i=1}</sup>N</em>{o,i}d_iK(x_i,x)+b_o <span class="math display">\[
==Attention== the expression for $K$ must satisfy Mercer&#39;s Condition
\]</span> K=
<span class="math display">\[\begin{bmatrix}
k(x_1,x_1) &amp; \dotsm &amp; K(x_1,x_N)\\
\vdots &amp; \ddots &amp; \vdots\\
K(x_N,x_1) &amp; \dotsm &amp; K(x_N,x_N)
\end{bmatrix}\]</span>
<p>$$ is positive semi-definite.</p>
<h2 id="summary">10. Summary</h2>
<p>Given a training set <span class="math inline">\(S=\{(x_i,d_i)\}\)</span></p>
<ol type="1">
<li>Find a suitable kernel and check Mercer's condition</li>
<li>Choose a value for C</li>
<li>Solve for <span class="math inline">\(\alpha_{o,i}\)</span></li>
<li>Determine <span class="math inline">\(b_o\)</span></li>
<li>Use discriminant function</li>
</ol>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/11/EE5904 Neural Network/3.SOM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cheng-Zilong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cheng-Zilong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/05/11/EE5904 Neural Network/3.SOM/" class="post-title-link" itemprop="url">Chapter 3. Self-Organizing Maps</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-05-11 13:45:47 / Modified: 14:36:19" itemprop="dateCreated datePublished" datetime="2019-05-11T13:45:47+08:00">2019-05-11</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5904-Neural-Network/" itemprop="url" rel="index"><span itemprop="name">EE5904 Neural Network</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="self-organizing-maps">Self-Organizing Maps</h1>
<h2 id="introduction">1. Introduction</h2>
<p>Learning is a process by which the free parameters of a neural network are adapted through a process of stimulation by the environment in which the network is embedded.</p>
<p>The topology-conserving mapping can be achieved by SOMs. The network is two layers network. There are only input layer and output layer without hidden layer. There is no hidden layer.</p>
<p>The location of neurons in the output layer is important.</p>
<h2 id="architecture">2. Architecture</h2>
<p>For each output neuron j, there are a set of synaptic weights <span class="math inline">\(w_{ji}\)</span> connected from all input neurons</p>
<h3 id="randomly-initialize-all-the-weights.">1. Randomly initialize all the weights.</h3>
<h3 id="select-input-vector-xx_1x_2dotsmx_n-from-the-training-set">2. Select input vector <span class="math inline">\(x=[x_1,x_2,\dotsm,x_n]\)</span> from the training set</h3>
<p>It can be done by choosing randomly from the training set or one by one in a deterministic manner like that for sequential learning.</p>
<h3 id="compare-x-with-weights-w_j-for-each-neuron-j-to-determine-winner">3. Compare x with weights <span class="math inline">\(w_j\)</span> for each neuron j to determine winner</h3>
<p>Find the best-matching neuron w(x), usually the neuron whose weight vector has smallest Euclidean distance from the input vector. <span class="math display">\[
i(x)=\arg \min _j||x-w_j||
\]</span> The winning neuron is that which is in some sense closet to the input sector.</p>
<h3 id="update-winner-so-that-it-becomes-more-like-x-together-with-the-winners-neighbor">4. Update winner so that it becomes more like x, together with the winner's neighbor</h3>
<p>The typical choice of topological neighborhood function <span class="math display">\[
h_{j,i(x)}=\exp (-\frac{d_{j,i}^2}{2\sigma^2})
\]</span> where <span class="math display">\[
d_{j,i}=\sqrt{(l-n)^2+(m-k)^2}
\]</span> if the position of the neuron is described by the index of the neuron in the matrix (2-d lattice)</p>
<h3 id="adjust-parameters-learning-rate-neighborhood-function">5. Adjust parameters: learning rate &amp; neighborhood function</h3>
<p>Another unique feature of the SOM algorithm is that the size of the topological neighborhood shrinks with time. <span class="math display">\[
\sigma(n)=\sigma_0\exp (-\frac{n}{\tau_1})
\]</span> where <span class="math inline">\(\tau_1\)</span> is a time-constant to control the decay rate of the effective width.</p>
<p>Time-varying neighborhood function <span class="math display">\[
h_{j,i(x)}=\exp (-\frac{d_{j,i}^2}{2\sigma(n)^2})
\]</span> It means that the influence of the winner on its neighbors decrease with time.</p>
<p>Parameters are adapted by <span class="math display">\[
w_j(n+1)=w_j(n)+\eta(n)h_{j,i(x)}(n)(x-w_j(n))
\]</span> That means the weight is the weighted average of the input and the current weight.</p>
<p>The synaptic weight vector <span class="math inline">\(w_i\)</span> of winning neuron i moves to the input vector x.</p>
<p>All the neurons in the neighborhood of the winning neuron also move to the input vector.</p>
<p>The learning-rate parameter <span class="math inline">\(\eta\)</span> should also decrease with time <span class="math display">\[
\eta(n)=\eta_0 \exp (-\frac{n}{\tau_2})
\]</span></p>
<h3 id="repeat-step-2-until-the-map-has-converged.">6. Repeat step 2 until the map has converged.</h3>
<h2 id="two-phases-of-the-adaptive-process">3. Two phases of the adaptive process</h2>
<h3 id="the-first-phase">1. The First Phase</h3>
<p>An ordering or self-organizing phase</p>
<p>The ordering phase may take as many as 1000 iterations and possibly more.</p>
<p>The learning rate should begin with a value close to 0.1; there after it should decrease gradually, but remain above 0.01. <span class="math display">\[
\eta_0=0.1\quad \tau_2=T\quad \eta(n)=0.1\exp(-\frac{n}{T})
\]</span> T is the total number of iterations for the first phase.</p>
<p>The neighborhood function should initially include almost all neurons, and then shrink with time. We may set the initial size of the width equal to the “radius” of the lattice. For instance if the size of the lattice is <span class="math inline">\(M\times N\)</span>, then the initial width can be set as <span class="math display">\[
\sigma_0=\frac{\sqrt{M^2+N^2}}{2}
\]</span> The time constant can be chosen as <span class="math display">\[
\tau_1=\frac{T}{\log (\sigma_0)}
\]</span> Then at the end, <span class="math inline">\(\sigma(T)=1\)</span></p>
<h3 id="the-second-phase">2. The Second Phase</h3>
<p>Convergence Phase</p>
<p>The second phase is needed to fine tune the feature map.</p>
<p>As a general rule, the number of iterations must be at least 500 times the number of neurons in the network. Thus, the convergence phase may have to go on for thousands and possibly tens of thousands of iterations.</p>
<p>For a good statistical accuracy, the learning parameter <span class="math inline">\(\eta(n)\)</span> should be kept at a small value, on the order of 0.01.</p>
<p>In many applications, the second phase is not needed if convergence of the parameters are not critical.</p>
<h2 id="summary">4. Summary</h2>
<p>For n-dimensional input space and m output neurons</p>
<ol type="1">
<li>Randomly initialize the weight vector <span class="math inline">\(w_i\)</span> for neuron</li>
<li>Sampling: choose an input vector x from the training set</li>
<li>Determine winner neuron k</li>
<li>Update all weight vectors of all neurons (include itself) in the neighborhood of the winning neuron</li>
<li>If convergence criterion met, Stop</li>
</ol>
<p>Example</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">X = <span class="built_in">randn</span>(<span class="number">800</span>,<span class="number">2</span>);</span><br><span class="line">s2 = sum(X.^<span class="number">2</span>,<span class="number">2</span>);</span><br><span class="line">trainX = (X.*<span class="built_in">repmat</span>(<span class="number">1</span>*(<span class="built_in">gammainc</span>(s2/<span class="number">2</span>,<span class="number">1</span>).^(<span class="number">1</span>/<span class="number">2</span>))./<span class="built_in">sqrt</span>(s2),<span class="number">1</span>,<span class="number">2</span>))';</span><br><span class="line"><span class="built_in">plot</span>(trainX(<span class="number">1</span>,:),trainX(<span class="number">2</span>,:),<span class="string">'+r'</span>); </span><br><span class="line">axis equal</span><br><span class="line"></span><br><span class="line">total_step_number = <span class="number">400000</span>;</span><br><span class="line">W = <span class="built_in">rand</span>(<span class="number">2</span>,<span class="number">64</span>); <span class="comment">%  row number (x-1)/8+1, column number (x-1)%8+1</span></span><br><span class="line">eta_0=<span class="number">0.1</span>; eta = eta_0; </span><br><span class="line">sigma_0 = <span class="number">4</span>; sigma = sigma_0;</span><br><span class="line"></span><br><span class="line">tau = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:(total_step_number/<span class="built_in">size</span>(trainX,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> xx=trainX</span><br><span class="line">        [~,winner_index] = <span class="built_in">min</span>(vecnorm(xx-W));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> nn=<span class="number">1</span>:<span class="built_in">size</span>(W,<span class="number">2</span>) <span class="comment">%update neighbors</span></span><br><span class="line">            d_ji=norm([<span class="built_in">fix</span>((nn<span class="number">-1</span>)/<span class="number">8</span>+<span class="number">1</span>);<span class="built_in">mod</span>((nn<span class="number">-1</span>),<span class="number">8</span>)+<span class="number">1</span>]- ...</span><br><span class="line">                [<span class="built_in">fix</span>((winner_index<span class="number">-1</span>)/<span class="number">8</span>+<span class="number">1</span>);<span class="built_in">mod</span>((winner_index<span class="number">-1</span>),<span class="number">8</span>)+<span class="number">1</span>]);</span><br><span class="line">        </span><br><span class="line">            h_ji=<span class="built_in">exp</span>(-(d_ji^<span class="number">2</span>/(<span class="number">2</span>*sigma^<span class="number">2</span>)));</span><br><span class="line">            W(:,nn) =  W(:,nn) + eta*h_ji*(xx-W(:,nn));</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        eta = eta_0 * <span class="built_in">exp</span>(-(<span class="built_in">i</span>/(total_step_number/<span class="built_in">size</span>(trainX,<span class="number">2</span>))));</span><br><span class="line">        sigma = sigma_0 * <span class="built_in">exp</span>(-(<span class="built_in">i</span>/tau));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">figure</span>(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">axis equal</span><br><span class="line"><span class="built_in">plot</span>(trainX(<span class="number">1</span>,:),trainX(<span class="number">2</span>,:),<span class="string">'+r'</span>);</span><br><span class="line"><span class="built_in">plot</span>(W(<span class="number">1</span>,:),W(<span class="number">2</span>,:),<span class="string">'*b'</span>)</span><br><span class="line"></span><br><span class="line">result = <span class="built_in">zeros</span>(<span class="number">2</span>,<span class="number">8</span>,<span class="number">8</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">64</span></span><br><span class="line">    result(:,<span class="built_in">fix</span>((<span class="built_in">i</span><span class="number">-1</span>)/<span class="number">8</span>+<span class="number">1</span>),<span class="built_in">mod</span>((<span class="built_in">i</span><span class="number">-1</span>),<span class="number">8</span>)+<span class="number">1</span>) = W(:,<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">8</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="number">7</span></span><br><span class="line">       <span class="built_in">plot</span>([result(<span class="number">1</span>,<span class="built_in">i</span>,<span class="built_in">j</span>),result(<span class="number">1</span>,<span class="built_in">i</span>,<span class="built_in">j</span>+<span class="number">1</span>)],[result(<span class="number">2</span>,<span class="built_in">i</span>,<span class="built_in">j</span>),result(<span class="number">2</span>,<span class="built_in">i</span>,<span class="built_in">j</span>+<span class="number">1</span>)],<span class="string">'*-'</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">7</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="number">8</span></span><br><span class="line">       <span class="built_in">plot</span>([result(<span class="number">1</span>,<span class="built_in">i</span>,<span class="built_in">j</span>),result(<span class="number">1</span>,<span class="built_in">i</span>+<span class="number">1</span>,<span class="built_in">j</span>)],[result(<span class="number">2</span>,<span class="built_in">i</span>,<span class="built_in">j</span>),result(<span class="number">2</span>,<span class="built_in">i</span>+<span class="number">1</span>,<span class="built_in">j</span>)],<span class="string">'*-'</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<figure>
<img src="assets/1555772820715.png" alt="Figure."><figcaption aria-hidden="true">Figure.</figcaption>
</figure>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/19/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><span class="page-number current">20</span><a class="page-number" href="/page/21/">21</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="extend next" rel="next" href="/page/21/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cheng-Zilong</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">71</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cheng-Zilong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
