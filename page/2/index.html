<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Orandragon&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Orandragon&#39;s Blog">
  <link rel="canonical" href="http://yoursite.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/23/Nonlinear Optimization/3.1 Basic Convex Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/08/23/Nonlinear Optimization/3.1 Basic Convex Analysis/" class="post-title-link" itemprop="url">3. Basic Convex Analysis (1)</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-08-23 17:28:12" itemprop="dateCreated datePublished" datetime="2019-08-23T17:28:12+08:00">2019-08-23</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-07 17:29:50" itemprop="dateModified" datetime="2019-09-07T17:29:50+08:00">2019-09-07</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="basic-convex-analysis-1">3. Basic Convex Analysis (1)</h1>
<p>Linear functions are very important in linear programming and applied mathematics. However, convex functions have a wider range of applications than the class of linear functions. In fact, linear functions are special convex functions.</p>
<h2 id="convex-sets">3.1 Convex sets</h2>
<p><strong>Definition</strong> (Convex set)</p>
<p>A set <span class="math inline">\(D\subseteq \mathbb R^n\)</span> is said to be convex if for any two points <span class="math inline">\(x,y\in D\)</span>, the linear segment joining <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> also lies in <span class="math inline">\(D\)</span>. That is, <span class="math display">\[
\lambda x+(1-\lambda )y \in D \quad\forall \lambda \in[0,1],\forall x,y\in D.
\]</span> <strong>Notation</strong></p>
<p>We will sometimes use <span class="math inline">\([x,y]\)</span> to denote the line segment joining <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, i.e. <span class="math inline">\([x,y]=\{\lambda x+(1-\lambda)y:\lambda\in[0,1]\}\)</span>.</p>
<p>To prove a given set <span class="math inline">\(C\)</span> is convex, we use the following procedures,</p>
<ol type="1">
<li>Choose two arbitrary points <span class="math inline">\(x,y\in C\)</span> and any <span class="math inline">\(\lambda \in [0,1]\)</span>.</li>
<li>Argue that <span class="math inline">\(\lambda x+(1-\lambda )y\in C\)</span>.</li>
</ol>
<p><strong>Example</strong> (Half space)</p>
<p>Given <span class="math inline">\(a\in \mathbb R^n\)</span>, the half space <span class="math inline">\(H=\{x\in \mathbb R^n|a^Tx\le \alpha\}\)</span> is a convex set.</p>
<p>Proof.</p>
<p>Consider <span class="math inline">\(x,y\in H\)</span>, and choose <span class="math inline">\(\lambda \in [0,1]\)</span>, we have <span class="math display">\[
a^T(\lambda x+(1-\lambda )y)=\lambda a^Tx+(1-\lambda)a^Ty\le \lambda \alpha+(1-\lambda)\alpha=\alpha.
\]</span> <strong>Example</strong> (n-ball)</p>
<p>Given <span class="math inline">\(a\in \mathbb R^n\)</span>, <span class="math inline">\(\alpha &gt;0\)</span>. The n-ball <span class="math display">\[
\bar B(a,\alpha)=\{x\in \mathbb R^n\,|\,||x-a||\le \alpha\}
\]</span> is a convex set.</p>
<p>Proof.</p>
<p>Consider <span class="math inline">\(x,y\in \bar B\)</span>, and choose <span class="math inline">\(\lambda \in [0,1]\)</span>, we have <span class="math display">\[
\begin{array}{rCl}
||\lambda x+(1-\lambda )y-a||&amp;=&amp;||\lambda x+(1-\lambda)y-\lambda a-(1-\lambda)a||\\
&amp;=&amp;||\lambda(x-a)+(1-\lambda)(y-a)||\\
&amp;\le&amp; \lambda ||x-a||+(1-\lambda )||y-a||\\
&amp;\le&amp; \lambda \alpha +(1-\lambda)\alpha\\ 
&amp;=&amp;\alpha.
\end{array}
\]</span> It is not very easy to prove the convexity by definition.</p>
<p><strong>Proposition</strong></p>
<p>(Finite or Infinite) intersection of convex sets is a convex set, i.e. if <span class="math inline">\(C_1,C_2,\dots,C_m\)</span> are convex sets in <span class="math inline">\(\mathbb R^n\)</span>, then <span class="math inline">\(C=\cap_{i=1}^m C_i\)</span> is also convex.</p>
<p>Proof.</p>
<p>Choose <span class="math inline">\(x,y \in \bigcap_{i=1}^k C_i\)</span> and <span class="math inline">\(\lambda\in [0,1]\)</span>. Since <span class="math inline">\(x,y \in C_i\)</span> and <span class="math inline">\(C_i\)</span> is a convex set. We have <span class="math inline">\(\lambda x+(1-\lambda)y\in C_i\;\forall i=1,\dotsm k\)</span>. That means <span class="math inline">\(\lambda x+(1-\lambda)y\in \bigcap_{i=1}^k C_i\)</span>. Q.E.D.</p>
<p><strong>Example</strong> (Polyhedron or polyhedral set)</p>
<p>The polyhedron or polyhedral sets <span class="math inline">\(\{x\in\mathbb R^n\;|\;Ax\le b\}\)</span> are convex.</p>
<h2 id="convex-and-concave-functions">3.2 Convex and concave functions</h2>
<p><strong>Definition</strong> (Convex function/Strictly convex function)</p>
<p>Let <span class="math inline">\(D\subseteq \mathbb R^n\)</span> be a <strong>convex set</strong>. Consider a function <span class="math inline">\(f:D\rightarrow \mathbb R\)</span>.</p>
<ol type="a">
<li>The function is said to be convex if <span class="math display">\[
f(\lambda x+(1-\lambda )y)\le \lambda f(x)+(1-\lambda )f(y)\quad \forall x,y\in D,\lambda\in[0,1]
\]</span></li>
<li>The function is said to be strictly convex if <span class="math display">\[
f(\lambda x+(1-\lambda )y)&lt; \lambda f(x)+(1-\lambda )f(y)\quad \forall x,y\in D,\lambda\in[0,1]
\]</span> <strong>Notation</strong></li>
</ol>
<p><span class="math inline">\(-f\)</span> is convex $$ <span class="math inline">\(f\)</span> is concave.</p>
<p>To prove that a given function <span class="math inline">\(f:D\rightarrow \mathbb R\)</span> is convex, we use the following procedures,</p>
<ol type="1">
<li>Choose two arbitrary points <span class="math inline">\(x,y\in D\)</span> and any <span class="math inline">\(\lambda \in [0,1]\)</span></li>
<li>Argue that <span class="math inline">\(f(\lambda x+(1-\lambda )y)\le \lambda f(x)+(1-\lambda)f(y)\)</span>.</li>
</ol>
<p><strong>Example</strong></p>
<p>Let <span class="math inline">\(f:\mathbb R^2\rightarrow \mathbb R\)</span> where <span class="math inline">\(f(x)=||x||=(x_1^2+x_2^2)\frac{1}{2}\)</span>, show that <span class="math inline">\(f\)</span> is convex in <span class="math inline">\(\mathbb R^2\)</span>.</p>
<p>Proof.</p>
<p>Choose <span class="math inline">\(x,y\in \mathbb R^2\)</span>, and <span class="math inline">\(\lambda \in [0,1]\)</span>, then we have <span class="math display">\[
f(\lambda x+(1-\lambda)y)=||\lambda x+(1-\lambda )y||\le \lambda ||x||+(1-\lambda)||y||.
\]</span> <strong>Example</strong></p>
<p>Let <span class="math inline">\(a\in \mathbb R^n\)</span> and <span class="math inline">\(\alpha \in \mathbb R\)</span>. Consider <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R,f(x)=a^Tx+\alpha\)</span>. Prove that <span class="math inline">\(f\)</span> is convex.</p>
<p>Proof.</p>
<p>Choose <span class="math inline">\(x,y\in \mathbb R^2\)</span>, and <span class="math inline">\(\lambda \in [0,1]\)</span>, then we have <span class="math display">\[
f(\lambda x+(1-\lambda)y)=a^T(\lambda x+(1-\lambda)y)+\alpha =\lambda f(x)+(1-\lambda)f(y).
\]</span> <strong>Example</strong></p>
<p>Show that <span class="math inline">\(f:\mathbb R^2\rightarrow \mathbb R\)</span> defined by <span class="math inline">\(f(x)=x_1^2-4x_2\)</span> is convex.</p>
<p>Proof.</p>
<p>Choose <span class="math inline">\(x,y\in \mathbb R^2\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>, then we have <span class="math display">\[
\begin{array}{rCl}
f(\lambda x+(1-\lambda)y)&amp;=&amp;(\lambda x_1+(1-\lambda)y_1)^2-4(\lambda x_2+(1-\lambda)y_2)\\
&amp;=&amp;\lambda^2x_1^2+2\lambda x_1(1-\lambda)y_1+(1-\lambda)^2y_1^2-4\lambda x_2-4(1-\lambda)y_2\\
&amp;\le&amp; \lambda^2x_1^2+\lambda (1-\lambda)(x_1^2+y_1^2)+(1-\lambda)^2y_1^2-4\lambda x_2-4(1-\lambda)y_2\\
&amp;=&amp;\lambda f(x)+(1-\lambda)f(y).
\end{array}
\]</span> <strong>Proposition</strong></p>
<p>If <span class="math inline">\(f_1,f_2:D\rightarrow \mathbb R\)</span> are convex function on a convex set <span class="math inline">\(D\subseteq \mathbb R^n\)</span>, then</p>
<ol type="1">
<li><span class="math inline">\(f_1+f_2\)</span> is a convex function on <span class="math inline">\(D\)</span>.</li>
<li><span class="math inline">\(\alpha f_1\)</span> is a convex function on D for <span class="math inline">\(\alpha \ge 0\)</span>.</li>
</ol>
<p>Proof.</p>
<p>Let <span class="math inline">\(g=f_1+f_2\)</span>. For <span class="math inline">\(x,y\in D\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>, we have <span class="math display">\[
\begin{array}{rCl}
g(\lambda x+(1-\lambda)y)&amp;=&amp;f_1(\lambda x+(1-\lambda)y)+f_2(\lambda x+(1-\lambda)y)\\
&amp;\le&amp; \lambda g+(1-\lambda)g.
\end{array}
\]</span> Proof of 2 is straightforward.</p>
<p><strong>Corollary</strong></p>
<p>Let <span class="math inline">\(f_1,f_2,\dotsm,f_k:D\rightarrow \mathbb R\)</span> are convex functions on a convex set <span class="math inline">\(D\subseteq \mathbb R^n\)</span>. Then <span class="math display">\[
f(x)=\sum_{j=1}^k\alpha_jf_j(x),
\]</span> where <span class="math inline">\(\alpha_j\ge 0\)</span>, is also a convex function. If one of the functions is strictly convex, then <span class="math inline">\(f(x)\)</span> is strictly convex.</p>
<p>Proof.</p>
<p>It is straightforward to use the previous proposition.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(h:D\rightarrow \mathbb R\)</span> be convex function and <span class="math inline">\(g:\mathcal X\rightarrow \mathbb R\)</span> be a <strong>nondecreasing convex</strong> function with <span class="math inline">\(h(D)\subset \mathcal X\)</span>. Then the composition function <span class="math inline">\(f=g\circ h:D\rightarrow \mathbb R\)</span> is a convex function.</p>
<p>Proof. <span class="math display">\[
\begin{array}{rCl}
f(\lambda x+(1-\lambda )y)&amp;=&amp;g(h(\lambda x+(1-\lambda)y))\\
&amp;\le&amp; g(\lambda h(x)+(1-\lambda)h(y))\text{ (Since $h$ is convex and $g$ is non-decreasing)}\\
&amp;\le&amp; \lambda g(h(x))+(1-\lambda)g(h(y)) \text{ (Since $g$ is convex)}.
\end{array}
\]</span> <strong>Proposition</strong></p>
<p>Let <span class="math inline">\(h:D\rightarrow \mathbb R\)</span> be convex function and <span class="math inline">\(g:\mathcal X\rightarrow \mathbb R\)</span> be a <strong>nonincreasing concave</strong> function with <span class="math inline">\(h(D)\subset \mathcal X\)</span>. Then the composition function <span class="math inline">\(f=g\circ h:D\rightarrow \mathbb R\)</span> is a concave function.</p>
<p>Proof. Same as the previous proposition.</p>
<p>The next result depicts a useful relationship between convex function and convex set.</p>
<p><strong>Proposition</strong></p>
<p>Suppose <span class="math inline">\(D\subset \mathbb R^n\)</span> is convex. If <span class="math inline">\(f:D\rightarrow \mathbb R\)</span> is convex, then for any <span class="math inline">\(\alpha \in \mathbb R\)</span>, the set <span class="math display">\[
S_\alpha=\{x\in D\;|\;f(x)\le \alpha\}
\]</span> is convex.</p>
<p>Proof.</p>
<p>We choose <span class="math inline">\(x,y\in S_\alpha\)</span>, and choose <span class="math inline">\(\lambda \in [0,1]\)</span>. Then we have <span class="math display">\[
f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda )f(y)\le \lambda \alpha +(1-\lambda )\alpha = \alpha
\]</span> That means the point <span class="math inline">\(\lambda x+(1-\lambda)y\)</span> is also in the set. Q.E.D.</p>
<p><strong>Proposition</strong></p>
<p>Suppose <span class="math inline">\(f:D\rightarrow \mathbb R\)</span> is a function defined on a convex set <span class="math inline">\(D\subset \mathbb R^n\)</span>. The epigraph of <span class="math inline">\(f\)</span> is the following subset of <span class="math inline">\(\mathbb R^{n+1}\)</span>: <span class="math display">\[
E_f=\{[x;\alpha]\;|\; x\in D,\alpha\in \mathbb R,f(x)\le \alpha\}
\]</span> The epigraph <span class="math inline">\(E_f\)</span> is a convex set if and only if <span class="math inline">\(f\)</span> is convex function.</p>
<p>Proof.</p>
<p><span class="math inline">\(\implies\)</span></p>
<p>If the epigraph <span class="math inline">\(E_f\)</span> is a convex set, then we choose <span class="math inline">\(x,y\in E_f\)</span>, and choose <span class="math inline">\(\lambda \in [0,1]\)</span>. We have <span class="math display">\[
\lambda [x;\alpha]+(1-\lambda)[y;\beta]\in E_f
\]</span> for any <span class="math inline">\(\alpha,\beta \in \mathbb R\)</span>.</p>
<p>Then we have <span class="math display">\[
[\lambda x+(1-\lambda )y;\lambda f(x)+(1-\lambda )f(y)]\in E_f
\]</span> Then by definition, we have <span class="math inline">\(f(\lambda x+(1-\lambda )y)\le \lambda f(x)+(1-\lambda )f(y)\)</span>. Q.E.D.</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>If the function <span class="math inline">\(f\)</span> is a convex function, then we choose <span class="math inline">\([x;\alpha],[y;\beta]\in E_f\)</span>, and choose <span class="math inline">\(\lambda \in [0,1]\)</span>. We have <span class="math display">\[
\lambda [x;\alpha]+(1-\lambda )[y;\beta]=[\lambda x+(1-\lambda)y;\lambda \alpha+(1-\lambda)\beta]
\]</span> Then we find <span class="math display">\[
f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y)\le \lambda \alpha+(1-\lambda)\beta
\]</span> That mean <span class="math display">\[
[\lambda x+(1-\lambda)y;\lambda \alpha+(1-\lambda)\beta]\in E_f
\]</span> Q.E.D.</p>
<p><strong>Proposition</strong> (Jensen's inequality)</p>
<p>Suppose <span class="math inline">\(f:S\rightarrow \mathbb R\)</span> is a convex function on a convex set <span class="math inline">\(S\subset \mathbb R^n\)</span>, and <span class="math inline">\(x_1,x_2,\dotsm,x_n\in S\)</span>. Let <span class="math display">\[
x=\sum_{j=1}^k \lambda_j x_j,\quad \sum_{j=1}^k\lambda_j=1,\quad \lambda_j\ge 0
\]</span> Then <span class="math display">\[
f(x)\le \sum_{j=1}^k\lambda_k f(x_k)
\]</span></p>
<p>Proof. Skipped.</p>
<h2 id="differentiable-convex-functions">3.3 Differentiable convex functions</h2>
<p>Testing or checking the convexity of a function by definition is not a easy task. However, when the function is twice differentiable, the task becomes much easier.</p>
<p><strong>Theorem</strong> (Tangent plane characterization)</p>
<p>Suppose that <span class="math inline">\(f(x)\)</span> has continuous first partial derivatives on an open convex set <span class="math inline">\(S\)</span> in <span class="math inline">\(\mathbb R^n\)</span>. Then,</p>
<ol type="1">
<li>the function is convex if and only if</li>
</ol>
<p><span class="math display">\[
f(x)+\nabla f(x)(y-x)\le f(y)\quad \forall x,y\in S
\]</span></p>
<ol start="2" type="1">
<li>the function is strictly convex if and only if</li>
</ol>
<p><span class="math display">\[
f(x)+\nabla f(x)(y-x)&lt; f(y)\quad \forall x,y\in S
\]</span></p>
<p>Proof.</p>
<p><span class="math inline">\(\implies\)</span></p>
<p>Suppose <span class="math inline">\(f:D\rightarrow \mathbb R\)</span> is convex, then we have <span class="math display">\[
f(\lambda y+(1-\lambda )x)\le \lambda f(y)+(1-\lambda )f(x)\quad \forall x,y\in D,\forall \lambda \in [0,1]
\]</span> Then we have <span class="math display">\[
\frac{f(x +\lambda (y-x))-f(x)}{\lambda}\le f(y)-f(x)
\]</span> When <span class="math inline">\(\lambda \rightarrow 0\)</span>, the left hand side is <span class="math inline">\(\nabla f(x) ^T(y-x)\)</span>.</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>If we have <span class="math inline">\(f(x)+\nabla f(x)(y-x)\le f(y)\)</span></p>
<p>Let <span class="math inline">\(u,v\in S\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>, we consider <span class="math inline">\(w=u+(1-\lambda )v\)</span></p>
<p>We have <span class="math display">\[
f(w)+\nabla f(w)(u-w)\le f(u)\\
f(w)+\nabla f(w)(v-w)\le f(v)
\]</span> Since we have <span class="math inline">\(v-w=-\frac{\lambda}{1-\lambda}(u-w)\)</span>, we can cancel <span class="math inline">\(\nabla f(\cdot)\)</span>. Q.E.D.</p>
<p><strong>Remark</strong></p>
<p>The equation <span class="math inline">\(z=f(x)+\nabla f(x)^T(y-x)\)</span> define the tangent plane to the surface <span class="math inline">\(z=f(x)\)</span> at <span class="math inline">\(y=x\)</span>. Thus the theorem shows that the tangent plane is below the convex function.</p>
<p><strong>Proposition</strong> (Monotone gradient condition)</p>
<p>Let <span class="math inline">\(f:S\rightarrow \mathbb R\)</span> be differentiable function on the open convex subset <span class="math inline">\(S\)</span> of <span class="math inline">\(\mathbb R^n\)</span>. Then <span class="math inline">\(f\)</span> is convex on <span class="math inline">\(S\)</span> if and only if <span class="math display">\[
\left\langle \nabla f(x)-\nabla f(y),x-y\right\rangle\ge 0\quad \forall x,y\in S
\]</span> Proof.</p>
<p><span class="math inline">\(\implies\)</span> <span class="math display">\[
f(x)+\nabla f(x)^T(y-x)\le f(y)\\
f(y)-\nabla f(y)^T(y-x)\le f(x)
\]</span> By doing the summation, Q.E.D.</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>Suppose we have <span class="math inline">\(\left\langle \nabla f(x)-\nabla f(y),x-y\right\rangle\ge 0\quad \forall x,y\in S\)</span>. Define <span class="math inline">\(g(t)=f(x+t(y-x))\)</span>. Then we have <span class="math display">\[
g&#39;(t)=&lt;\nabla f(x+t(y-x)),y-x&gt;
\]</span> Then we have <span class="math display">\[
g&#39;(t)-g&#39;(0)=&lt;\nabla f(x+t(y-x))-\nabla f(x),t(y-x)&gt;\ge 0\quad \forall t\in[0,1]
\]</span> Then we have <span class="math display">\[
g(1)=f(y)=g(0)+\int _0^1 g&#39;(t)dt\ge g(0)+g&#39;(0)=f(x)+&lt;\nabla f(x),y-x&gt;
\]</span> Q.E.D.</p>
<p><strong>Theorem</strong> (Test for convexity of a differentiable function)</p>
<p>Suppose that <span class="math inline">\(f(x)\)</span> has continuous second partial derivatives on an open convex set <span class="math inline">\(D\subset \mathbb R^n\)</span>, the function is convex on <span class="math inline">\(D\)</span> if and only if the Hessian matrix is positive semidefinite.</p>
<h2 id="strongly-convex-functions">3.4 Strongly convex functions</h2>
<p><strong>Definition</strong></p>
<p>Let <span class="math inline">\(D\subseteq \mathbb R^n\)</span> be a convex set. A function <span class="math inline">\(f:D\rightarrow \mathbb R\)</span> is said to be strongly convex if there exists a constant <span class="math inline">\(c&gt;0\)</span> such that for all <span class="math inline">\(x,y\in D\)</span>, and <span class="math inline">\(\lambda \in[0,1]\)</span>, <span class="math display">\[
f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y)-\frac{c}{2}\lambda(1-\lambda)||x-y||^2
\]</span> <strong>Theorem</strong></p>
<p>Suppose <span class="math inline">\(f:D\rightarrow \mathbb R\)</span> is a differentiable function. Then the following statements are equivalent.</p>
<ol type="1">
<li><p><span class="math inline">\(f(y)\ge f(x)+\langle \nabla f(x),y-x\rangle+(c/2)||y-x||^2\)</span></p></li>
<li><span class="math inline">\(g(x)=f(x)-(c/2)||x||^2\)</span> is convex</li>
<li><p><span class="math inline">\(\langle\nabla f(x)-\nabla f(y),x-y\rangle\ge c||x-y||^2\)</span></p></li>
<li><p><span class="math inline">\(f\)</span> is strongly convex.</p></li>
</ol>
<p>Proof. (<span class="math inline">\(1\iff 2\)</span>)</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>If <span class="math inline">\(g(x)=f(x)-(c/2)||x||^2\)</span> is convex, then we have <span class="math display">\[
g(y)\ge g(x)+\nabla g(x)(y-x)\\
\implies f(y)-(c/2)||y||^2\ge f(x)-(c/2)||x||^2+\nabla f(x)^T(y-x)-cx^T(y-x)\\
\implies f(y)\ge f(x)+\nabla f(x)^T(y-x)+(c/2)\left(||y||^2-2x^Ty+||x||^2\right)
\]</span> <span class="math inline">\(\implies\)</span></p>
<p>By Tangent plane characterization in Section 3.3.</p>
<p>(<span class="math inline">\(2\iff 3\)</span>) Monotone gradient.</p>
<p>(<span class="math inline">\(1\iff 4\)</span>) Straightforward. Q.E.D.</p>
<p>The following conditions are implied by the strong convexity of <span class="math inline">\(f\)</span>. Let <span class="math inline">\(x^\star\)</span> be a minimizer of <span class="math inline">\(f\)</span> over <span class="math inline">\(D\)</span>.</p>
<ol type="1">
<li><span class="math inline">\((1/2c)||\nabla f(x)||^2\ge f(x)-f(x^\star)\)</span>.</li>
<li><span class="math inline">\(||\nabla f(x)-\nabla f(y)||\ge c||x-y||\)</span>.</li>
<li><span class="math inline">\(f(y)\le f(x)+\langle \nabla f(x),y-x\rangle +(1/2c)||\nabla f(y)-\nabla f(x)||^2\)</span>.</li>
<li><span class="math inline">\(\langle \nabla f(x)-\nabla f(y),x-y\rangle \le (1/c)||\nabla f(x)-\nabla f(y)||^2\)</span>.</li>
</ol>
<p>Proof.</p>
<ol type="1">
<li><p>Since we have <span class="math inline">\(f(y)\ge f(x)+&lt;\nabla f(x),y-x&gt;+(c/2)||y-x||^2\)</span>, for any fixed point <span class="math inline">\(x\)</span>, we can derive the minimal value of the right hand side. <span class="math inline">\(f(x)+&lt;\nabla f(x),y-x&gt;+(c/2)||y-x||^2\ge f(x)-(1/2c)||\nabla (f)||^2\)</span>. Then we have <span class="math display">\[
f(y)\ge f(x)-(1/2c)||\nabla f(x)||^2 \quad \forall x,y \in \mathbb R^n.
\]</span> Then we let <span class="math inline">\(y=x^\star\)</span>. Q.E.D.</p>
<p>By this condition, we can see that if the gradient is getting smaller, that means the current value of the cost function is getting to the optimal value.</p></li>
<li><p>From the condition 3 in the Theorem 3.3</p></li>
<li><p>We define the function <span class="math inline">\(\phi_x(z)=f(z)-\langle \nabla f(x),z\rangle\)</span>. We can find the analytical minimizer of this function as <span class="math display">\[
\nabla \phi_x(z)=\nabla f(z)-\nabla f(x)=0\implies z^\star =x.
\]</span> Then we use the condition 1, <span class="math display">\[
(1/2c)||\nabla f(y)-\nabla f(x)||^2\ge f(y)-\langle \nabla f(x),x-y\rangle-f(x).
\]</span></p></li>
<li><p>By doing summary of the following two inequalities, <span class="math display">\[
f(y)\le f(x)+\langle \nabla f(x),y-x\rangle +(1/2c)||\nabla f(y)-\nabla f(x)||^2\\
f(x)\le f(y)+\langle \nabla f(y),y-x\rangle +(1/2c)||\nabla f(x)-\nabla f(y)||^2.Q.E.D.
\]</span></p></li>
</ol>
<p>Q.E.D.</p>
<h2 id="unconstrained-convex-programming-problem">3.5 Unconstrained convex programming problem</h2>
<p>A convex programming problem is a minimization problem whose objective function is convex and the feasible set is convex. In general, a local minimizer is not a global minimizer. However, for a convex minimization problem, a local minimizer is also a global minimizer.</p>
<p>An <strong>unconstrained convex nonlinear programming problem</strong> is the following: <span class="math display">\[
\text{minimize }f(x)\\x\in D.
\]</span> <strong>Theorem</strong></p>
<p>Let <span class="math inline">\(D\)</span> be a nonempty open convex subset of <span class="math inline">\(\mathbb R^n\)</span>, and <span class="math inline">\(f:D\rightarrow \mathbb R\)</span> is a convex function. Consider the convex minimization problem. Suppose <span class="math inline">\(x^\star \in D\)</span> is a local minimizer to the problem, then</p>
<ol type="1">
<li><span class="math inline">\(x^\star\)</span> is a global minimizer</li>
<li>If <span class="math inline">\(f\)</span> is strictly convex, then <span class="math inline">\(x^\star\)</span> is the unique global minimizer.</li>
</ol>
<p>Proof.</p>
<ol type="1">
<li><p>We assume <span class="math inline">\(x^\star\in D\)</span> is a local minimizer, which means there exists a ball <span class="math inline">\(B_\epsilon(x^\star)\)</span>, such that for all points <span class="math inline">\(x\in B_\epsilon(x^\star)\)</span>, <span class="math inline">\(f(x)\ge f(x^\star)\)</span>. Then we need to prove <span class="math inline">\(x^\star\)</span> is a global minimizer, which means for all points <span class="math inline">\(y\in D\)</span>, we have <span class="math inline">\(f(y)\ge f(x^\star)\)</span>.</p>
<p>We consider the point <span class="math inline">\(\lambda y+(1-\lambda )x^\star \in D\)</span>. If $$ is enough small, then the point is also in the ball. That means we have <span class="math display">\[
\lambda f(y)+(1-\lambda)f(x^\star)\ge f(\lambda y+(1-\lambda)x^\star)\ge f(x^\star).
\]</span> Q.E.D.</p></li>
<li><p>Assume <span class="math inline">\(x^\star \in D\)</span> is the global optimizer. If there is another global minimizer <span class="math inline">\(\bar x\)</span>, for which we have <span class="math inline">\(f(\bar x)=f(x^\star)\)</span>.</p>
<p>Then we choose <span class="math inline">\(w=(1/2)x^\star+(1/2)\bar x\)</span>. <span class="math display">\[
f(w)=f((1/2)x^\star+(1/2)\bar x)&lt; (1/2)f(x^\star)+(1/2)f(\bar x)=f(x^\star),
\]</span> which contradicts that the point <span class="math inline">\(x^\star\)</span> is the global optimal point. Q.E.D.</p></li>
</ol>
<p><strong>Corollary</strong></p>
<p>If <span class="math inline">\(f\)</span> is a convex function with continuous first partial derivatives on some open convex set <span class="math inline">\(D\)</span>, then any stationary point of <span class="math inline">\(f\)</span> is a global minimizer of <span class="math inline">\(f\)</span>.</p>
<h2 id="unconstrained-convex-quadratic-programming">3.6 Unconstrained convex quadratic programming</h2>
<p><strong>Definition</strong> (Quadratic function)</p>
<p>A quadratic function <span class="math inline">\(q:\mathbb R^n\rightarrow \mathbb R\)</span> is defined by <span class="math display">\[
q(x)=\frac{1}{2}x^TQx+c^Tx
\]</span> The quadratic function is a convex function if <span class="math inline">\(Q\)</span> is positive semidefinite.</p>
<p>The point <span class="math inline">\(x^\star \in S\)</span> is a global minimizer of the problem if and only if <span class="math inline">\(Qx^\star =-c\)</span>.</p>
<h2 id="projection-onto-a-closed-convex-set">3.7 Projection onto a closed convex set</h2>
<p><strong>Theorem</strong></p>
<p>Let <span class="math inline">\(f:C\rightarrow \mathbb R\)</span> be a convex and continuously differentiable function on the convex set <span class="math inline">\(C\subset \mathcal E\)</span>. Consider the constrained minimization problem, <span class="math display">\[
\min\{f(x)\;|\;x\in C\}.
\]</span> Then <span class="math inline">\(x^\star\in C\)</span> is a global minimizer if and only if <span class="math display">\[
\langle \nabla f(x^\star),x-x^\star\rangle\ge 0,\;\forall x\in C.
\]</span> Proof.</p>
<p><span class="math inline">\(\Longleftarrow\)</span></p>
<p>Suppose <span class="math inline">\(\langle \nabla f(x^\star),x-x^\star\rangle\ge 0,\;\forall x\in C\)</span>.Since <span class="math inline">\(f\)</span> is convex, we have <span class="math display">\[
f(y)\ge f(x)+\nabla f(x)^T(y-x)\;\forall x,y\in C.
\]</span> Then <span class="math display">\[
f(y)\ge f(x^\star)+\nabla f(x^\star)^T(y-x^\star)\ge f(x^\star)\;\forall y\in C.
\]</span> That means <span class="math inline">\(x^\star\)</span> is the global minimizer.</p>
<p><span class="math inline">\(\implies\)</span></p>
<p>We will approve the sufficient condition by contradiction.</p>
<p>Suppose <span class="math inline">\(x^\star\)</span> is the global minimizer and suppose that there exists <span class="math inline">\(\bar x\in C\)</span> such that <span class="math inline">\(\langle \nabla f(x^\star),\bar x-x^\star\rangle&lt; 0\)</span>.</p>
<p>Consider the point <span class="math inline">\(w=t\bar x+(1-t)x^\star\)</span>. Note that <span class="math inline">\(f(w)-f(x^\star)\le \nabla f(w)^T(w-x^\star)\)</span>.Then <span class="math display">\[
0\le f(w)-f(x^\star)\le \nabla f(w)^T(w-x^\star)\\
=\langle \nabla f(t\bar x+(1-t)x^\star),(t\bar x+(1-t)x^\star-x^\star)\rangle\\
=\langle \nabla f(t\bar x+(1-t)x^\star),t(\bar x-x^\star)\rangle\\
=\langle \nabla f(t(\bar x-x^\star)+x^\star),t(\bar x-x^\star)\rangle.
\]</span> Let <span class="math inline">\(t\rightarrow 0\)</span>, we have <span class="math inline">\(\langle \nabla f(x^\star),\bar x-x^\star\rangle = 0\)</span>. Q.E.D.</p>
<p><strong>Remark</strong></p>
<p>Using the concept of the <strong>normal cone</strong>, we can rephrase the theorem as follows,</p>
<p><span class="math inline">\(x^\star\)</span> is a global minimizer of the problem <span class="math inline">\(\min\{f(x)\;|\;x\in C\}\)</span> if and only if <span class="math inline">\(-\nabla f(x^\star)\in N_C(x^\star)\)</span>.</p>
<p><strong>Theorem</strong> (Projection theorem)</p>
<p>Let <span class="math inline">\(C\)</span> be a closed convex set in <span class="math inline">\(\mathcal E\)</span>,</p>
<ol type="1">
<li><p>For every <span class="math inline">\(z\in \mathcal E\)</span>, there exists a unique minimizer (denoted as <span class="math inline">\(\Pi_C(z)\)</span> and called as the <strong>projection</strong> of <span class="math inline">\(z\)</span> onto <span class="math inline">\(C\)</span>) of <span class="math display">\[
\min\{\frac{1}{2}||x-z||^2\;|\; x\in C\},
\]</span> where <span class="math inline">\(||\cdot||\)</span> is the Euclidean norm.</p></li>
<li><p><span class="math inline">\(x^\star=\Pi_C(z)\)</span> is the projection of <span class="math inline">\(z\)</span> onto <span class="math inline">\(C\)</span> if and only if <span class="math display">\[
\langle z-x^\star,x-x^\star\rangle\le 0\quad\forall x\in C.
\]</span></p></li>
<li><p>(Firmly non-expansive property) For any <span class="math inline">\(z,w\in \mathcal E\)</span>, <span class="math display">\[
||\Pi_C(z)-\Pi_C(w)||^2\le \langle z-w,\Pi_C(z)-\Pi_C(w)\rangle.
\]</span> Hence, <span class="math inline">\(||\Pi_C(z)-\Pi_C(w)||\le ||z-w||\)</span>. That is, <span class="math inline">\(\Pi_c(\cdot)\)</span> is <strong>Lipschitz continuous</strong> with modulus 1.</p></li>
</ol>
<p>Proof.</p>
<ol type="1">
<li><p>Choose <span class="math inline">\(\bar x\in C\)</span>. Define the set <span class="math inline">\(S=\{x\in C\;|\;f(x)\le f(\bar x)\}\)</span>. It can be proved that the set <span class="math inline">\(S\)</span> is closed and bounded. By Weierstrass Theorem, there exists <span class="math inline">\(x^\star \in S\)</span> such that <span class="math inline">\(f(x^\star)\le f(x)\quad\forall x\in S\)</span>. And we also have <span class="math display">\[
f(x)&gt;f(\bar x)\ge f(x^\star)\quad\forall x\in C\backslash S.
\]</span> Thus we have shown for all <span class="math inline">\(x\in C\backslash S\cup S=C\)</span>, <span class="math inline">\(f(x^\star )\le f(x)\)</span>.</p>
<p>Since the function <span class="math inline">\(f(x)=\frac{1}{2}||x-z||^2\)</span> is strictly convex on <span class="math inline">\(C\)</span>, the minimizer is unique.</p></li>
<li><p>By monotone of the gradient, we have <span class="math display">\[
\langle \nabla f(x^\star),x-x^\star\rangle\ge 0
\]</span> Q.E.D.</p></li>
<li>From condition 2, we have <span class="math inline">\(w,z\in \mathcal E\)</span>. Since <span class="math inline">\(\Pi_C(w)\in \mathcal E\)</span> and <span class="math inline">\(\Pi_C(z)\in \mathcal E\)</span>. Then we have <span class="math display">\[
\langle z-\Pi_C(z),\Pi_C(w)-\Pi_C(z)\rangle\le  0\quad (1)\\
\langle w-\Pi_C(w),\Pi_C(z)-\Pi_C(w)\rangle\le  0\\
\langle\Pi_C(w)-w,\Pi_C(w)-\Pi_C(z)\rangle\le  0\quad (2).
\]</span>
<ol type="1">
<li><ul>
<li><ol start="2" type="1">
<li><span class="math display">\[
\langle z-w-\Pi_C(z)+\Pi_C(w),\Pi_C(w)-\Pi_C(z)\rangle\le 0
\]</span></li>
</ol></li>
</ul></li>
</ol></li>
</ol>
<p>Q.E.D.</p>
<p><strong>Remark</strong></p>
<p>If <span class="math inline">\(C\)</span> is a linear subspace of <span class="math inline">\(\mathbb R^n\)</span>, then <span class="math inline">\(z-x^\star \perp C\)</span>. Thus <span class="math inline">\(z\)</span> can be decomposed into two perpendicular components, <span class="math display">\[
z=\Pi_C(z)+(z-\Pi_C(z)),\quad where\quad &lt;\Pi_C(z),z-\Pi_C(z)&gt;=0
\]</span> <strong>Example</strong></p>
<p>Let <span class="math inline">\(C=\mathbb R_+^n\)</span>. Then for any <span class="math inline">\(z\in \mathbb R^n\)</span>, <span class="math inline">\(\Pi_C(z)=\max\{z,0\}\)</span>.</p>
<p>Proof. <span class="math display">\[
\min\{\frac{1}{2}||x-z||^2\;|\; x\in \mathbb R^n_+\}
\]</span> Since we have <span class="math display">\[
||x-z||^2=(x^2_1-2z_1x_1+z^2_1)+\dotsm+(x^2_n-2z_nx_1+z^2_n)
\]</span> Every part should be minimized.</p>
<p><strong>Example</strong></p>
<p><strong>Example</strong></p>
<p>Let <span class="math inline">\(C=\mathbb S_+^n\)</span> is the positive semidefinite matrices space. For a given <span class="math inline">\(A\in\mathbb S_n\)</span>, let the eigenvalue decomposition of <span class="math inline">\(A\)</span> be <span class="math inline">\(A=U\text{diag}(d)U^T\)</span>, then we have <span class="math inline">\(\Pi_C(A)=U(\text{diag}(\max\{d,0\}))U^T\)</span>.</p>
<p>Not that <span class="math inline">\(U\)</span> is orthogonal. Then we have <span class="math display">\[
\min\{\frac{1}{2}||X-A||^2_F\;|\; X\in\mathbb  S^n_+\}\\
=\min\{\frac{1}{2}||U(U^TXU-\text{diag}(d))U^T||^2_F\;|\; X\in\mathbb  S^n_+\}\\
=\min\{\frac{1}{2}||U^TXU-\text{diag}(d)||^2_F\;|\; X\in\mathbb  S^n_+\}\text{ (Trace Property)}\\
=\min\{\frac{1}{2}||\bar X-\text{diag}(d)||^2_F\;|\; \bar X\in\mathbb  S^n_+\}\\
=\min\{\frac{1}{2}||\text{diag}(x)-\text{diag}(d)||^2_F\;|\; x\ge0\}\\
=\min\{\frac{1}{2}||x-d||^2\;|\; x\ge0\}
\]</span> Then we have <span class="math inline">\(x^\star =\max\{0,d\}\)</span>.</p>
<p><strong>Definition</strong> (Fréchet differentiable)</p>
<p>Let <span class="math inline">\(\mathcal E_1,\mathcal E_2\)</span> be two differentiable finite-dimensional real Euclidean spaces. A function <span class="math inline">\(f:\mathcal E_1\rightarrow \mathcal E_2\)</span> is said to be Fréchet differentiable at <span class="math inline">\(\mathcal x\in E_1\)</span> if there exists a linear map <span class="math inline">\(f&#39;(x):\mathcal E_1\rightarrow \mathcal E_2\)</span> such that for any <span class="math inline">\(h\rightarrow 0\)</span>, <span class="math display">\[
f(x+h)-f(x)-f&#39;(x)[h]=o(||h||).
\]</span> This can be explained by <span class="math display">\[
\lim_{n\rightarrow 0}\frac{f(x+h)-f(x)}{h}=f&#39;(x)\\ \iff\lim_{n\rightarrow 0}\frac{f(x+h)-f(x)-f&#39;(x)[h]}{h}=0\\ \iff f(x+h)-f(x)-f&#39;(x)[h]=o(||h||).
\]</span> <strong>Proposition</strong></p>
<p>Let <span class="math inline">\(C\)</span> be a nonempty closed convex set in <span class="math inline">\(\mathcal E\)</span>. For any <span class="math inline">\(x\)</span>, let <span class="math display">\[
\theta(x)=\frac{1}{2}||x-\Pi_C(x)||^2.
\]</span> The <span class="math inline">\(\theta(\cdot)\)</span> is a continuously differentiable convex function and <span class="math display">\[
\nabla \theta(x)=x-\Pi_C(x).
\]</span></p>
<h2 id="convex-separation">3.8 Convex Separation</h2>
<p>In this section, some propositions will be given without proof.</p>
<p><strong>Definition</strong></p>
<ol type="1">
<li>The <strong>affine hull</strong> <span class="math inline">\(\text{aff(S)}\)</span> of a set S in <span class="math inline">\(\mathcal E\)</span> is the smallest affine set containing <span class="math inline">\(S\)</span>, i.e. in intersection of all affine set containing <span class="math inline">\(S\)</span>. That can be shown by</li>
</ol>
<p><span class="math display">\[
\text{aff(S)}=\{\sum_{i=1}^k\alpha_ix_i\;|\; \sum_{i=1}^k \alpha_i=1,x_i\in S,k&gt;0\}.
\]</span></p>
<ol start="2" type="1">
<li>The <strong>convex hull</strong> <span class="math inline">\(\text{conv(S)}\)</span> or <span class="math inline">\(\text{co(S)}\)</span> of a set <span class="math inline">\(S\)</span> in <span class="math inline">\(\mathcal E\)</span> is the smallest convex set containing <span class="math inline">\(S\)</span>, i.e. the intersection of all convex set containing <span class="math inline">\(S\)</span>.</li>
</ol>
<p><span class="math display">\[
\text{conv(S)}=\{\sum_{i=1}^k\alpha_ix_i\;|\; \sum_{i=1}^k \alpha_i=1,x_i\in S,\alpha_i\ge 0,k&gt;0\}
\]</span></p>
<ol start="3" type="1">
<li>A point <span class="math inline">\(x\)</span> is a <strong>relative interior point</strong> of <span class="math inline">\(S\)</span> if <span class="math inline">\(x\)</span> is an interior point relative to <span class="math inline">\(\text{aff(S)}\)</span>. The set of all relative interior points of <span class="math inline">\(S\)</span> is denoted as <span class="math inline">\(\text{relint(S)}\)</span> or <span class="math inline">\(\text{ri(S)}\)</span></li>
</ol>
<p><span class="math display">\[
\text{ri(S)}=\{x\in \text{aff(S)}\;|\; \exist \varepsilon &gt;0, B_{\varepsilon}(x)\cap \text{aff(S)}\subset S\}.
\]</span></p>
<p><strong>Examples</strong></p>
<ol type="1">
<li>The affine hull of the singleton (a set made of one single element) is the singleton itself.</li>
<li>The affine hull of a set of two different points is the line through them.</li>
<li>The affine hull of a set of three points not on one line is the plane going through them.</li>
<li>The affine hull of a set of four points not in a plane in <span class="math inline">\(\mathbb R^3\)</span> is the entire space <span class="math inline">\(\mathbb R^3\)</span>.</li>
<li><span class="math inline">\(\text{aff}(\mathbb R_+^n)=\mathbb R^n\)</span>.</li>
</ol>
<p><strong>Proposition</strong> Let <span class="math inline">\(C\)</span> be a subset of a Euclidean space <span class="math inline">\(\mathcal E\)</span>. If <span class="math inline">\(C\)</span> is convex, then <span class="math inline">\(\text{aff(ri C)}=\text{aff(C)}=\text{aff(cl(C))}\)</span></p>
<p>In the next proposition we will prove that any point outside a closed convex set can be separated from that set with a hyperplane.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(\Omega \in \mathbb R^n\)</span> be a nonempty closed convex set and let <span class="math inline">\(\bar x\notin \Omega\)</span>. Then there exists a nonzero vector <span class="math inline">\(v\in \mathbb R^n\)</span> such that <span class="math display">\[
\sup\{\langle v,x\rangle \;|\;x\in \Omega\}&lt;\langle v,\bar x\rangle
\]</span> Denote <span class="math inline">\(\bar w=\Pi_C(\bar x)\)</span>. let <span class="math inline">\(v=\bar x-\bar w\)</span>. For any <span class="math inline">\(x\in \Omega\)</span> we know that <span class="math display">\[
&lt;v,x-\bar w&gt;=&lt;\bar x-\bar w,x-\bar w&gt;\le 0\\
&lt;v,x-\bar x+\bar x-\bar w&gt;\\
&lt;v,x-\bar x+v&gt;=\\
&lt;v,x&gt;-&lt;v,\bar x&gt;+||v||^2\le 0
\]</span> Q.E.D.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(\Omega_1\)</span> and <span class="math inline">\(\Omega_2\)</span> be nonempty, closed, convex subsets of <span class="math inline">\(\mathbb R^n\)</span> with <span class="math inline">\(\Omega_1\cap \Omega_2=\empty\)</span>. If <span class="math inline">\(\Omega_1\)</span> or <span class="math inline">\(\Omega_2\)</span> is bounded, then there is a nonzero element <span class="math inline">\(v\in \mathbb R^n\)</span> such that <span class="math display">\[
\sup\{&lt;v,x&gt;\;|\;x\in \Omega_1\}&lt;\inf\{&lt;v,y&gt;\;|\;y\in \Omega_2\}.
\]</span> Proof.</p>
<p>Define <span class="math inline">\(\Omega=\Omega_1-\Omega_2\)</span>. Then <span class="math inline">\(\Omega\)</span> is nonempty closed convex set and <span class="math inline">\(0\notin \Omega\)</span>. Then we choose the point <span class="math inline">\(0\)</span> and use the previous proposition. Q.E.D.</p>
<p><strong>Proposition</strong> (Separation in a subspace)</p>
<p>Let <span class="math inline">\(L\)</span> be a subspace of <span class="math inline">\(\mathbb R^n\)</span> and let <span class="math inline">\(\Omega \subset L\)</span> be a nonempty convex set with <span class="math inline">\(\bar x\in L\)</span> and <span class="math inline">\(\bar x\notin \bar \Omega\)</span> (<span class="math inline">\(\bar \Omega\)</span> is the closure of <span class="math inline">\(\Omega\)</span>). Then there exists <span class="math inline">\(w\in L,w\neq 0\)</span> such that <span class="math display">\[
\sup\{\langle w,x\rangle\;|\; x\in \Omega\}&lt;\langle w,\bar x\rangle.
\]</span> <strong>Definition</strong></p>
<p>We say that two empty convex set <span class="math inline">\(\Omega_1\)</span> and <span class="math inline">\(\Omega_2\)</span> can be properly separated if there exists a nonzero vector <span class="math inline">\(v\in R^n\)</span> such that <span class="math display">\[
\sup\{&lt;v,x&gt;\;|\; x\in\Omega_1\}\le \inf\{&lt;v,y&gt;\;|\; y\in\Omega_2\}\\
\inf\{&lt;v,x&gt;\;|\; x\in\Omega_1\}&lt; \sup\{&lt;v,y&gt;\;|\; y\in\Omega_2\}
\]</span> <strong>Proposition</strong></p>
<p>The following assertions hold for affine.</p>
<ol type="1">
<li>A set <span class="math inline">\(\Omega\subset \mathbb R\)</span> is affine if and only if <span class="math inline">\(\Omega\)</span> contains all affine combinations of its elements.</li>
<li>If <span class="math inline">\(\Omega_1\)</span> is an affine subset of <span class="math inline">\(\mathbb R^n\)</span> and <span class="math inline">\(\Omega_2\)</span> is an affine subsets of <span class="math inline">\(\mathbb R^m\)</span>, then <span class="math inline">\(\Omega_1\times \Omega_2\)</span> is an affine subset of <span class="math inline">\(\mathbb R^n\times \mathbb R^m\)</span>.</li>
<li>Let <span class="math inline">\(B:\mathbb R^n\rightarrow \mathbb R^m\)</span> be an affine mapping if <span class="math inline">\(\Omega\)</span> is an affine subset of <span class="math inline">\(\mathbb R^n\)</span> and <span class="math inline">\(\Theta\)</span> is an affine subset of <span class="math inline">\(\mathbb R^m\)</span>. Then the image <span class="math inline">\(B(\Omega)\)</span> is an affine subset of <span class="math inline">\(\mathbb R^m\)</span> and the inverse image <span class="math inline">\(B^{-1}(\Theta)\)</span> is an affine subset of <span class="math inline">\(\mathbb R^n\)</span>.</li>
<li>Let <span class="math inline">\(\Omega,\Omega_1,\Omega_2\)</span> are affine subsets of <span class="math inline">\(\mathbb R^n\)</span>. Then the sum <span class="math inline">\(\Omega_1+\Omega_2\)</span> and the scalar product <span class="math inline">\(\lambda \Omega\)</span> for any <span class="math inline">\(\lambda \in \mathbb R\)</span> are also affine subsets of <span class="math inline">\(\mathbb R^n\)</span>.</li>
<li>Given <span class="math inline">\(\Omega\subset \mathbb R^n\)</span>, its affine hull is the smallest affine set containing <span class="math inline">\(\Omega\)</span>.</li>
<li>A set <span class="math inline">\(\Omega\)</span> is a linear subspace of <span class="math inline">\(\mathbb R^n\)</span> if and only if <span class="math inline">\(\Omega\)</span> is an affine set containing the origin.</li>
</ol>
<p><strong>Definition</strong> (Affine independence)</p>
<p>The elements <span class="math inline">\(v_0,\dotsm,v_m\)</span> in <span class="math inline">\(\mathbb R^m\)</span>, <span class="math inline">\(m\ge 1\)</span> are affinely independent if <span class="math display">\[
\sum_{i=1}^m \lambda_i v_i=0,\quad \sum_{i=1}^m \lambda_i=0\implies \lambda_i=0\quad \forall i=0,\dotsm,m.
\]</span> Roughly speaking, affine independence is like linear independence but without the restriction that the subset of lower dimension the points lie in contains the origin. So three points in space are affinely independent if the smallest flat thing containing them is a plane. They're affinely dependent if they lie on a line (or are the same point).</p>
<p><strong>Definition</strong> (Simplex)</p>
<p>Let <span class="math inline">\(v_0,\dotsm,v_m\)</span> be affinely independent in <span class="math inline">\(\mathbb R^n\)</span>. Then the set <span class="math display">\[
\Delta_m:\text{conv}\{v_i\;|\; i=0,\dotsm,m\}
\]</span> is called an m-simplex in <span class="math inline">\(\mathbb R^n\)</span> with the vertices <span class="math inline">\(v_0,\dotsm,v_m\)</span>.</p>
<p><strong>Lemma</strong></p>
<p>Let <span class="math inline">\(\Omega\)</span> be a nonempty, convex set in <span class="math inline">\(\mathbb R^n\)</span> of dimension <span class="math inline">\(m\ge 1\)</span>. Then there exists <span class="math inline">\(m+1\)</span> affinely independent elements <span class="math inline">\(v_0,\dotsm,v_m\)</span> in <span class="math inline">\(\Omega\)</span>.</p>
<p><strong>Theorem</strong></p>
<p>Let <span class="math inline">\(\Omega \subset \mathbb R^n\)</span> be a nonempty, convex set. The following assertions hold</p>
<ol type="1">
<li>We always have <span class="math inline">\(\text{ri } (\Omega)\neq \empty\)</span></li>
<li>We have <span class="math inline">\([a,b)\subset \text{ri }(\Omega)\)</span> for any <span class="math inline">\(a\in \text{ri }(\Omega)\)</span> and <span class="math inline">\(b\in\bar \Omega\)</span>.</li>
</ol>
<p><strong>Theorem</strong></p>
<p>Let <span class="math inline">\(\Omega_1, \Omega_2\)</span> be nonempty convex subsets of <span class="math inline">\(\mathbb R^n\)</span>. Then we have <span class="math display">\[
\text{ri }(\Omega_1-\Omega_2)=\text{ri }(\Omega_1)-\text{ri }(\Omega_2).
\]</span> <strong>Lemma</strong></p>
<p>Let <span class="math inline">\(\Omega\)</span> be a nonempty convex subset of <span class="math inline">\(\mathbb R^n\)</span>. Suppose that <span class="math inline">\(0\in \bar \Omega\backslash \text{ri }(\Omega)\)</span>. Then <span class="math inline">\(\text{aff }(\Omega)\)</span> is a subspace of <span class="math inline">\(\mathbb R^n\)</span>.</p>
<p><strong>Lemma</strong></p>
<p>A nonempty subset <span class="math inline">\(\Omega\)</span> of <span class="math inline">\(\mathbb R^n\)</span> is affine if and only if <span class="math inline">\(\Omega-\omega\)</span> is a subspace of <span class="math inline">\(\mathbb R^n\)</span> for any <span class="math inline">\(\omega\in \Omega\)</span>.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(\Omega\)</span> be a nonempty convex set. Suppose that <span class="math inline">\(\bar x\in \text{ri }(\Omega)\)</span> and <span class="math inline">\(\bar y\in \Omega\)</span>. Then there exists <span class="math inline">\(t&gt;0\)</span> such that <span class="math display">\[
\bar x+t(\bar x-\bar y)\in \Omega
\]</span> <strong>Proposition</strong></p>
<p>Let <span class="math inline">\(\Omega\)</span> be a nonempty convex set in <span class="math inline">\(\mathbb R^n\)</span>. Then <span class="math inline">\(0\notin \text{ri }(\Omega)\)</span> if and only if the sets <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(\{0\}\)</span> can be properly separated, i.e. there exists <span class="math inline">\(v\in \mathbb R^n\)</span> such that <span class="math display">\[
\sup\{\langle v,x\rangle\;|\;x\in \Omega\}\le 0\\
\inf\{\langle v,x\rangle\;|\;x\in \Omega\}&lt;0
\]</span> <strong>Theorem</strong></p>
<p>Let <span class="math inline">\(\Omega_1\)</span> and <span class="math inline">\(\Omega_2\)</span> be two nonempty convex subsets of <span class="math inline">\(\mathbb R^n\)</span>. Then <span class="math inline">\(\Omega_1\)</span> and <span class="math inline">\(\Omega_2\)</span> can be properly separated if and only if <span class="math inline">\(\text{ri }(\Omega_1)\cap \text{ri }(\Omega_2)=\empty\)</span></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/24/Convex Optimization/11. Interior-Point Methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/07/24/Convex Optimization/11. Interior-Point Methods/" class="post-title-link" itemprop="url">11. Interior-point methods</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-07-24 21:58:01" itemprop="dateCreated datePublished" datetime="2019-07-24T21:58:01+08:00">2019-07-24</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-08-25 23:23:28" itemprop="dateModified" datetime="2019-08-25T23:23:28+08:00">2019-08-25</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Convex-Optimization/" itemprop="url" rel="index"><span itemprop="name">Convex Optimization</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="interior-point-methods">11. Interior-point methods</h1>
<h2 id="inequality-constrained-minimization-problems">11.1 Inequality constrained minimization problems</h2>
<p>In this section, we discuss interior-point methods for solving convex optimization problems that include inequality constraints. <span class="math display">\[
\coo[f_0(x)]{f_i(x)\le 0\\Ax=b}
\]</span> We also assume the problem is strictly feasible, there exists dual optimal <span class="math inline">\(\lambda^\star\)</span> and <span class="math inline">\(v^\star\)</span> together with <span class="math inline">\(x^\star\)</span> satisfying the KKT conditions <span class="math display">\[
\begin{array}{rcl}
Ax^\star =b\quad f_i(x)&amp;\le&amp; 0\\
\lambda^\star &amp;\succeq&amp; 0\\
\nabla f_0(x^\star)+\sum_{i=1}^{m} \lambda_i^\star\nabla f_i(x^\star)+A^Tv^\star&amp;=&amp;0\\
\lambda_i^\star f_i(x^\star)&amp;=&amp;0
\end{array}
\]</span></p>
<p>Interior-point method solve the problem by applying newton’s method to a sequence of equality constrained problem. We will concentrate on a particular interior-point method, called the barrier method, for which we give a proof of convergence and a complexity analysis. We also describe the primal-dual interior point method, but do note give analysis.</p>
<p>We can view interior-point methods as another level in the hierarchy of convex optimization algorithms.</p>
<ol type="1">
<li>Linear equality constrained quadratic problems are the simplest.</li>
<li>Newton’s method is the next level.</li>
<li>Interior-point method form the next level.</li>
</ol>
<h2 id="logarithmic-barrier-function-and-central-path">11.2 Logarithmic barrier function and central path</h2>
<p>Our goal is to approximately formulate the inequality constrained problem as an equality constrained problem to which, newton’s method can be applied. Our first step is to rewrite the problem, making the equality constraints implicit in the objective, <span class="math display">\[
\coo[f_0(x)+\sum_{i=1}^mI_-(f_i(x))]{Ax=b}
\]</span> where <span class="math inline">\(I_-(u)=\begin{cases}0\quad u\le 0\\\infty\quad u&gt;0\end{cases}\)</span></p>
<h3 id="logarithmic-barrier">11.2.1 Logarithmic barrier</h3>
<p>The basic idea of the barrier method is to approximate the indicator function <span class="math inline">\(I_-\)</span> by the function <span class="math display">\[
\hat I_-(u)=-\frac{1}{t}\log (-u)
\]</span> Then the problem becomes <span class="math display">\[
\coo[f_0(x)+\sum_{i=1}^m -(1/t)\log (-f_i(x))]{Ax=b}
\]</span> The objective here is convex, since <span class="math inline">\(-(1/t)\log(-u)\)</span> is convex and increasing in u, and differentiable.</p>
<p>The function <span class="math display">\[
\phi(x)=-\sum_{i=1}^m \log(-f_i(x))
\]</span> is called the logarithmic barrier or log barrier for the problem.</p>
<p>Of course, the problem is only an approximation of the original problem, so one may wondering how well. Intuitive suggests, the approximation improves as the parameter <span class="math inline">\(t\)</span> grows. One the other hand, when the parameter <span class="math inline">\(t\)</span> is large, the function <span class="math inline">\(f_0+(1/t)\phi\)</span> is difficult to minimize by Newton’ method, since the Hessian varies rapidly near the boundary feasible set. We will see that this problem can be circumvented by solving a sequence of problems, increasing the parameter <span class="math inline">\(t\)</span> (and therefore, accuracy of the approximation) at each step, and starting each Newton minimization at the solution of the problem for the previous value of <span class="math inline">\(t\)</span>.</p>
<p>For future reference, we note that the gradient and Hessian of the logarithmic barrier function <span class="math inline">\(\phi\)</span> are given by <span class="math display">\[
\nabla \phi(x)=\sum_{i=1}^m \frac{1}{-f_i(x)}\nabla f_i(x)\\
\nabla^2 \phi(x)=\sum_{i=1}^{m} \frac{1}{f_i(x)^2}\nabla f_i(x)\nabla f_i(x)^T+\sum_{i=1}^m  \frac{1}{-f_i(x)}\nabla^2 f_i(x)
\]</span></p>
<h3 id="central-path">11.2.2 Central path</h3>
<p>We now consider in more detail the minimization problem. It will simplify notation later if we use the following form, <span class="math display">\[
\coo[tf_0(x)+\phi(x)]{Ax=b}
\]</span> We now assume the problem can be solved by newton method, and it has a <strong>unique solution</strong> for each <span class="math inline">\(t&gt;0\)</span></p>
<p>For <span class="math inline">\(t&gt;0\)</span>, we define <span class="math inline">\(x^\star(t)\)</span> as the solution. The <strong>central path</strong> associated with the problem is defined as the set of points <span class="math inline">\(x^\star(t)\)</span> for <span class="math inline">\(t&gt;0\)</span>, which we call, the central points. Points on the central path are characterized by the following necessary and sufficient conditions, <span class="math inline">\(x^\star(t)\)</span> is strictly feasible, i.e. <span class="math display">\[
Ax^\star(t)=b\\
f_i(x^\star (t))&lt;0
\]</span> and there exists a <span class="math inline">\(\hat v\)</span> such that <span class="math display">\[
0=t\nabla f_0(x^\star(t))+\nabla \phi(x^\star(t))+A^T\hat v\\
=t\nabla f_0(x^\star(t))+\sum_{i=1}^m \frac{1}{-f_i(x^\star(t))}\nabla f_i(x^\star(t))+A^T\hat v
\]</span> <strong>Dual points from central path</strong></p>
<p>From the above, we can derive an important property of the central path: every central point yields a dual feasible point, and hence a lower bound on the optimal <span class="math inline">\(p^\star\)</span>. We define <span class="math inline">\(\lambda_i^\star (t)=-\frac{1}{tf_i(x^\star(t))}\)</span>, <span class="math inline">\(v^\star(t)=\hat v/t\)</span></p>
<p>Then we have <span class="math display">\[
\nabla f_0(x^\star(t))+\sum_{i=1}^m\lambda_i^\star (t)\nabla f_i(x^\star(t))+A^Tv^\star (t)=0
\]</span> We see the optimal point <span class="math inline">\(x^\star(t)\)</span> minimizes the Lagrangian, <span class="math display">\[
L(x,\lambda,v)=f_0(x)+\sum_{i=1}^m \lambda_if_i(x)+v^T(Ax-b)
\]</span> <strong>Interpretation via KKT conditions</strong> <span class="math display">\[
\begin{array}{rcl}
Ax^\star =b\quad f_i(x)&amp;\le&amp; 0\\
\lambda^\star &amp;\succeq&amp; 0\\
\nabla f_0(x^\star)+\sum_{i=1}^{m} \lambda_i^\star\nabla f_i(x^\star)+A^Tv^\star&amp;=&amp;0\\
\lambda_i^\star f_i(x^\star)&amp;=&amp;1/t
\end{array}
\]</span> <strong>Force field interpretation</strong></p>
<h2 id="the-barrier-method">11.3 The barrier method</h2>
<p>We have seen that the point <span class="math inline">\(x^\star(t)\)</span> is <span class="math inline">\(m/t\)</span>-suboptimal, and that a certificate of this accuracy is provided by the dual feasible pair <span class="math inline">\(\lambda^\star(t)\)</span> and <span class="math inline">\(v^\star(t)\)</span>. This suggestion a very straightforward method for solving the original problem with a guaranteed specified accuracy <span class="math inline">\(\epsilon\)</span>. We simply take <span class="math inline">\(t=m/\epsilon\)</span> and solve the equality constrained problem <span class="math display">\[
\coo[(m/\epsilon)f_0(x)+\phi(x)]{Ax=b}
\]</span> using newton’s method. This method could be called unconstrained minimization method, since it allows us to solve simple problem, but it does not work well in other cases. As a result it rarely, if ever, used.</p>
<h3 id="the-barrier-method-1">11.3.1 The barrier method</h3>
<p>A simple extension of the unconstrained optimization method does work well. It is based on solving a sequence of unconstrained minimization problems, using the last point found as the starting point for the next unconstrained problem.</p>
<p>In other words, we compute <span class="math inline">\(x^\star(t)\)</span> for a sequence of increasing values of <span class="math inline">\(t\)</span>, until <span class="math inline">\(t\ge m/\epsilon\)</span>, which guarantee that we have an $-suboptimal $ solution of the original problem. When the method is proposed firstly, it is called sequential unconstrained minimization technique. Today, the method is usually called the <strong>barrier method</strong> or <strong>path-following</strong> method.</p>
<h3 id="examples">11.3.2 Examples</h3>
<h3 id="convergence-analysis">11.3.3 Convergence Analysis</h3>
<h3 id="newton-step-for-modified-kkt-conditions">11.3.4 Newton step for modified KKT conditions</h3>
<p>In the barrier method, the newton step <span class="math inline">\(\Delta x_{nt}\)</span>, and associated dual variables are given by the linear equations, <span class="math display">\[
\begin{bmatrix}
t\nabla^2f_0(x)+\nabla^2 \phi(x)&amp;A^T\\A&amp;0
\end{bmatrix}
\begin{bmatrix}
\Delta x_{nt}\\v_{nt}
\end{bmatrix}=-
\begin{bmatrix}
t\nabla f_0(x)+\nabla \phi(x)\\0
\end{bmatrix}
\]</span> In this section, we will show how these newton steps for the centering problem can be interpreted as Newton steps for directly solving the modified KKT equations, <span class="math display">\[
\nabla f_0(x)+\sum_{i=1}^m \lambda_if_i(x)+A^Tv=0\\
-\lambda_if_i(x)=1/t\\
Ax=b
\]</span> in a particular way.</p>
<h2 id="feasibility-and-phase-i-methods">11.4 Feasibility and phase I methods</h2>
<h2 id="complexity-analysis-via-self-concordance">11.5 Complexity analysis via self-concordance</h2>
<h2 id="problems-with-generalized-inequalities">11.6 Problems with generalized inequalities</h2>
<h2 id="primal-dual-interior-point-methods">11.7 Primal-dual Interior-point methods</h2>
<p>In this section, we describe a basic primal-dual interior-point method. Primal-dual interior-point methods are very similar to the barrier method, with some differences.</p>
<ol type="1">
<li><strong>There is only one loop or iteration</strong>, i.e. there is no distinction between inner and outer iterations as in the barrier method. At each iteration, both the primal and dual variables are updated.</li>
<li>The search direction in a primal-dual interior-point method are obtained from Newton’s method, applied to the <strong>modified KKT equations.</strong></li>
<li>In a primal-dual interior-point method, the primal-dual iterates are not necessarily feasible.</li>
</ol>
<p>Primal-dual interior-point methods are often more efficient than the barrier method, especially when high accuracy is required, since they can exhibit better than linear convergence.</p>
<h3 id="primal-dual-search-direction">11.7.1 Primal-dual search direction</h3>
<p>As in the barrier method, we start with the modified KKT conditions, expressed as <span class="math inline">\(r_t(x,\lambda,v)=0\)</span>, where we define <span class="math display">\[
r_t(x,\lambda,v)=
\begin{bmatrix}
\nabla f_0(x)+Df(x)^T\lambda+A^Tv\\
-diag(\lambda)f(x)-(1/t)1\\
Ax-b
\end{bmatrix}
\]</span> If <span class="math inline">\(x\)</span>, <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(v\)</span> satisfy <span class="math inline">\(r_t(x,\lambda,v)=0\)</span> (and <span class="math inline">\(f_i(x)&lt;0\)</span>), then <span class="math inline">\(x=x^\star (t)\)</span>, <span class="math inline">\(\lambda = \lambda^\star (t)\)</span>, <span class="math inline">\(v=v^\star (t)\)</span>. In particular, <span class="math inline">\(x\)</span> is primal feasible, and <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(v\)</span> are dual feasible, with duality gap <span class="math inline">\(m/t\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(r_{dual}=\nabla f_0(x)+Df(x)^T\lambda+A^Tv\)</span> is called the <strong>dual residual</strong>.</li>
<li><span class="math inline">\(r_{pri}=Ax-b\)</span> is called the <strong>primal residual</strong>.</li>
<li><span class="math inline">\(r_{cent}=\nabla f_0(x)+Df(x)^T\lambda+A^Tv\)</span> is called the <strong>centrality residual</strong>.</li>
</ol>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
