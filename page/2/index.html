<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Orandragon&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Orandragon&#39;s Blog">
  <link rel="canonical" href="http://yoursite.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/14/Nonlinear Optimization/5. Basic nonlinear programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/09/14/Nonlinear Optimization/5. Basic nonlinear programming/" class="post-title-link" itemprop="url">5. Basic Nonlinear Programming</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-09-14 22:32:04" itemprop="dateCreated datePublished" datetime="2019-09-14T22:32:04+08:00">2019-09-14</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-20 16:04:11" itemprop="dateModified" datetime="2019-09-20T16:04:11+08:00">2019-09-20</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/09/14/Nonlinear Optimization/5. Basic nonlinear programming/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/09/14/Nonlinear Optimization/5. Basic nonlinear programming/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="thanks-for-the-module-ma5268-nus-and-prof.-toh">Thanks for the module MA5268 NUS and Prof. Toh</h1>
<h1 id="basic-nonlinear-programming">5. Basic Nonlinear Programming</h1>
<p>This chapter is concerned with the following multi-dimensional general constrained minimization problem, <span class="math display">\[
\begin{array}{rCll}
\min &amp; f(x)\\
\text{subject to}&amp; g_i(x)=0,&amp; i=1,2,\dots,m\\
&amp;h_j(x)\le0,&amp; j=1,2,\dots,p\\
&amp;x\in\mathbb R^n,
\end{array}
\]</span> where <span class="math inline">\(f,g_i,h_j:\mathbb R^n\rightarrow \mathbb R\)</span> are continuous functions, for <span class="math inline">\(i=1,2\dots,m\)</span> and <span class="math inline">\(j=1,2,\dots,p\)</span>. The feasible set of the problem is the following subset of <span class="math inline">\(\mathbb R^n\)</span>, <span class="math display">\[
S=\left\{
x\in\mathbb R^n\;\; 
\begin{array}{|l}
g_i(x)=0,\;i=1,2,\dots,m
\\
h_j(x)\le0,\;j=1,2,\dots,p
\end{array}
\right\}.
\]</span> <span class="math inline">\(S\)</span> is a closed set.</p>
<p>The KKT necessary conditions are very useful in locating possible candidates for a global minimizer.</p>
<h2 id="regular-point">5.1 Regular point</h2>
<p><strong>Definition</strong> (Active constraint)</p>
<p>Let <span class="math inline">\(x^*\in S\)</span>. An inequality constraint <span class="math inline">\(h_j(x)\le 0\)</span> is said to be active at <span class="math inline">\(x^*\)</span> if <span class="math inline">\(h_j(x^*)=0\)</span>. Otherwise, it is said to be inactive at <span class="math inline">\(x^*\)</span>.</p>
<p>Graphically, if the constraint is active, then the point lies on the boundary defined by the condition <span class="math inline">\(h_j(x)=0\)</span>.</p>
<p><strong>Definition</strong> (Regular point)</p>
<p>Let <span class="math inline">\(x^* \in S\)</span> be a feasible point. Let <span class="math display">\[
J(x^*)=\{j\in\{1,\dotsm,p\}\;|\;h_j(x^*)=0\},
\]</span> be the index set of active constraints at <span class="math inline">\(x^*\)</span>. Suppose the set of gradient vectors <span class="math display">\[
\{\nabla g_i(x^*)\;|\; i=1,2,\dotsm,m\}\cup\{\nabla h_j(x^*)\;|\;j\in J(x^*)\}
\]</span> is linearly independent. Then we say <span class="math inline">\(x^*\)</span> is a regular point or the regularity condition holds at <span class="math inline">\(x^*\)</span>.</p>
<p>The above condition is called constraint qualification at <span class="math inline">\(x^*\in S\)</span>. There are other types of constraint qualifications. In this chapter, we shall only consider the above linearly independence constraint qualification (LICQ).</p>
<p>In particular, if <span class="math inline">\(x^*\)</span> is an interior-point of the feasible region, then <span class="math inline">\(J(x^*)=\O\)</span>. We shall call a point <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(J(x^*)=\O\)</span> a regular point.</p>
<h2 id="karush-kuhn-tucker-necessary-conditions">5.2 Karush-Kuhn-Tucker necessary conditions</h2>
<p>We state the KKT necessary conditions for a local minimizer <span class="math inline">\(x^*\)</span> at which the regularity condition holds.</p>
<p><strong>Theorem</strong> (KKT first order necessary conditions)</p>
<p>Suppose <span class="math inline">\(f,g_i,h_j:\mathbb R^n\rightarrow \mathbb R\)</span>, where <span class="math inline">\(i=1,2,\dotsm,m\)</span> and <span class="math inline">\(j=1,2,\dotsm,p\)</span>, has continuous first partial derivatives on the feasible set <span class="math inline">\(S\)</span>.</p>
<p>Suppose <span class="math inline">\(x^*\in S\)</span> is a <strong>regular point</strong>.</p>
<p>If <span class="math inline">\(x^*\)</span> is a local minimizer, then there exists (unique) scalars <span class="math inline">\(\lambda_1^*,\dotsm,\lambda_m^*\)</span> and <span class="math inline">\(\mu_1^*,\dotsm,\mu_p^*\)</span> such that the following conditions hold, <span class="math display">\[
\begin{array}{rCl}
\nabla f(x^*)+\sum_{i=1}^m\lambda_i^*\nabla g_i(x^*)+\sum_{j=1}^p\mu_j^*\nabla h_j(x^*)&amp;=&amp;0\\
\mu_j^*\ge 0 &amp;&amp; \forall j=1,\dotsm,p\\
\mu_j^*=0&amp;&amp;\forall j\notin J(x^*),
\end{array}
\]</span> where <span class="math inline">\(J(x^*)\)</span> is the index set of active inequality constraints at <span class="math inline">\(x^*\)</span>.</p>
<p><strong>Theorem</strong> (KKT second order necessary conditions)</p>
<p>Suppose <span class="math inline">\(f,g_i,\)</span> and <span class="math inline">\(h_j\)</span> have continuous second partial derivatives on <span class="math inline">\(S\)</span>. Let <span class="math display">\[
H_L(x^*)=H_f(x^*)+\sum_{i=1}^m \lambda_i^*H_{g_i}(x^*)+\sum_{j=1}^p \mu_j^*H_{h_j}(x^*).
\]</span> If <span class="math inline">\(x^*\)</span> is a local minimizer, then <span class="math display">\[
y^TH_L(x^*)y\ge 0,
\]</span> for all <span class="math inline">\(y\in T(x^*)\)</span>, where $$ T(x^*)= { yR^n</p>
<p>\begin{array}{|rl} g_i(x<sup><em>)^Ty=0, &amp;i=1,2,m\ h_j(x^</em>)</sup>T y=0, &amp; jJ(x^*). \end{array} }. $$ <strong>Remark</strong> (On the KKT necessary conditions)</p>
<ol type="1">
<li><p>We say that <span class="math inline">\((x^*,\lambda^*,\mu^*)\)</span> is a KKT point whenever <span class="math inline">\((x^*,\lambda^*,\mu^*)\)</span> satisfies the KKT first order conditions. KKT points are some times also called KKT solutions.</p></li>
<li><p>The scalar <span class="math inline">\(\lambda_1^*,\dotsm,\lambda_m^*\)</span> and <span class="math inline">\(\mu_1^*,\dotsm,\mu_p^*\)</span> are called <strong>Lagrange multipliers</strong>.</p></li>
<li><p>The conditions <span class="math inline">\(\mu_j^*=0,\;\forall j\notin J(x^*)\)</span> means that the Lagrange multiplier corresponding to the inactive constraint must be zero. Since <span class="math inline">\(h_j(x^*)&lt;0\;\forall j\notin J(x^*)\)</span> and <span class="math inline">\(h_j(x^*)=0\;\forall j\in J(x^*)\)</span>, the condition <span class="math inline">\(\mu_j^*=0\;\forall j\notin J(x^*)\)</span> is equivalent to <span class="math display">\[
\mu_j^*h_j(x^*)=0,\quad \forall j=1,\dotsm,p.
\]</span> This is called the <strong>complementary slackness condition</strong>.</p></li>
<li><p>The set <span class="math inline">\(T(x^*)\)</span> consists of vectors in the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span>. To see this, note that the normal space <span class="math inline">\(N(x^*)\)</span> to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span> is the subspace spanned by the set of normal vectors <span class="math display">\[
\nabla g_1(x^*),\nabla g_1(x^*),\dotsm\nabla g_m(x^*),\quad \nabla h_j(x^*)\;\forall j\in J(x^*).
\]</span> Now the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span> is the subspace of vectors that are orthogonal to all the normal vectors at <span class="math inline">\(x^*\)</span>. <span class="math display">\[
\text{Tangent} (x^*)=\{y\in\mathbb R^n\;|\; u^Ty=0,\forall u\in N(x^*)\}
\]</span></p></li>
<li><p>In deciding whether <span class="math inline">\(x^*\)</span> is a local optimizer, the definiteness of <span class="math inline">\(H_L(x)\)</span> is tested only for vectors in <span class="math inline">\(T(x^*)\)</span>.</p></li>
</ol>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(S\)</span> be the feasible set. Suppose <span class="math inline">\(x^*\in S\)</span> and <span class="math inline">\(J(x^*)\)</span> is the index set of active constraints at <span class="math inline">\(x^*\)</span>.</p>
<p>Consider the matrix <span class="math display">\[
\mathcal D(x^*)=\left(\nabla g_1(x^*),\nabla g_2(x^*),\dotsm,\nabla g_m(x^*),[\nabla h_j(x^*)\;|\;j\in J(x^*)]\right).
\]</span> Then <span class="math display">\[
y^TH_L(x^*)y\ge 0,\quad \forall y\in T(x^*)\iff Z(x^*)^TH_L(x^*)Z(x^*)\in \mathbb S_+^n,
\]</span> where <span class="math inline">\(Z(x^*)\in \mathbb R^{n\times q}\)</span> is a matrix whose columns form a basis of the null space of <span class="math inline">\(\mathcal D(x^*)^T\)</span>.</p>
<h2 id="examples-to-illustrate-the-kkt-necessary-conditions.">5.3 Examples to illustrate the KKT necessary conditions.</h2>
<p><strong>Corollary</strong></p>
<p>Suppose the following two conditions hold,</p>
<ol type="1">
<li>A global minimizer <span class="math inline">\(x^*\)</span> is known to exist.</li>
<li><span class="math inline">\(x^*\)</span> is a regular point.</li>
</ol>
<p>Then <span class="math inline">\(x^*\)</span> is a KKT point, i.e. there exists <span class="math inline">\(\lambda^*\in \mathbb R^m\)</span> and <span class="math inline">\(\mu^*\in\mathbb R^p\)</span> such that the following conditions hold <span class="math display">\[
\begin{array}{rCl}
\nabla f(x^*)+\sum_{i=1}^m\lambda_i^*\nabla g_i(x^*)+\sum_{j=1}^p\mu_j^*\nabla h_j(x^*)=0\\
g_i(x^*)=0,\;i=1,2,\dotsm,m\\
\mu_j^*\ge 0,\;h_j(x^*)\le 0,\;\mu_j^*h_j(x^*)=0,\;\forall j=1,2,\dotsm,p.
\end{array}
\]</span> <strong>Example</strong> (Projection onto a simplex)</p>
<p>Given <span class="math inline">\(g\in\mathbb R^n\)</span> and <span class="math inline">\(b&gt;0\)</span>. Consider the problem, <span class="math display">\[
\min\left\{f(x)=\frac{1}{2}||x-g||^2\;|\; e^Tx=b,x\ge 0 \right\},
\]</span> where <span class="math inline">\(e\)</span> is the vector of all ones. Its optimal solution <span class="math inline">\(x^*\)</span> can be computed analytically.</p>
<p><strong>Solution</strong></p>
<p>Assume <span class="math inline">\(g_1\ge g_2\ge \dotsm\ge g_n\)</span>.</p>
<ol type="1">
<li><p>First we prove that the optimal solution <span class="math inline">\(x^*\)</span> must also satisfy the condition <span class="math inline">\(x_1^*\ge x_2^*\ge\dotsm\ge x_n^*\)</span>. Suppose on the contrary that there exists indices <span class="math inline">\(i&lt;j\)</span> such that with <span class="math inline">\(x_i^*&lt;x_j^*\)</span>. Consider a new point <span class="math inline">\(\bar x\)</span> defined by (Just switch two elements) <span class="math display">\[
\bar x=\begin{cases}
x_k^*&amp;\text{if }k\neq i,j\\
x_j^*&amp;\text{if }k=i\\
x_i^*&amp;\text{if }k=j
\end{cases}.
\]</span> Then we have <span class="math display">\[
f(\bar x)-f(x^*)=\frac{1}{2}\left((x_i^*-g_j)^2+(x_j^*-g_i^2)\right)-\frac{1}{2}\left((x_i^*-g_i)^2+(x_j^*-g_j^2)\right)\\
=(g_i-g_j)(x_i^*-x_j^*)\le 0.
\]</span> Hence <span class="math inline">\(f(\bar x)\le f(x^*)\)</span>.</p></li>
<li><p>Then we use KKT conditions. The KKT conditions are given by, <span class="math display">\[
x^*-g-\mu-\lambda e=0,\quad \mu\ge 0,\;\lambda\in \mathbb R,\; x^*\ge 0\\
e^Tx^*=b\\
\mu x^*=0.
\]</span> The first equation imply that <span class="math inline">\(x^*=g+\mu+\lambda e\)</span>.</p>
<p>Now if <span class="math inline">\(g_i+\lambda &gt;0\)</span>, set <span class="math inline">\(x_i^*=g_i+\lambda\)</span> and <span class="math inline">\(\mu_i=0\)</span>. If <span class="math inline">\(g_i+\lambda \le 0\)</span>, set <span class="math inline">\(x_i^*=0\)</span> and <span class="math inline">\(\mu_i=-(g_i+\lambda)\)</span>. Now suppose <span class="math inline">\(x_i^*&gt;0\)</span> for <span class="math inline">\(i=1,\dotsm,r\)</span> and <span class="math inline">\(x^*_i=0\)</span> for <span class="math inline">\(i=r+1,\dotsm,n\)</span>. Then we have <span class="math display">\[
b=e^Tx=\sum_{i=1}^rx^*_i=\sum_{i=1}^r g_i+\lambda r\implies \lambda =\frac{1}{r}\left(b-\sum_{i=1}^rg_i\right).
\]</span> Now we need to find the largest <span class="math inline">\(r\)</span> denoted as <span class="math inline">\(\bar r\)</span> such that <span class="math display">\[
\bar r = \max \{r\;|\;x_r\ge 0\},
\]</span> where <span class="math inline">\(x_r=g_r+\lambda=g_r+\frac{1}{r}\left(b-\sum_{i=1}^rg_i\right)\)</span>.</p>
<p>Finally, we have <span class="math display">\[
x_i^*=\begin{cases} g_i+\lambda &amp; \text{if } i\le \bar r\\0&amp; \text{if }i&gt;\bar r.\end{cases}
\]</span></p></li>
<li><p>If the components of <span class="math inline">\(g\)</span> is not sorted in a descending order, we can find a permutation matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(\hat g=Pg\)</span> has its components arranged in a descending order. Then the optimal solution <span class="math inline">\(x^*\)</span> is given by <span class="math inline">\(x^*=P^T\hat x^*\)</span>.</p></li>
</ol>
<h2 id="interpretation-of-the-lagrange-multipliers">5.4 Interpretation of the Lagrange multipliers</h2>
<p>The Lagrange multipliers <span class="math inline">\(\lambda_i^*(i=1,\dotsm,m)\)</span>, <span class="math inline">\(\mu_j^*(j=1,\dotsm,p)\)</span> associated with the local optimizer <span class="math inline">\(x^*\)</span> in the KKT theorem measures the sensitivity of <span class="math inline">\(f(x^*)\)</span> to a small perturbation of the corresponding constraints <span class="math inline">\(g_i(x)=0\)</span> or <span class="math inline">\(h_j(x)\le 0\)</span>. In this section, we shall justify this interpretation for an equality constrained NLP. Consider the following equality constrained NLP, <span class="math display">\[
\begin{array}{rll}
\min&amp;f(x)\\
\text{subject to} &amp;g_i(x)=0&amp;\forall i=1,2,\dotsm,m.
\end{array}
\]</span> Suppose the constraints are relaxed as follows, <span class="math display">\[
\hat g_i(x;c)=g_i(x)+c_i,\quad i=1,2,\dotsm,m.
\]</span> Let <span class="math inline">\(C\subseteq \mathbb R^n\)</span> be an open neighborhood of <span class="math inline">\(0\)</span>. For each <span class="math inline">\(c\in C\)</span>, suppose there is a local minimizer <span class="math inline">\(x^*(c)\)</span> of <span class="math inline">\(f\)</span> on the constraint set <span class="math display">\[
\{x\in\mathbb R^n\;|\; \hat g_i(x;c)=0,\;\forall i=1,\dotsm,m\}.
\]</span> Note that <span class="math inline">\(x^*(0)=x^*\)</span> and <span class="math inline">\(\lambda^*(0)=\lambda^*\)</span>. Assume the regularity condition holds at each <span class="math inline">\(x^*(c)\)</span>. Now by the KKT theorem, there exists <span class="math inline">\(\lambda^*(c)\in\mathbb R^m\)</span> such that <span class="math display">\[
\nabla f(x^*(c))+\sum_{i=1}^m\lambda_i^*(c)\nabla \hat g_i(x^*(c))=0.
\]</span> Note that <span class="math display">\[
\frac{\partial \hat g_i}{x_l}(x)=\frac{\partial g_i}{x_l}(x),
\]</span> thus for each <span class="math inline">\(1\le l\le n\)</span>, we have <span class="math display">\[
\frac{\partial f}{\partial x_l}(x^*(c))+\sum_{i=1}^m \lambda_i^*(c)\frac{\partial g_i}{\partial x_l}(x^*(c))=0.
\]</span> <strong>Proposition</strong></p>
<p>Let <span class="math inline">\(F(c)=f(x^*(c))\)</span>. Suppose that <span class="math inline">\(F(c)\)</span> changes smoothly with respect to changes in <span class="math inline">\(c\)</span>. Then <span class="math display">\[
\frac{\partial F(c)}{\partial c_k}=\lambda^*_k(c),\quad \forall k=1,\dotsm,m.
\]</span> Proof. <span class="math display">\[
\frac{\partial F(c)}{\partial c_k}=\sum_{l=1}^n \frac{\partial f}{\partial x_l}(x^*(c))\frac{\partial x_l(c)}{\partial c_k}\\
=\sum_{l=1}^n\left[-\sum_{i=1}^m \lambda_i^*(c)\frac{\partial g_i}{\partial x_l}(x^*(c))\right]\frac{\partial x_l(c)}{\partial c_k}\\
=-\sum_{i=1}^m \lambda_i^*(c)\left[\sum_{l=1}^n\frac{\partial g_i}{\partial x_l}(x^*(c))\frac{\partial x_l(c)}{\partial c_k}\right].
\]</span> Since <span class="math inline">\(g_i(x^*(c))=-c_i\)</span> for all <span class="math inline">\(i=1,\dotsm,m\)</span>, we have <span class="math display">\[
\sum_{l=1}^n\frac{\partial g_i}{\partial x_l}(x^*(c))\frac{\partial x_l(c)}{\partial c_k}=-\frac{\partial x_l(c)}{\partial c_k}=\begin{cases}0&amp;\text{if }k\neq i\\-1&amp;\text{if }k=i\end{cases}.
\]</span> Therefore, we have <span class="math display">\[
\frac{\partial F(c)}{c_k}=\lambda_k^*(c).
\]</span> <strong>Remark</strong></p>
<ol type="1">
<li>Proposition says that the small change in the <span class="math inline">\(k\)</span>th constraint from <span class="math inline">\(g_k(x)=0\)</span> to <span class="math inline">\(g_k(x)+c_k=0\)</span> will change the optimal objective value <span class="math inline">\(f(x^*)\)</span> at the rate of <span class="math inline">\(\lambda_k^*\)</span>. That is the new optimal objective value is given approximately by <span class="math inline">\(f(x^*)+\lambda_k^*c_k\)</span>.</li>
<li>It is also applied to the inequality constraints.</li>
</ol>
<p><strong>Example</strong></p>
<p>From the example, the global minimizer <span class="math inline">\(x^*=[0;1]\)</span> with multiplier <span class="math inline">\(\mu_3=\frac{1}{2}\)</span>. Thus relaxing the constraint to <span class="math inline">\(g_i(x)+\epsilon\le 0\)</span> would change the objective value by <span class="math inline">\(\frac{1}{2}\epsilon\)</span>.</p>
<h2 id="kkt-sufficient-conditions">5.5 KKT sufficient conditions</h2>
<p>When a KKT point satisfies a stronger second order condition, we obtain a strict local minimizer. Note that the sufficient conditions for a strict local minimizer <strong>do not</strong> require regularity conditions.</p>
<p><strong>Theorem</strong> (KKT sufficient conditions)</p>
<p>Let <span class="math inline">\(f,g_i,h_j:\mathbb R^n\rightarrow \mathbb R,\;\forall i=1,2,\dotsm,m\)</span> and <span class="math inline">\(j=1,2,\dotsm,p\)</span> be functions with continuous second partial derivatives.</p>
<p>Let <span class="math inline">\(S\)</span> be the feasible set of NLP. Suppose <span class="math inline">\(x^*\in S\)</span> is a KKT point, i.e., there exist <span class="math inline">\(\lambda^*\in\mathbb R^m\)</span> and <span class="math inline">\(\mu^*\in\mathbb R^p\)</span> such that <span class="math display">\[
\begin{array}{rCl}
\nabla f(x^*)+\sum_{i=1}^m\lambda_i^*\nabla g_i(x^*)+\sum_{j=1}^p\mu_j^*\nabla h_j(x^*)=0\\
g_i(x^*)=0,\;i=1,2,\dotsm,m\\
\mu_j^*\ge 0,\;h_j(x^*)\le 0,\;\mu_j^*h_j(x^*)=0,\;\forall j=1,2,\dotsm,p,
\end{array}
\]</span> where<span class="math inline">\(J(x^*)\)</span> is the index set of active constraints at <span class="math inline">\(x^*\)</span>. Let <span class="math display">\[
H_L(x^*)=H_f(x^*)+\sum_{i1=}^m \lambda_i^*H_{g_i}(x^*)+\sum_{j=1}^p\mu_{j}^*H_{h_{j}}(x^*).
\]</span> Suppose <span class="math display">\[
y^TH_L(x^*)y&gt;0,\;\forall y\in T(x^*)-\{0\},
\]</span> where <span class="math inline">\(T(x^*)\)</span> is the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span>. Then <span class="math inline">\(x^*\)</span> is a <strong>strict local minimizer</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<p>Suppose <span class="math display">\[
y^TH_L(x^*)y&lt;0,\;\forall y\in T(x^*)-\{0\},
\]</span> where <span class="math inline">\(T(x^*)\)</span> is the tangent space to <span class="math inline">\(S\)</span> at <span class="math inline">\(x^*\)</span>. Then <span class="math inline">\(x^*\)</span> is a <strong>strict local maximizer</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<h2 id="kkt-conditions-for-constrained-convex-programming-problems">5.6 KKT conditions for constrained convex programming problems</h2>
<p>Convexity is a very strong condition. In fact, for a convex problem, a feasible point <span class="math inline">\(x^*\)</span> is a KKT point implies it is a global minimizer.</p>
<p>Consider the following convex programming problem, $$ \begin{array}{rCl} &amp;f(x)\  &amp; g_i(x)=a_i^Tx-b_i=0,&amp;i=1,,m\ &amp;h_j(x)0,&amp;j=1,,p</p>
<p>\end{array} <span class="math display">\[
where $f,h_j:\mathbb R^n\rightarrow \mathbb R$ are convex function. It can be expressed as
\]</span> \begin{array}{rCl} &amp;f(x)\  &amp; Ax-b=0\ &amp;h_j(x)0,&amp;j=1,,p</p>
<p>\end{array} $$ <strong>Theorem</strong> (KKT point is an optimal solution under convexity)</p>
<p>Suppose <span class="math inline">\(f,h_j:\mathbb R^n\rightarrow \mathbb R,\;j=1,\dotsm,p\)</span> are <strong>differentiable convex functions</strong>, and <span class="math inline">\(g_i(x)=a_i^Tx-b_i,\;i=1,\dotsm,m\)</span>. Let <span class="math inline">\(S\)</span> be the feasible region of NLP. If <span class="math inline">\(x^*\in S\)</span> is a KKT point, then <span class="math inline">\(x^*\)</span> is a global minimizer of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>.</p>
<p>Proof.</p>
<p>If <span class="math inline">\(x^*\)</span> is a KKT point, then we need to prove <span class="math inline">\(f(x)\ge f(x^*)\)</span>.</p>
<p>Since we have <span class="math display">\[
f(y)\ge f(x)+\nabla f(x)^T(y-x)\implies \\
f(x)\ge f(x^*)+\nabla f(x^*)(x-x^*)
\]</span> Therefore, we only need to prove <span class="math display">\[
\nabla f(x^*)(x-x^*)\ge 0.
\]</span> <span class="math inline">\(x^*\in S\)</span> is a KKT point implies <span class="math inline">\(\exists \lambda \in \mathbb R^m\)</span> and <span class="math inline">\(\mu_j^*\ge 0,\;\forall j\in J(x^*)\)</span> such that <span class="math display">\[
\nabla f(x^*)+\sum_{i=1}^m \lambda_i^*a_i+\sum_{j\in J(x^*)}\mu_j^*\nabla h_j(x^*)=0\\
\implies\\
\nabla f(x^*)(x-x^*)+\sum_{i=1}^m \lambda_i^*a_i(x-x^*)+\sum_{j\in J(x^*)}\mu_j^*\nabla h_j(x^*)(x-x^*)=0\\
\implies\\
\nabla f(x^*)(x-x^*)=-\sum_{j\in J(x^*)}\mu_j^*\nabla h_j(x^*)(x-x^*).
\]</span> Since <span class="math inline">\(h_j\)</span> is convex, we have <span class="math display">\[
0\ge h_j(x)\ge h_j(x^*)+\nabla h_j(x^*)(x-x^*)=\nabla h_j(x^*)(x-x^*).
\]</span> Then we have <span class="math display">\[
\nabla f(x^*)(x-x^*)\ge 0.
\]</span> <strong>Remark</strong></p>
<p>The converse of the Theorem is not true without additional assumption, i.e. a global minimizer of a convex program may not be a KKT point. With regularity condition, a global minimizer is a KKT point. For convex programming problem with at least one inequality constraints, the <strong>Slater's condition</strong> ensures that a global minimizer is a KKT point.</p>
<p><strong>Theorem</strong> (Conversely)</p>
<p>Suppose <span class="math inline">\(f,h_j:\mathbb R^n\rightarrow \mathbb R,\;j=1,\dotsm,p\)</span> are <strong>differentiable convex functions</strong>, and <span class="math inline">\(g_i(x)=a_i^Tx-b_i,\;i=1,\dotsm,m\)</span>. Suppose <span class="math inline">\(p\ge 1\)</span> and that the <strong>Slater's condition</strong> holds, i.e., there exists <span class="math inline">\(\hat x\in\mathbb R^n\)</span> such that <span class="math inline">\(g_i(\hat x)=0,\;\forall i=1,\dotsm,m\)</span> and <span class="math inline">\(h_j(\hat x)&lt;0,\;\forall j=1,\dotsm,p\)</span>. Let <span class="math inline">\(S\)</span> be the feasible region. Suppose <span class="math inline">\(x^*\in S\)</span> is a global minimizer of <span class="math inline">\(f\)</span> on <span class="math inline">\(S\)</span>. Then <span class="math inline">\(x^*\)</span> is a KKT point.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/" class="post-title-link" itemprop="url">4. Gradient Methods</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-09-06 13:19:39" itemprop="dateCreated datePublished" datetime="2019-09-06T13:19:39+08:00">2019-09-06</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-20 16:04:18" itemprop="dateModified" datetime="2019-09-20T16:04:18+08:00">2019-09-20</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="thanks-for-the-module-ma5268-nus-and-prof.-toh">Thanks for the module MA5268 NUS and Prof. Toh</h1>
<h1 id="gradient-methods">Gradient methods</h1>
<h2 id="gradient-methods-for-unconstrained-optimization-problem">4.1 Gradient methods for unconstrained optimization problem</h2>
<p>Consider the multidimensional unconstrained minimization problem <span class="math display">\[
\begin{array}{rCl}
\text{minimize} &amp; f(x)\\
\text{subject to}&amp; x\in S.
\end{array}
\]</span> In the situation where <span class="math inline">\(\nabla f(x)=0\)</span> cannot be solved analytically, we look for an approximate solution via iterative methods.</p>
<p>General framework of an optimization algorithm.</p>
<hr>
<p>For <span class="math inline">\(k=0,1,...,\)</span> if <span class="math inline">\(x^{(k)}\)</span> is optimal stop else determine an improved estimate of the solution <span class="math inline">\(x^{(k+1)}=x^{(k)}+\alpha_{k}\times p^{(k)}\)</span> end end</p>
<hr>
<p>Here <span class="math inline">\(p^{(k)}\)</span> is a <strong>search direction</strong> that we hope points towards the solution, or that improves our solution in some sense. The scalar <span class="math inline">\(\alpha_k\)</span> is a step length that determines the point <span class="math inline">\(x^{(k+1)}\)</span>. Once the search direction has been solved, the step length can be solved as some auxiliary one-dimensional problems.</p>
<p><strong>Descent property</strong> At a given point <span class="math inline">\(x^*\)</span>, let <span class="math inline">\(\hat d=-\frac{\nabla f(x^*)}{||\nabla f(x^*)||}\)</span>. The value of <span class="math inline">\(f\)</span> decreases most rapidly along the unit direction <span class="math inline">\(\hat d\)</span> and the rate of change of <span class="math inline">\(f\)</span> at <span class="math inline">\(x^*\)</span> along the direction <span class="math inline">\(\hat d\)</span> is <span class="math inline">\(-||\nabla f(x^*)||\)</span>, i.e., as <span class="math inline">\(x\)</span> moves along <span class="math inline">\(\hat d\)</span> from <span class="math inline">\(x^*\)</span> by a small distance <span class="math inline">\(\delta\)</span>, the value <span class="math inline">\(f(x)\)</span> is changed by the amount <span class="math inline">\(-||\nabla f(x^*)||\delta\)</span>.</p>
<p>In general, a direction <span class="math inline">\(d\)</span> such that <span class="math inline">\(\langle \nabla f(x^*),d\rangle &lt;0\)</span> is called a descent direction.</p>
<p>The direction <span class="math inline">\(-\nabla f(x^*)\)</span> is known as the steepest descent direction since it gives the fastest rate of decrease in <span class="math inline">\(f(x)\)</span> among all directions.</p>
<p><strong>Steepest descent method with exact line search</strong></p>
<hr>
<p><strong>[step 0]</strong> Select an initial point <span class="math inline">\(x^{(0)}\)</span>, and <span class="math inline">\(\epsilon&gt;0\)</span> <strong>[step k]</strong> For <span class="math inline">\(k=0,1,...,\)</span></p>
<ol type="1">
<li><p>evaluate <span class="math inline">\(d^{(k)} = -\nabla f(x^{(k)})\)</span></p></li>
<li><p>if <span class="math inline">\(||d^{(k)}||&lt;\epsilon\)</span>, stop and <span class="math inline">\(x^{(k)}\)</span> is an approximate solution</p>
<p>else</p>
<ol type="1">
<li><p>find the value <span class="math inline">\(t_k\)</span> that minimizes the one-dimensional function <span class="math display">\[
g(t)=f(x^{(k)}+td^{(k)})
\]</span></p></li>
<li><p>set <span class="math inline">\(x^{(k+1)}=x^{(k)}+t_kd^{(k)}\)</span>.</p></li>
</ol></li>
</ol>
<p>end</p>
<hr>
<p><strong>Remark</strong></p>
<p>The most difficult part of the steepest descent method usually is to find the <span class="math inline">\(t_k\)</span> that minimizes <span class="math inline">\(f\)</span> along the gradient direction.</p>
<p><strong>Theorem 4.1</strong> The steepest descent method with exact linesearch moves in <strong>perpendicular</strong> steps.</p>
<p><strong>Remarks</strong></p>
<ol type="1">
<li><p>Monotonic decreasing property</p>
<p>If <span class="math inline">\(x^{(k)}\)</span> is a steepest descent sequence for a function <span class="math inline">\(f(x)\)</span>, and if <span class="math inline">\(\nabla f(x^{(k)})\neq 0\)</span> for some <span class="math inline">\(k\)</span>, then <span class="math inline">\(f(x^{(k+1)})&lt;f(x^{(k)})\)</span>.</p></li>
<li><p>Convergence of a steepest descent method</p>
<p>Suppose <span class="math inline">\(f(x)\)</span> is a coercive function with continuous first derivatives on <span class="math inline">\(\mathbb R^n\)</span>. Let <span class="math inline">\(x^{(0)}\in \mathbb R^n\)</span>. Suppose <span class="math inline">\(\{x^{(k)}\}\)</span> is the steepest descent sequence for <span class="math inline">\(f(x)\)</span> with initial point <span class="math inline">\(x^{(0)}\)</span>. Then some subsequence of <span class="math inline">\(\{x^{(k)}\}\)</span> converges. The limit of any convergent subsequence of <span class="math inline">\(\{x^{(k)}\}\)</span> is a critical point of <span class="math inline">\(f(x)\)</span>.</p></li>
</ol>
<h3 id="convergence-rate-of-the-steepest-descent-method-for-an-unconstrained-convex-quadratic-minimization-problem">4.1.1 Convergence rate of the steepest descent method for an unconstrained convex quadratic minimization problem</h3>
<p>Here we consider <span class="math display">\[
\text{minimize}_{x\in\mathbb R^n}\; q(x)=\frac{1}{2}x^TQx,
\]</span> where <span class="math inline">\(Q\)</span> is symmetric positive definite.</p>
<p><strong>Proposition</strong></p>
<p>For a symmetric positive definite <span class="math inline">\(Q\)</span>, suppose that <span class="math inline">\(\{x^{(k)}\}\)</span> is a sequence obtained from the steepest descent method with exact line search applied to the function <span class="math inline">\(q(x)\)</span>. Then</p>
<ol type="1">
<li><p>Let <span class="math inline">\(d^{k}=\nabla q(x^k)=Qx^k\)</span>, <span class="math display">\[
\frac{q(x^{k+1})}{q(x^{k})}=1-\frac{\langle d^k,d^k\rangle^2}{\langle d^k,Qd^k\rangle\langle d^k,Q^{-1}d^k\rangle}
\]</span></p></li>
<li><p><span class="math display">\[
\frac{q(x^{k+1})}{q(x^{k})}\le \left[\frac{\kappa (Q)-1}{\kappa (Q)+1}\right]^2=\rho(Q)
\]</span></p>
<p>where <span class="math inline">\(\kappa (Q) = \lambda_n/\lambda_1\)</span>, and <span class="math inline">\(\lambda_n\)</span> and <span class="math inline">\(\lambda_1\)</span> are the largest and smallest eigenvalues of <span class="math inline">\(Q\)</span>, respectively. The number <span class="math inline">\(\kappa(Q)\)</span> is called the condition number of <span class="math inline">\(Q\)</span>. When <span class="math inline">\(\kappa(Q)\ge 1\)</span> is small, say less than <span class="math inline">\(10^3\)</span>, <span class="math inline">\(Q\)</span> is said to be well-conditioned.</p></li>
</ol>
<p>Proof is intuitive and skipped.</p>
<p><strong>Remark</strong></p>
<ol type="1">
<li><p>From proposition, we see that the convergence rate <span class="math inline">\(\rho (Q)\)</span> of the steepest descent method depends on <span class="math inline">\(\kappa(Q)\)</span>. When <span class="math inline">\(\kappa(Q)\)</span> is large, the convergence rate <span class="math display">\[
\rho(Q)\approx 1-\frac{4}{\kappa(Q)}.
\]</span></p></li>
<li><p>The number of iterations needed to reduce the relative error <span class="math inline">\(q(x_k)/q(x_0)\)</span> to smaller than <span class="math inline">\(\epsilon\)</span> is given by <span class="math display">\[
k=\left[\frac{\log \epsilon }{\log \rho(Q)}\right]+1
\]</span> where <span class="math inline">\([a]\)</span> denotes the largest integer less than or equal to <span class="math inline">\(a\)</span>.</p></li>
</ol>
<h3 id="convergence-rate-for-the-steepest-descent-method-for-strongly-convex-function">4.1.2 Convergence rate for the steepest descent method for strongly convex function</h3>
<p>Let <span class="math inline">\(S\subset \mathbb R^n\)</span> be a convex set and <span class="math inline">\(f:S\rightarrow \mathbb R\)</span> a convex function. We assume that <span class="math inline">\(f\)</span> is strongly convex with parameter <span class="math inline">\(m\)</span> and has <span class="math inline">\(M\)</span>-Lipschitz continuous gradient on <span class="math inline">\(S\)</span>. Then its Hessian satisfies the following property, <span class="math display">\[
mI\preceq H_f(x)\preceq MI\;\forall x\in S.
\]</span> <strong>Lemma</strong></p>
<p>Let <span class="math inline">\(x^*\)</span> be a minimizer. Then <span class="math display">\[
f(x)-\frac{1}{2m}||\nabla f(x)||^2\le f(x^*)\le f(x)-\frac{1}{2M}||\nabla f(x)||^2 \; \forall x\in S.
\]</span> <strong>Theorem</strong></p>
<p>Let <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> be strongly convex with parameter <span class="math inline">\(m\)</span> and its gradient is <span class="math inline">\(M\)</span>-Lipschitz. Let <span class="math inline">\(x^*\)</span> be the unique of <span class="math inline">\(f\)</span> over <span class="math inline">\(\mathbb R^n\)</span>. Define <span class="math inline">\(E_k = f(x^k)-f(x^*)\)</span>, where <span class="math inline">\(\{x^k\}\)</span> is generated by the steepest descent method with exact linesearch. Then, <span class="math display">\[
E_{k+1}\le E_k-\frac{1}{2M}||\nabla f(x^k)||^2\\
E_{k+1}\le E_k\left(1-\frac{m}{M}\right).
\]</span> Proof.</p>
<p>By using the property of <span class="math inline">\(M\)</span>-Lipschitz, we have <span class="math display">\[
f(y)\le f(x)+\langle \nabla f(x),y-x \rangle+\frac{M}{2}||y-x||^2\;\forall x,y\in\mathbb R^n.
\]</span></p>
<p>If we choose <span class="math inline">\(y=x^k-td^k\)</span>, <span class="math inline">\(d^k=\nabla f(x^k)\)</span> and <span class="math inline">\(x=x^k\)</span>, we have <span class="math display">\[
f(x^k-td^k)\le f(x^k)-t\langle \nabla f(x^k),d^k \rangle+\frac{Mt^2}{2}||d^k||^2.
\]</span> We define the function <span class="math display">\[
f(t)=f(x^k)-t\langle \nabla f(x^k),d^k \rangle+\frac{Mt^2}{2}||d^k||^2.
\]</span> Then we have <span class="math display">\[
f_{\min}=f(\frac{1}{M})=f(x^k)-\frac{1}{2M}||d^k||^2.
\]</span> Therefore, we have <span class="math display">\[
f(x^{k+1})\le f(x^k)-\frac{1}{2M}||d^k||^2.
\]</span> The first inequality can be proved.</p>
<p>For the second inequality, since we have <span class="math display">\[
E_k\le \frac{1}{2m}||\nabla f(x)||^2.
\]</span> Then we have <span class="math inline">\(-||d^k||^2\le -2mE_k\)</span>. The second inequality can be proved.</p>
<p><strong>Remark</strong></p>
<p>From the theorem, we see that <span class="math display">\[
E(x^{k+1})/E(x^1)\le (1-m/M)^k\le \epsilon,
\]</span> which implies that we need the number of iterations <span class="math inline">\(k\)</span> to satisfy <span class="math display">\[
k\ge \frac{\log \epsilon ^{-1}}{\log\rho^{-1}}\approx \frac{m}{M}\log \epsilon^{-1} \;(\text{ if }m/M\ll 1),
\]</span> where <span class="math inline">\(\rho = 1-m/M\)</span>.</p>
<h2 id="line-search-strategies">4.2 Line search strategies</h2>
<ol type="1">
<li><p>Minimization rule = exact line search. <span class="math display">\[
\alpha_k=\arg\min\{f(x^k+\alpha d^k)\;|\;\alpha \ge 0\}.
\]</span> If the line search interval is limited to <span class="math inline">\(\alpha \in [0,\bar \alpha]\)</span>, it is called limited minimization rule.</p></li>
<li><p>Armijo rule (<strong>backtracking</strong> method).</p>
<p>Let <span class="math inline">\(\sigma\in(0,0.5)\)</span> and <span class="math inline">\(\beta\in (0,1)\)</span>. Start with <span class="math inline">\(\bar\alpha\)</span> and continue with $=,,$ until the following inequality is satisfied, <span class="math display">\[
f(x^k+\alpha d^k)\le f(x^k)+\alpha\sigma\langle \nabla f(x^k),d^k\rangle.
\]</span> Let <span class="math inline">\(r\)</span> be the first integer satisfying the inequality. Set <span class="math inline">\(\alpha_k=\beta^r\bar \alpha\)</span>.</p></li>
<li><p>Non-monotone line search.</p></li>
</ol>
<h2 id="accelerated-proximal-gradient-method-for-convex-programming">4.3 Accelerated proximal gradient method for convex programming</h2>
<p>Consider a smooth convex function <span class="math inline">\(f\)</span> with <span class="math inline">\(L\)</span>-Lipschitz continuous gradient. We are interested in solving <span class="math display">\[
\min \{F(x)=f(x)+g(x)\;|\;x\in \mathbb R^n\},
\]</span> where <span class="math inline">\(g:\mathbb R^n\rightarrow (-\infty,\infty]\)</span> is a proper closed convex function.</p>
<p><strong>Example</strong></p>
<p>In sparse regression problem, <span class="math inline">\(f(x)=\frac{1}{2}||Ax-b||^2\)</span> and <span class="math inline">\(g(x)=\rho ||x||_1\)</span>.</p>
<p>For a given <span class="math inline">\(\bar x\)</span> and <span class="math inline">\(H\succeq 0\)</span>, consider the convex quadratic function, <span class="math display">\[
q(x;\bar x)= f(\bar x)+\langle \nabla f(\bar x),x-\bar x\rangle +\frac{1}{2}\langle x-\bar x,H(x-\bar x)\rangle.
\]</span> At the current point <span class="math inline">\(\bar x\)</span>, proximal gradient and APG methods solve a sub-problem of the form, <span class="math display">\[
\hat x =\arg \min \{g(x)+q(x;\bar x)\;|\;x\in \mathbb R^n\}.
\]</span> <strong>Accelerated proximal gradient (APG) method</strong></p>
<p>Given a positive sequence <span class="math inline">\(\{t_k\}\)</span> such that <span class="math inline">\(t_{k+1}^2-t_{k+1}\le t_k^2\)</span> starting with <span class="math inline">\(t_0=1\)</span>, <span class="math inline">\(t_1=1\)</span>. Given <span class="math inline">\(x^0\)</span>. For <span class="math inline">\(k=0,1,\dots\)</span>, do the following iterations.</p>
<p><strong>[Step 1]</strong> Set <span class="math inline">\(\beta_k=(t_k-1)/t_{k+1}\)</span> and <span class="math inline">\(\bar x^k=x^k+\beta(x^k-x^{k-1})\)</span>.</p>
<p><strong>[Step 2]</strong> Compute <span class="math display">\[
x^{k+1}=\arg \min \{g(x)+q(x;\bar x^k)\;|\;x\in \mathbb R^n\}.
\]</span> When <span class="math inline">\(t_k=1\)</span> for all <span class="math inline">\(k\)</span>, then <span class="math inline">\(\bar x^k=x^k\)</span> for all <span class="math inline">\(k\)</span>, and the method is the standard <strong>proximal gradient method</strong>.</p>
<p><strong>Lemma</strong></p>
<p>Assume that <span class="math inline">\(f(\hat x)\le q(\hat x;\bar x)\)</span>. (This assumption can be satisfied by choosing <span class="math inline">\(H\)</span>) Then we have the following <strong>decent property</strong>, <span class="math display">\[
F(x)+\frac{1}{2}||x-\bar x||_H^2\ge F(\hat x)+\frac{1}{2}||x-\hat x||_H^2\;\forall x\in \mathbb R^n.
\]</span> Proof.</p>
<p>We have <span class="math display">\[
\begin{array}{rCl}
F(x)-F(\hat x)&amp;=&amp;F(x)-f(\hat x)-g(\hat x)\\
&amp;\ge&amp; F(x)-q(\hat x;\bar x)-g(\hat x)\\
&amp;=&amp; g(x)-g(\hat x)+f(x)-f(\bar x)-\langle \nabla f(\bar x),\hat x-\bar x\rangle-\frac{1}{2}||\hat x-\bar x||_H^2.
\end{array}
\]</span> By convexity of <span class="math inline">\(g\)</span> and <span class="math inline">\(f\)</span>, we have <span class="math display">\[
g(x)-g(\hat x)\ge \langle \lambda,x-\hat x\rangle\quad \lambda \in \partial g(\hat x)\\
f(x)-f(\bar x)\ge \langle \nabla f(\bar x),x-\bar x\rang.
\]</span> Then we have <span class="math display">\[
F(x)-F(\hat x)\ge \langle \lambda+\nabla f(\bar x),x-\hat x\rangle -\frac{1}{2}||\hat x-\bar x||_H^2.
\]</span> From the optimality condition, we have <span class="math display">\[
0\in \partial g(\hat x)+\nabla f(\bar x)+H(\hat x-\bar x)\\
\implies\\
\lambda +\nabla f(\bar x)= H(\bar x-\hat x).
\]</span> Then we have <span class="math display">\[
F(x)-F(\hat x)\ge \langle H(\bar x-\hat x),x-\hat x\rangle -\frac{1}{2}||\hat x-\bar x||_H^2\\
=\frac{1}{2}||x-\hat x||_H^2-\frac{1}{2}||x-\bar x||_H^2.
\]</span> Q.E.D.</p>
<p><strong>Theorem</strong></p>
<p>Let <span class="math inline">\(x^*\)</span> be a minimizer of <span class="math inline">\(F(\cdot)\)</span>. Define <span class="math inline">\(E(\cdot)=F(\cdot)-F(x^*)\ge 0\)</span>. Assume that <span class="math inline">\(f(x^{k+1})\le q(x^{k+1};\bar x^k)\)</span> for all <span class="math inline">\(k\)</span>. Then <span class="math display">\[
E(x^{k+1})\le \frac{1}{2t_{k+1}^2}||x^*-x^0||_H^2.
\]</span> In practice, the sequence <span class="math inline">\(\{t_k\}\)</span> is typically defined recursively as follows, <span class="math display">\[
t_{k+1}=\frac{1+\sqrt{1+4t_k^2}}{2}\implies t_k\ge \frac{k}{2}\quad \forall k\ge 1
\]</span> Hence, <span class="math display">\[
0\le F(x^k)-F(x^*)\le \frac{1}{2t_k^2}||x^*-x^0||_H^2\le \frac{2}{k^2}||x^*-x^0||_H^2.
\]</span> That is, the iteration complexity of APG is <span class="math inline">\(O(1/k^2)\)</span>.</p>
<p><strong>Theorem</strong></p>
<p>Assume that <span class="math inline">\(f(x^{k+1})\le q(x^{k+1};\bar x^k)\)</span> for all <span class="math inline">\(k\)</span>. If <span class="math inline">\(t_k=1\)</span> for all <span class="math inline">\(k\)</span>. Then <span class="math display">\[
kE(x^k)+\frac{1}{2}||x^k-x^*||_H^2\le E(x^1)+\frac{1}{2}||x^1-x^*||_H^2\le \frac{1}{2}||x^0-x^*||_H^2.
\]</span> Hence <span class="math display">\[
0\le F(x^k)-F(x^*)\le \frac{1}{2k}||x^0-x^*||_H^2.
\]</span> That is, the iteration complexity of the proximal gradient method is <span class="math inline">\(O(1/k)\)</span>.</p>
<p><strong>Example</strong></p>
<p>Consider the sparse regression problem, <span class="math display">\[
\min \{\frac{1}{2}||Ax-b||^2+\rho||x||_1\;|\; x\in\mathbb R^n\},
\]</span> where <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span>, <span class="math inline">\(b\in \mathbb R^m\)</span> and <span class="math inline">\(\rho\)</span> are given data. Let <span class="math inline">\(f(x)=\frac{1}{2}||Ax-b||^2\)</span> and <span class="math inline">\(g(x)=\rho||x||_1\)</span>. Then <span class="math inline">\(\nabla f(x)=A^T(Ax-b)\)</span> is Lipschitz continuous with modulus <span class="math inline">\(L=\lambda_\max(AA^T)\)</span>. Pick <span class="math inline">\(H=LI_n\)</span>, the APG subproblem is given by <span class="math display">\[
\begin{array}{rCl}
x^{k+1}&amp;=&amp;\arg\min_{x}\left\{g(x)+\langle \nabla f(\bar x^k),x-\bar x^k\rangle +\frac{L}{2}||x-\bar x^k||^2\;|\; x\in\mathbb R^n\right\}\\
&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}(x-\bar x^k+\frac{2}{L}\nabla f(\bar x^k))^2-\frac{2}{L}||\nabla f(\bar x^k)||^2\right\}\\
&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}[x-(\bar x^k-\frac{2}{L}\nabla f(\bar x^k))]^2\right\}
\end{array}
\]</span> Then if we define <span class="math inline">\(y^k=\bar x^k-\frac{2}{L}\nabla f(\bar x^k)\)</span>, we have <span class="math display">\[
\begin{array}{rCl}
x^{k+1}&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}(x-y^k)^2\right\}.
\end{array}
\]</span> To be done.............</p>
<p><strong>Example</strong></p>
<p>Given <span class="math inline">\(G\in \mathbb S^n\)</span>, consider the projection problem onto the closed convex cone <span class="math inline">\(DNN_n^*=S_+^N+\mathcal N^n\)</span>. This problem can be formulated as, <span class="math display">\[
\min\left\{\frac{1}{2}||S+Z-G||^2\;|\;S\in\mathbb S^n_+,\;Z\in\mathcal N^n\right\}\\
=\min\left\{\frac{1}{2}||Z-(G-S)||^2\;|\;S\in\mathbb S^n_+,\;Z\in\mathcal N^n\right\}\\
=\min\left\{\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2\;|\;S\in\mathbb S^n_+\right\}\\
=\min\left\{\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2+\delta_{\mathbb S_+^n}(S)\right\}\\
\]</span> We define <span class="math inline">\(f(S)=\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2\)</span>. From the optimality condition, we have <span class="math display">\[
0\in\nabla f(\bar S)+\partial \delta_{\mathbb S_+^n}\quad \text{$f(\bar S)$ is differentiable}.\\
-\nabla f(\bar S)\in \delta_{\mathbb S_+^n}(S)\\
\bar S-\nabla f(\bar S)\in (I+\delta_{\mathbb S_+^n})(\bar S)
\]</span> Then we have <span class="math display">\[
\bar S=(I+\delta_{\mathbb S_+^n})^{-1}(\bar S-\nabla f(\bar S))=\Pi_{\mathbb S_+^n}(\bar S-\nabla f(\bar S)).
\]</span> To be done..........</p>
<h2 id="gradient-projection-method">4.4 Gradient projection method</h2>
<p>Let <span class="math inline">\(f:Q\rightarrow \mathbb R\)</span> be a continuously differentiable function (not necessarily convex) defined on a closed convex subset <span class="math inline">\(Q\)</span> of <span class="math inline">\(\mathbb R^n\)</span> with <span class="math inline">\(L\)</span>-Lipschitz continuous gradient. Consider <span class="math display">\[
\min\{f(x)\;|\; x\in Q\}.
\]</span> We say that <span class="math inline">\(\bar x\in Q\)</span> is a stationary point if <span class="math display">\[
\langle \nabla f(\bar x),y-\bar x\rangle \ge 0\quad\forall y\in Q.
\]</span> The gradient projection method generate a sequence of iterates as follows, <span class="math display">\[
x^{k+1}=P_Q(x^k-\alpha \nabla f(x^k)).
\]</span> For each <span class="math inline">\(x\in Q\)</span>, let <span class="math display">\[
x(\alpha )=x-\alpha\nabla f(x),\quad x_Q(\alpha )=P_Q(x(\alpha)).
\]</span></p>
<h2 id="stochastic-gradient-descent-method">4.5 Stochastic gradient descent method</h2>
<p>Suppose <span class="math inline">\(\tilde Z\)</span> is an n-dimensional random variable with mean <span class="math inline">\(\mu \in \mathbb R^n\)</span>. Consider the following problem, <span class="math display">\[
\min_{x\in\mathbb R^n}h(x)=\frac{1}{2}\mathbb E[||x-\tilde Z||^2],
\]</span> where <span class="math inline">\(\mathbb E(\cdot)\)</span> denotes the expectation with respect to the distribution of <span class="math inline">\(Z\)</span>, i.e., <span class="math display">\[
E[||x-\tilde Z||^2]=\int ||x-\tilde Z(w)||^2dP(w).
\]</span> If <span class="math inline">\(Z\)</span> is a discrete random variable, then <span class="math display">\[
E[||x-\tilde Z||^2]=\sum_{i=1}^Np_i||x-a_i||^2.
\]</span> In practice, to solve this problem, one may draw <span class="math inline">\(m\)</span> independent identically distributed samples of <span class="math inline">\(\tilde Z\)</span>, say <span class="math inline">\(S=\{z_1,\dots,z_m\}\)</span>, and then solve <span class="math display">\[
\min_{x\in\mathbb R^n}F_S(x)=\frac{1}{m}\sum_{i=1}^m \frac{1}{2}||x-z_i||^2.
\]</span> The corresponding optimality condition is <span class="math display">\[
0=\nabla F_S(x)=\frac{1}{m}\sum_{i=1}^m(x-z_i)\implies x=\frac{1}{m}\sum_{i=1}^mz_i.
\]</span></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
