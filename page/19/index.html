<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Orandragon&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/19/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Orandragon&#39;s Blog">
  <link rel="canonical" href="http://yoursite.com/page/19/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/22/EE5907 Pattern Recognition/1. Probability/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2020/12/22/EE5907 Pattern Recognition/1. Probability/" class="post-title-link" itemprop="url">Chapter 1. Probability</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-12-22 13:46:13 / Modified: 14:44:47" itemprop="dateCreated datePublished" datetime="2020-12-22T13:46:13+08:00">2020-12-22</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/EE5907-Pattern-Recognition/" itemprop="url" rel="index"><span itemprop="name">EE5907 Pattern Recognition</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2020/12/22/EE5907 Pattern Recognition/1. Probability/" class="post-meta-item leancloud_visitors" data-flag-title="Chapter 1. Probability" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/12/22/EE5907 Pattern Recognition/1. Probability/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/12/22/EE5907 Pattern Recognition/1. Probability/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="types-of-machine-learning">1. Types of Machine Learning</h2>
<p>Supervised Learning: Great if we know what we what to predict</p>
<p>Unsupervised learning: Great if we want to discover something new</p>
<h2 id="design-cycle">2. Design Cycle</h2>
<p>Data collection</p>
<p>Preprocessing: Image enhancement, remove background, segmentation</p>
<p>Divide data into training and test sets (Feature Extraction, Train classifier)</p>
<p>Re-visit previous steps if performance unsatisfactory</p>
<h2 id="probability-review">3. Probability Review</h2>
<p>A random variable x is a quantity that is uncertain.</p>
<p>If x is discrete, then the probability of x <span class="math inline">\(p(x)\)</span> is probability mass function (pmf)</p>
<p>if x is continuous, then the probability of x <span class="math inline">\(p(x)\)</span> is probability distribution function (pdf)</p>
<p><span class="math inline">\(p(x,y)\)</span> is joint probability distribution of x and y. we have <span class="math display">\[
p(y)=\int_x p(x,y)dx\quad or\quad p(y)=\sum_xp(x,y)
\]</span> This is called marginalization.</p>
<p>Suppose we observe y to be <span class="math inline">\(y_1\)</span> , then <span class="math inline">\(p(x|y=y_1)\)</span> is how likely x will take on various values given this observation, that read as conditional probability of x given y is equal to <span class="math inline">\(y_1\)</span> . We have <span class="math display">\[
p(x|y)=\frac{p(x,y)}{p(y)}=\frac{p(y|x)p(x)}{\int_xp(x,y)dx}=\frac{p(y|x)p(x)}{\int_xp(y|x)p(x)dx}
\]</span></p>
<h2 id="discrete-distributions">4. Discrete Distributions</h2>
<p>==Bernoulli Distribution== <span class="math display">\[
P(x)=\lambda^x(1-\lambda)^{1-x}
\]</span> where x=0, 1</p>
<p>Bernoulli Distribution describes situation where only two possible outcomes x=0, 1</p>
<p>==Categorical Distribution== <span class="math display">\[
P(x=k)=\lambda_k
\]</span> Categorical Distribution describes situation where K possible outcomes</p>
<p>==Binomial Distribution==</p>
<p>The binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own boolean-valued outcome: a random variable containing a single bit of information: success/yes/true/one (with probability p) or failure/no/false/zero (with probability q = 1 − p). <span class="math display">\[
P(x=k)=\left(
\begin{array}{c}
n\\
k
\end{array}
\right)p^{k}(1-p)^k
\]</span> ==Multinomial Distribution==</p>
<p>The multinomial distribution is a generalization of the binomial distribution. For example, it models the probability of counts for rolling a k-sided die n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories. <span class="math display">\[
\begin{array}{ccc}
P(X_1=x_1,X_2=x_2,X_3=x_3\dotsm)=\frac{n!}{x_1!x_2!\dotsm x_k!}p_1^{x_1}p_2^{x_2}\dotsm p_k^{x_k}\quad when \sum_{i=1}^kx_i=n\\
otherwise\quad P(X_1=x_1,X_2=x_2,X_3=x_3\dotsm)=0
\end{array}
\]</span> ==Poisson Distribution==</p>
<p>The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. <span class="math display">\[
P(x=k)=e^{-\lambda}\frac{\lambda^k}{k!}
\]</span></p>
<h2 id="continuous-distributions">5. Continuous Distributions</h2>
<p>==Gaussian Distribution (Normal Distribution)==</p>
<p>It states that averages of samples of observations of random variables independently drawn from independent distributions converge in distribution to the normal, that is, they become normally distributed when the number of observations is sufficiently large <span class="math display">\[
P(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</span> ==Exponential Distribution==</p>
<p>The exponential distribution is the probability distribution that describes the time between events in a Poisson point process. <span class="math display">\[
P(X=x)=\lambda e^{-\lambda x}
\]</span> ==Gamma Distribution==</p>
<p>The gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution. <span class="math display">\[
P(x|\alpha,\beta)=\frac{\beta^\alpha x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)}
\]</span> where <span class="math inline">\(\Gamma(\alpha)\)</span> is the Gamma Function.</p>
<p>==Beta Distribution==</p>
<p>The beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable and control the shape of the distribution. It is a special case of the Dirichlet distribution.</p>
<p>The beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines.</p>
<p>In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. For example, the beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions. <span class="math display">\[
P(x|\alpha,\beta)=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}
\]</span> where <span class="math inline">\(B(\alpha,\beta)\)</span> is Beta Function defined by <span class="math display">\[
B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
\]</span> ==Dirichlet Distribution==</p>
<p>the Dirichlet distribution often denoted as Dir(<span class="math inline">\(\alpha\)</span>) is a family of continuous multivariate probability distributions parameterized by a vector <span class="math inline">\(\alpha\)</span> of positive reals. It is a multivariate generalization of the beta distribution, hence its alternative name of Multivariate Beta distribution (MBD). <span class="math display">\[
P(x|\alpha)=\frac{1}{B(\alpha)}\prod_{i=1}^Kx_i^{\alpha_i-1}
\]</span></p>
<h2 id="bayes-rule">5. Bayes' Rule</h2>
<p>For continuous <span class="math display">\[
p(x,y)=p(x)p(y|x)=p(y)p(x|y) \implies \\
p(y|x)=\frac{p(y)p(x|y)}{p(x)}=\frac{p(y)p(x|y)}{\int p(x,y)dy}=\frac{p(y)p(x|y)}{\int p(y)p(x|y) dy}
\]</span> For discrete <span class="math display">\[
p(x,y)=p(x)p(y|x)=p(y)p(x|y) \implies \\
p(y|x)=\frac{p(y)p(x|y)}{p(x)}=\frac{p(y)p(x|y)}{\sum_y p(x,y)}=\frac{p(y)p(x|y)}{\sum_y p(y)p(x|y) }
\]</span> Denotion <span class="math display">\[
\underbrace{p(y|x)}_\text{Posterior}=\frac{\overbrace{p(y)}^\text{Prior}\quad\overbrace{p(x|y)}^\text{Liklihood}}{\underbrace{p(x)}_\text{Evidence}}
\]</span> Prior: What we know before seeing x</p>
<p>Posterior: What we know after seeing x</p>
<p>Likelihood: Propensity for observing a certain value of x given certain value of y</p>
<p>Evidence: Make sure the left hand is a valid distribution</p>
<h2 id="expectation">6. Expectation</h2>
<p>Definition of expectation <span class="math display">\[
E(f(x))=\sum_xf(x)p(x)\\
E(f(x))=\int_xf(x)p(x)dx\\
\]</span> Definition of mean <span class="math display">\[
E(x)=\int_xxp(x)dx=\mu_x
\]</span> Definition of variance <span class="math display">\[
E((x-\mu_x)^2)=\int_x(x-\mu_x)^2p(x)dx=\sigma_x^2
\]</span> then <span class="math inline">\(\sigma_x\)</span> is called standard deviation</p>
<p>Definition of covariance <span class="math display">\[
E((x-\mu_x)(y-\mu_y))=\int\int(x-\mu_x)(y-\mu_y)p(x,y)dxdy=Cov(x,y)
\]</span> Some important conclusions for expectation <span class="math display">\[
E(af(x)+bf(y))=aE(f(x))+bE(f(y))\\
Cov(x,y)=E(xy)-E(x)E(y)\\
Var(x)=Cov(x,x)=E(x^2)-E^2(x)
\]</span> Definition of Conditional Expectation <span class="math display">\[
E(f(x,y)|y)=E_{p(x|y)}[f(x,y)]=\sum_xf(x,y)p(x|y)
\]</span> Definition of Conditional Independece <span class="math display">\[
p(x_1,x_2|x_3)=p(x_1|x_3)p(x_2|x_3)
\]</span> Attention!</p>
<p><span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are independent does not imply <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are conditionally independent given <span class="math inline">\(x_3\)</span>.</p>
<p><span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are independent given <span class="math inline">\(x_3\)</span> does not imply <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are independent.</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/13/Convex Optimization/11. Interior-Point Methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/09/13/Convex Optimization/11. Interior-Point Methods/" class="post-title-link" itemprop="url">Chapter 11. Interior-point methods</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-09-13 13:49:01 / Modified: 14:00:59" itemprop="dateCreated datePublished" datetime="2019-09-13T13:49:01+08:00">2019-09-13</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Convex-Optimization/" itemprop="url" rel="index"><span itemprop="name">Convex Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2019/09/13/Convex Optimization/11. Interior-Point Methods/" class="post-meta-item leancloud_visitors" data-flag-title="Chapter 11. Interior-point methods" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/09/13/Convex Optimization/11. Interior-Point Methods/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/09/13/Convex Optimization/11. Interior-Point Methods/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="interior-point-methods">11. Interior-point methods</h1>
<h2 id="inequality-constrained-minimization-problems">11.1 Inequality constrained minimization problems</h2>
<p>In this section, we discuss interior-point methods for solving convex optimization problems that include inequality constraints. <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f_0(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
f_i(x)\le 0\\Ax=b
\end{array}
\end{array}
\]</span> We also assume the problem is strictly feasible, there exists dual optimal <span class="math inline">\(\lambda^\star\)</span> and <span class="math inline">\(v^\star\)</span> together with <span class="math inline">\(x^\star\)</span> satisfying the KKT conditions <span class="math display">\[
\begin{array}{rcl}
Ax^\star =b\quad f_i(x)&amp;\le&amp; 0\\
\lambda^\star &amp;\succeq&amp; 0\\
\nabla f_0(x^\star)+\sum_{i=1}^{m} \lambda_i^\star\nabla f_i(x^\star)+A^Tv^\star&amp;=&amp;0\\
\lambda_i^\star f_i(x^\star)&amp;=&amp;0
\end{array}
\]</span></p>
<p>Interior-point method solve the problem by applying newton’s method to a sequence of equality constrained problem. We will concentrate on a particular interior-point method, called the barrier method, for which we give a proof of convergence and a complexity analysis. We also describe the primal-dual interior point method, but do note give analysis.</p>
<p>We can view interior-point methods as another level in the hierarchy of convex optimization algorithms.</p>
<ol type="1">
<li>Linear equality constrained quadratic problems are the simplest.</li>
<li>Newton’s method is the next level.</li>
<li>Interior-point method form the next level.</li>
</ol>
<h2 id="logarithmic-barrier-function-and-central-path">11.2 Logarithmic barrier function and central path</h2>
<p>Our goal is to approximately formulate the inequality constrained problem as an equality constrained problem to which, newton’s method can be applied. Our first step is to rewrite the problem, making the equality constraints implicit in the objective, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f_0(x)+\sum_{i=1}^mI_-(f_i(x))
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}
\]</span> where <span class="math inline">\(I_-(u)=\begin{cases}0\quad u\le 0\\\infty\quad u&gt;0\end{cases}\)</span></p>
<h3 id="logarithmic-barrier">11.2.1 Logarithmic barrier</h3>
<p>The basic idea of the barrier method is to approximate the indicator function <span class="math inline">\(I_-\)</span> by the function <span class="math display">\[
\hat I_-(u)=-\frac{1}{t}\log (-u)
\]</span> Then the problem becomes <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
f_0(x)+\sum_{i=1}^m -(1/t)\log (-f_i(x))
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}
\]</span> The objective here is convex, since <span class="math inline">\(-(1/t)\log(-u)\)</span> is convex and increasing in u, and differentiable.</p>
<p>The function <span class="math display">\[
\phi(x)=-\sum_{i=1}^m \log(-f_i(x))
\]</span> is called the logarithmic barrier or log barrier for the problem.</p>
<p>Of course, the problem is only an approximation of the original problem, so one may wondering how well. Intuitive suggests, the approximation improves as the parameter <span class="math inline">\(t\)</span> grows. One the other hand, when the parameter <span class="math inline">\(t\)</span> is large, the function <span class="math inline">\(f_0+(1/t)\phi\)</span> is difficult to minimize by Newton’ method, since the Hessian varies rapidly near the boundary feasible set. We will see that this problem can be circumvented by solving a sequence of problems, increasing the parameter <span class="math inline">\(t\)</span> (and therefore, accuracy of the approximation) at each step, and starting each Newton minimization at the solution of the problem for the previous value of <span class="math inline">\(t\)</span>.</p>
<p>For future reference, we note that the gradient and Hessian of the logarithmic barrier function <span class="math inline">\(\phi\)</span> are given by <span class="math display">\[
\nabla \phi(x)=\sum_{i=1}^m \frac{1}{-f_i(x)}\nabla f_i(x)\\
\nabla^2 \phi(x)=\sum_{i=1}^{m} \frac{1}{f_i(x)^2}\nabla f_i(x)\nabla f_i(x)^T+\sum_{i=1}^m  \frac{1}{-f_i(x)}\nabla^2 f_i(x)
\]</span></p>
<h3 id="central-path">11.2.2 Central path</h3>
<p>We now consider in more detail the minimization problem. It will simplify notation later if we use the following form, <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
tf_0(x)+\phi(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}
\]</span> We now assume the problem can be solved by newton method, and it has a <strong>unique solution</strong> for each <span class="math inline">\(t&gt;0\)</span></p>
<p>For <span class="math inline">\(t&gt;0\)</span>, we define <span class="math inline">\(x^\star(t)\)</span> as the solution. The <strong>central path</strong> associated with the problem is defined as the set of points <span class="math inline">\(x^\star(t)\)</span> for <span class="math inline">\(t&gt;0\)</span>, which we call, the central points. Points on the central path are characterized by the following necessary and sufficient conditions, <span class="math inline">\(x^\star(t)\)</span> is strictly feasible, i.e. <span class="math display">\[
Ax^\star(t)=b\\
f_i(x^\star (t))&lt;0
\]</span> and there exists a <span class="math inline">\(\hat v\)</span> such that <span class="math display">\[
0=t\nabla f_0(x^\star(t))+\nabla \phi(x^\star(t))+A^T\hat v\\
=t\nabla f_0(x^\star(t))+\sum_{i=1}^m \frac{1}{-f_i(x^\star(t))}\nabla f_i(x^\star(t))+A^T\hat v
\]</span> <strong>Dual points from central path</strong></p>
<p>From the above, we can derive an important property of the central path: every central point yields a dual feasible point, and hence a lower bound on the optimal <span class="math inline">\(p^\star\)</span>. We define <span class="math inline">\(\lambda_i^\star (t)=-\frac{1}{tf_i(x^\star(t))}\)</span>, <span class="math inline">\(v^\star(t)=\hat v/t\)</span></p>
<p>Then we have <span class="math display">\[
\nabla f_0(x^\star(t))+\sum_{i=1}^m\lambda_i^\star (t)\nabla f_i(x^\star(t))+A^Tv^\star (t)=0
\]</span> We see the optimal point <span class="math inline">\(x^\star(t)\)</span> minimizes the Lagrangian, <span class="math display">\[
L(x,\lambda,v)=f_0(x)+\sum_{i=1}^m \lambda_if_i(x)+v^T(Ax-b)
\]</span> <strong>Interpretation via KKT conditions</strong> <span class="math display">\[
\begin{array}{rcl}
Ax^\star =b\quad f_i(x)&amp;\le&amp; 0\\
\lambda^\star &amp;\succeq&amp; 0\\
\nabla f_0(x^\star)+\sum_{i=1}^{m} \lambda_i^\star\nabla f_i(x^\star)+A^Tv^\star&amp;=&amp;0\\
\lambda_i^\star f_i(x^\star)&amp;=&amp;1/t
\end{array}
\]</span> <strong>Force field interpretation</strong></p>
<h2 id="the-barrier-method">11.3 The barrier method</h2>
<p>We have seen that the point <span class="math inline">\(x^\star(t)\)</span> is <span class="math inline">\(m/t\)</span>-suboptimal, and that a certificate of this accuracy is provided by the dual feasible pair <span class="math inline">\(\lambda^\star(t)\)</span> and <span class="math inline">\(v^\star(t)\)</span>. This suggestion a very straightforward method for solving the original problem with a guaranteed specified accuracy <span class="math inline">\(\epsilon\)</span>. We simply take <span class="math inline">\(t=m/\epsilon\)</span> and solve the equality constrained problem <span class="math display">\[
\begin{array}{|l|ll|}
\text{minimize}\quad &amp; 
\begin{array}{lll}
(m/\epsilon)f_0(x)+\phi(x)
\end{array}\\\hline
\text{subject to}\quad &amp; 
\begin{array}{lll}
Ax=b
\end{array}
\end{array}
\]</span> using newton’s method. This method could be called unconstrained minimization method, since it allows us to solve simple problem, but it does not work well in other cases. As a result it rarely, if ever, used.</p>
<h3 id="the-barrier-method-1">11.3.1 The barrier method</h3>
<p>A simple extension of the unconstrained optimization method does work well. It is based on solving a sequence of unconstrained minimization problems, using the last point found as the starting point for the next unconstrained problem.</p>
<p>In other words, we compute <span class="math inline">\(x^\star(t)\)</span> for a sequence of increasing values of <span class="math inline">\(t\)</span>, until <span class="math inline">\(t\ge m/\epsilon\)</span>, which guarantee that we have an $-suboptimal $ solution of the original problem. When the method is proposed firstly, it is called sequential unconstrained minimization technique. Today, the method is usually called the <strong>barrier method</strong> or <strong>path-following</strong> method.</p>
<h3 id="examples">11.3.2 Examples</h3>
<h3 id="convergence-analysis">11.3.3 Convergence Analysis</h3>
<h3 id="newton-step-for-modified-kkt-conditions">11.3.4 Newton step for modified KKT conditions</h3>
<p>In the barrier method, the newton step <span class="math inline">\(\Delta x_{nt}\)</span>, and associated dual variables are given by the linear equations, <span class="math display">\[
\begin{bmatrix}
t\nabla^2f_0(x)+\nabla^2 \phi(x)&amp;A^T\\A&amp;0
\end{bmatrix}
\begin{bmatrix}
\Delta x_{nt}\\v_{nt}
\end{bmatrix}=-
\begin{bmatrix}
t\nabla f_0(x)+\nabla \phi(x)\\0
\end{bmatrix}
\]</span> In this section, we will show how these newton steps for the centering problem can be interpreted as Newton steps for directly solving the modified KKT equations, <span class="math display">\[
\nabla f_0(x)+\sum_{i=1}^m \lambda_if_i(x)+A^Tv=0\\
-\lambda_if_i(x)=1/t\\
Ax=b
\]</span> in a particular way.</p>
<h2 id="feasibility-and-phase-i-methods">11.4 Feasibility and phase I methods</h2>
<h2 id="complexity-analysis-via-self-concordance">11.5 Complexity analysis via self-concordance</h2>
<h2 id="problems-with-generalized-inequalities">11.6 Problems with generalized inequalities</h2>
<h2 id="primal-dual-interior-point-methods">11.7 Primal-dual Interior-point methods</h2>
<p>In this section, we describe a basic primal-dual interior-point method. Primal-dual interior-point methods are very similar to the barrier method, with some differences.</p>
<ol type="1">
<li><strong>There is only one loop or iteration</strong>, i.e. there is no distinction between inner and outer iterations as in the barrier method. At each iteration, both the primal and dual variables are updated.</li>
<li>The search direction in a primal-dual interior-point method are obtained from Newton’s method, applied to the <strong>modified KKT equations.</strong></li>
<li>In a primal-dual interior-point method, the primal-dual iterates are not necessarily feasible.</li>
</ol>
<p>Primal-dual interior-point methods are often more efficient than the barrier method, especially when high accuracy is required, since they can exhibit better than linear convergence.</p>
<h3 id="primal-dual-search-direction">11.7.1 Primal-dual search direction</h3>
<p>As in the barrier method, we start with the modified KKT conditions, expressed as <span class="math inline">\(r_t(x,\lambda,v)=0\)</span>, where we define <span class="math display">\[
r_t(x,\lambda,v)=
\begin{bmatrix}
\nabla f_0(x)+Df(x)^T\lambda+A^Tv\\
-diag(\lambda)f(x)-(1/t)1\\
Ax-b
\end{bmatrix}
\]</span> If <span class="math inline">\(x\)</span>, <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(v\)</span> satisfy <span class="math inline">\(r_t(x,\lambda,v)=0\)</span> (and <span class="math inline">\(f_i(x)&lt;0\)</span>), then <span class="math inline">\(x=x^\star (t)\)</span>, <span class="math inline">\(\lambda = \lambda^\star (t)\)</span>, <span class="math inline">\(v=v^\star (t)\)</span>. In particular, <span class="math inline">\(x\)</span> is primal feasible, and <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(v\)</span> are dual feasible, with duality gap <span class="math inline">\(m/t\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(r_{dual}=\nabla f_0(x)+Df(x)^T\lambda+A^Tv\)</span> is called the <strong>dual residual</strong>.</li>
<li><span class="math inline">\(r_{pri}=Ax-b\)</span> is called the <strong>primal residual</strong>.</li>
<li><span class="math inline">\(r_{cent}=\nabla f_0(x)+Df(x)^T\lambda+A^Tv\)</span> is called the <strong>centrality residual</strong>.</li>
</ol>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/18/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><span class="page-number current">19</span><a class="page-number" href="/page/20/">20</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="extend next" rel="next" href="/page/20/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">71</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
